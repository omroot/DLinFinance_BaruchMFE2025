{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Transformer Architecture Ablation Study\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will implement a comprehensive ablation study comparing three Transformer architectures for time series forecasting:\n",
    "\n",
    "1. **Encoder-Decoder Transformer** (Seq2Seq style)\n",
    "2. **Encoder-Only Transformer** (BERT style)\n",
    "3. **Decoder-Only Transformer** (GPT style)\n",
    "\n",
    "You will test these on three synthetic time series datasets with different characteristics to understand which architecture works best for which type of data.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand different Transformer architecture variants\n",
    "- Learn how to conduct systematic ablation studies\n",
    "- Analyze model performance across different data characteristics\n",
    "- Practice hypothesis-driven experimental design\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "**H1**: Does encoder-decoder architecture perform better on seasonal patterns due to bidirectional attention?\n",
    "\n",
    "**H2**: Does decoder-only architecture struggle with noisy data due to error accumulation?\n",
    "\n",
    "**H3**: Does encoder-only architecture offer a good accuracy/efficiency tradeoff?\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Throughout this notebook, you will see cells marked with:\n",
    "- `# TODO:` - Code you need to write\n",
    "- `# YOUR CODE HERE` - Placeholder for your implementation\n",
    "- `raise NotImplementedError()` - Remove this after implementing\n",
    "\n",
    "Complete all TODOs to run the full ablation study and analyze the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup and Imports\n",
    "\n",
    "First, import all necessary libraries. The helper functions you need are defined in the next cell, so no separate utility module needs to be imported.\n",
    "\n",
    "**TODO**: Run the cell to set up your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "helper"
    ]
   },
   "outputs": [],
   "source": [
    "# Helper utilities consolidated from hw02_utils.py (do not edit)\n",
    "\"\"\"\n",
    "Transformer Architecture Ablation Study - Utility Functions\n",
    "Homework 2: Time Series Forecasting with Transformers\n",
    "\n",
    "This module contains all data generation, preprocessing, model architectures,\n",
    "training utilities, and evaluation functions for the ablation study.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: SYNTHETIC DATA GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_datasets() -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate three synthetic time series with different characteristics.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping dataset names to time series arrays\n",
    "\n",
    "    Datasets:\n",
    "        - Dataset A (Seasonal): Multiple frequency seasonal patterns\n",
    "        - Dataset B (Trend+Noise): Linear trend with high noise\n",
    "        - Dataset C (Mixed): Trend + seasonal + moderate noise\n",
    "\n",
    "    Example:\n",
    "        >>> datasets = generate_synthetic_datasets()\n",
    "        >>> print(datasets.keys())\n",
    "        dict_keys(['Dataset A (Seasonal)', 'Dataset B (Trend+Noise)', 'Dataset C (Mixed)'])\n",
    "    \"\"\"\n",
    "    t = np.arange(1000)\n",
    "\n",
    "    # Dataset A: Seasonal with Multiple Frequencies\n",
    "    series_A = (10 * np.sin(2*np.pi*t/50) +    # weekly pattern\n",
    "                5 * np.sin(2*np.pi*t/200) +     # monthly pattern\n",
    "                0.5 * np.random.randn(1000))    # small noise\n",
    "\n",
    "    # Dataset B: Trend with Noise\n",
    "    series_B = (0.05 * t +                      # steady trend\n",
    "                3 * np.random.randn(1000))      # high noise\n",
    "\n",
    "    # Dataset C: Trend-Seasonal Combination\n",
    "    series_C = (0.03 * t +                      # gentle trend\n",
    "                8 * np.sin(2*np.pi*t/100) +     # seasonal\n",
    "                0.8 * np.random.randn(1000))    # moderate noise\n",
    "\n",
    "    return {\n",
    "        'Dataset A (Seasonal)': series_A,\n",
    "        'Dataset B (Trend+Noise)': series_B,\n",
    "        'Dataset C (Mixed)': series_C\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_datasets(datasets: Dict[str, np.ndarray], save_path: str = 'synthetic_datasets.png'):\n",
    "    \"\"\"\n",
    "    Visualize all synthetic datasets in a 3-panel figure.\n",
    "\n",
    "    Args:\n",
    "        datasets: Dictionary mapping dataset names to time series\n",
    "        save_path: Path to save the figure\n",
    "\n",
    "    Example:\n",
    "        >>> datasets = generate_synthetic_datasets()\n",
    "        >>> plot_datasets(datasets)\n",
    "        ✓ Synthetic datasets plotted and saved\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "    for idx, (name, series) in enumerate(datasets.items()):\n",
    "        axes[idx].plot(series, linewidth=1)\n",
    "        axes[idx].set_title(name, fontsize=14, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Time Step')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Synthetic datasets plotted and saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for time series forecasting.\n",
    "\n",
    "    Creates sliding windows of (input, target) pairs for sequence-to-sequence learning.\n",
    "\n",
    "    Args:\n",
    "        data: 1D array of time series values\n",
    "        lookback: Number of past timesteps to use as input\n",
    "        forecast_horizon: Number of future timesteps to predict\n",
    "\n",
    "    Example:\n",
    "        >>> data = np.random.randn(1000)\n",
    "        >>> dataset = TimeSeriesDataset(data, lookback=50, forecast_horizon=10)\n",
    "        >>> x, y = dataset[0]\n",
    "        >>> print(x.shape, y.shape)\n",
    "        torch.Size([50, 1]) torch.Size([10, 1])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, lookback: int = 50, forecast_horizon: int = 10):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.lookback - self.forecast_horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[idx:idx+self.lookback]\n",
    "        y = self.data[idx+self.lookback:idx+self.lookback+self.forecast_horizon]\n",
    "        return x.unsqueeze(-1), y.unsqueeze(-1)  # Add feature dimension\n",
    "\n",
    "\n",
    "def prepare_data(series: np.ndarray,\n",
    "                 train_ratio: float = 0.7,\n",
    "                 val_ratio: float = 0.15,\n",
    "                 lookback: int = 50,\n",
    "                 forecast_horizon: int = 10,\n",
    "                 batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Split time series into train/val/test sets and create dataloaders.\n",
    "\n",
    "    Applies StandardScaler normalization using training set statistics.\n",
    "\n",
    "    Args:\n",
    "        series: Input time series array\n",
    "        train_ratio: Fraction for training set\n",
    "        val_ratio: Fraction for validation set\n",
    "        lookback: Input sequence length\n",
    "        forecast_horizon: Output sequence length\n",
    "        batch_size: Batch size for dataloaders\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader, scaler)\n",
    "\n",
    "    Example:\n",
    "        >>> series = np.random.randn(1000)\n",
    "        >>> train_loader, val_loader, test_loader, scaler = prepare_data(series)\n",
    "        >>> for x, y in train_loader:\n",
    "        ...     print(x.shape, y.shape)\n",
    "        ...     break\n",
    "        torch.Size([32, 50, 1]) torch.Size([32, 10, 1])\n",
    "    \"\"\"\n",
    "    n = len(series)\n",
    "    train_size = int(n * train_ratio)\n",
    "    val_size = int(n * val_ratio)\n",
    "\n",
    "    # Split data\n",
    "    train_data = series[:train_size]\n",
    "    val_data = series[train_size:train_size+val_size]\n",
    "    test_data = series[train_size+val_size:]\n",
    "\n",
    "    # Normalize using training statistics\n",
    "    scaler = StandardScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
    "    val_data_scaled = scaler.transform(val_data.reshape(-1, 1)).flatten()\n",
    "    test_data_scaled = scaler.transform(test_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(train_data_scaled, lookback, forecast_horizon)\n",
    "    val_dataset = TimeSeriesDataset(val_data_scaled, lookback, forecast_horizon)\n",
    "    test_dataset = TimeSeriesDataset(test_data_scaled, lookback, forecast_horizon)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, scaler\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: POSITIONAL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for Transformer models.\n",
    "\n",
    "    Adds position information to token embeddings using sine and cosine functions\n",
    "    of different frequencies.\n",
    "\n",
    "    Args:\n",
    "        d_model: Dimension of model embeddings\n",
    "        max_len: Maximum sequence length\n",
    "\n",
    "    Formula:\n",
    "        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to input tensor.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture A: Full Encoder-Decoder Transformer (Seq2Seq style).\n",
    "\n",
    "    Uses both encoder and decoder stacks with cross-attention.\n",
    "    Best for capturing complex sequential dependencies and seasonal patterns.\n",
    "\n",
    "    Architecture:\n",
    "        Input → Linear → Pos Encoding → Encoder →\n",
    "        Decoder (with cross-attention) → Linear → Output\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        nhead: Number of attention heads\n",
    "        num_encoder_layers: Number of encoder layers\n",
    "        num_decoder_layers: Number of decoder layers\n",
    "        dim_feedforward: Dimension of feedforward network\n",
    "        dropout: Dropout rate\n",
    "        lookback: Input sequence length\n",
    "        forecast_horizon: Output sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 64, nhead: int = 4,\n",
    "                 num_encoder_layers: int = 2, num_decoder_layers: int = 2,\n",
    "                 dim_feedforward: int = 256, dropout: float = 0.1,\n",
    "                 lookback: int = 50, forecast_horizon: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(1, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "\n",
    "        # Learnable decoder input\n",
    "        self.decoder_input = nn.Parameter(torch.randn(1, forecast_horizon, d_model))\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            src: Input tensor of shape [batch, lookback, 1]\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [batch, forecast_horizon, 1]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "\n",
    "        # Project input\n",
    "        src = self.input_projection(src)  # [batch, lookback, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # Prepare decoder input\n",
    "        tgt = self.decoder_input.expand(batch_size, -1, -1)  # [batch, forecast, d_model]\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # Transformer forward\n",
    "        output = self.transformer(src, tgt)  # [batch, forecast, d_model]\n",
    "\n",
    "        # Project to output\n",
    "        output = self.output_projection(output)  # [batch, forecast, 1]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture B: Encoder-Only Transformer (BERT-style).\n",
    "\n",
    "    Uses only encoder stack followed by a forecasting head.\n",
    "    More parameter-efficient than encoder-decoder.\n",
    "\n",
    "    Architecture:\n",
    "        Input → Linear → Pos Encoding → Encoder →\n",
    "        Flatten → MLP → Output\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        nhead: Number of attention heads\n",
    "        num_layers: Number of encoder layers\n",
    "        dim_feedforward: Dimension of feedforward network\n",
    "        dropout: Dropout rate\n",
    "        lookback: Input sequence length\n",
    "        forecast_horizon: Output sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 64, nhead: int = 4, num_layers: int = 2,\n",
    "                 dim_feedforward: int = 256, dropout: float = 0.1,\n",
    "                 lookback: int = 50, forecast_horizon: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(1, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Forecasting head\n",
    "        self.forecasting_head = nn.Sequential(\n",
    "            nn.Linear(d_model * lookback, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, forecast_horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            src: Input tensor of shape [batch, lookback, 1]\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [batch, forecast_horizon, 1]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "\n",
    "        # Project input\n",
    "        src = self.input_projection(src)  # [batch, lookback, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # Encode\n",
    "        encoded = self.encoder(src)  # [batch, lookback, d_model]\n",
    "\n",
    "        # Flatten and forecast\n",
    "        encoded_flat = encoded.reshape(batch_size, -1)  # [batch, lookback*d_model]\n",
    "        output = self.forecasting_head(encoded_flat)  # [batch, forecast]\n",
    "\n",
    "        return output.unsqueeze(-1)  # [batch, forecast, 1]\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture C: Decoder-Only Transformer (GPT-style).\n",
    "\n",
    "    Uses only decoder stack with causal (autoregressive) attention.\n",
    "    Generates predictions step-by-step, using previous predictions as input.\n",
    "\n",
    "    Architecture:\n",
    "        Input → Linear → Pos Encoding →\n",
    "        Autoregressive Decoder (with causal masking) → Linear → Output\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        nhead: Number of attention heads\n",
    "        num_layers: Number of decoder layers\n",
    "        dim_feedforward: Dimension of feedforward network\n",
    "        dropout: Dropout rate\n",
    "        lookback: Input sequence length\n",
    "        forecast_horizon: Output sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 64, nhead: int = 4, num_layers: int = 2,\n",
    "                 dim_feedforward: int = 256, dropout: float = 0.1,\n",
    "                 lookback: int = 50, forecast_horizon: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(1, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Decoder layers with causal mask\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with autoregressive generation.\n",
    "\n",
    "        Args:\n",
    "            src: Input tensor of shape [batch, lookback, 1]\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [batch, forecast_horizon, 1]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        seq_len = src.size(1)\n",
    "\n",
    "        # Project input\n",
    "        src = self.input_projection(src)  # [batch, lookback, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(src.device)\n",
    "\n",
    "        # Autoregressive decoding\n",
    "        memory = src  # Use input as memory\n",
    "\n",
    "        # Initialize output with last input value\n",
    "        current_input = src[:, -1:, :]  # [batch, 1, d_model]\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(self.forecast_horizon):\n",
    "            # Decode next step\n",
    "            output = self.decoder(current_input, memory)  # [batch, 1, d_model]\n",
    "            pred = self.output_projection(output)  # [batch, 1, 1]\n",
    "            predictions.append(pred)\n",
    "\n",
    "            # Use prediction as next input\n",
    "            current_input = self.input_projection(pred)\n",
    "            current_input = self.pos_encoder(current_input)\n",
    "\n",
    "        # Concatenate predictions\n",
    "        output = torch.cat(predictions, dim=1)  # [batch, forecast, 1]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count trainable parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "\n",
    "    Returns:\n",
    "        Number of trainable parameters\n",
    "\n",
    "    Example:\n",
    "        >>> model = EncoderDecoderTransformer()\n",
    "        >>> n_params = count_parameters(model)\n",
    "        >>> print(f\"Parameters: {n_params:,}\")\n",
    "        Parameters: 156,673\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training dataloader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "\n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             loader: DataLoader,\n",
    "             criterion: nn.Module,\n",
    "             device: torch.device) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        loader: Dataloader\n",
    "        criterion: Loss function\n",
    "        device: Device to evaluate on\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (average_loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    return total_loss / len(loader), predictions, targets\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                device: torch.device,\n",
    "                epochs: int = 50,\n",
    "                lr: float = 0.001,\n",
    "                patience: int = 10) -> Tuple[nn.Module, List[float], List[float], float]:\n",
    "    \"\"\"\n",
    "    Complete training loop with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: Training dataloader\n",
    "        val_loader: Validation dataloader\n",
    "        device: Device to train on\n",
    "        epochs: Maximum number of epochs\n",
    "        lr: Learning rate\n",
    "        patience: Early stopping patience\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (trained_model, train_losses, val_losses, training_time)\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses, training_time\n",
    "\n",
    "\n",
    "def compute_metrics(predictions: np.ndarray, targets: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics (MAE and RMSE).\n",
    "\n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        targets: Ground truth values\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (MAE, RMSE)\n",
    "    \"\"\"\n",
    "    mae = np.mean(np.abs(predictions - targets))\n",
    "    rmse = np.sqrt(np.mean((predictions - targets)**2))\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: VISUALIZATION AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_results(results_df: pd.DataFrame, save_path: str = 'ablation_visualization.png'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of ablation study results.\n",
    "\n",
    "    Creates a 2x2 grid showing:\n",
    "    1. MAE comparison across datasets\n",
    "    2. RMSE comparison across datasets\n",
    "    3. Model complexity vs accuracy\n",
    "    4. Training time comparison\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with columns ['Dataset', 'Model', 'MAE', 'RMSE',\n",
    "                                            'Parameters', 'Training Time (s)']\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. MAE comparison across datasets\n",
    "    pivot_mae = results_df.pivot(index='Model', columns='Dataset', values='MAE')\n",
    "    pivot_mae.plot(kind='bar', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('MAE Comparison Across Datasets', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    axes[0, 0].legend(title='Dataset', bbox_to_anchor=(1.05, 1))\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. RMSE comparison\n",
    "    pivot_rmse = results_df.pivot(index='Model', columns='Dataset', values='RMSE')\n",
    "    pivot_rmse.plot(kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('RMSE Comparison Across Datasets', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('RMSE')\n",
    "    axes[0, 1].legend(title='Dataset', bbox_to_anchor=(1.05, 1))\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Parameters vs MAE\n",
    "    for dataset in results_df['Dataset'].unique():\n",
    "        subset = results_df[results_df['Dataset'] == dataset]\n",
    "        axes[1, 0].scatter(subset['Parameters'], subset['MAE'],\n",
    "                          label=dataset, s=100, alpha=0.7)\n",
    "    axes[1, 0].set_title('Model Complexity vs Accuracy', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Number of Parameters')\n",
    "    axes[1, 0].set_ylabel('MAE')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Training time comparison\n",
    "    pivot_time = results_df.pivot(index='Model', columns='Dataset', values='Training Time (s)')\n",
    "    pivot_time.plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Training Time Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "    axes[1, 1].legend(title='Dataset', bbox_to_anchor=(1.05, 1))\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Visualization saved to {save_path}\")\n",
    "\n",
    "\n",
    "def analyze_hypotheses(results_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze the three hypotheses from the ablation study.\n",
    "\n",
    "    H1: Encoder-decoder best for seasonal patterns\n",
    "    H2: Decoder-only struggles with noisy data (error accumulation)\n",
    "    H3: Encoder-only offers good accuracy/efficiency tradeoff\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with experimental results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # H1: Attention Mechanism Hypothesis\n",
    "    print(\"\\n[H1] Encoder-decoder for seasonal patterns\")\n",
    "    seasonal_data = results_df[results_df['Dataset'] == 'Dataset A (Seasonal)']\n",
    "    print(seasonal_data[['Model', 'MAE', 'RMSE']].to_string(index=False))\n",
    "\n",
    "    best_seasonal = seasonal_data.loc[seasonal_data['MAE'].idxmin()]\n",
    "    print(f\"\\n✓ Best model: {best_seasonal['Model']} (MAE: {best_seasonal['MAE']:.4f})\")\n",
    "\n",
    "    if 'Encoder-Decoder' in best_seasonal['Model']:\n",
    "        print(\"→ H1 SUPPORTED: Encoder-Decoder performs best on seasonal data\")\n",
    "    else:\n",
    "        print(f\"→ H1 REJECTED: {best_seasonal['Model']} performs best\")\n",
    "\n",
    "    # H2: Autoregressive Complexity Hypothesis\n",
    "    print(\"\\n[H2] Decoder-only on trend vs noisy data\")\n",
    "    trend_data = results_df[results_df['Dataset'] == 'Dataset B (Trend+Noise)']\n",
    "    mixed_data = results_df[results_df['Dataset'] == 'Dataset C (Mixed)']\n",
    "\n",
    "    decoder_trend = trend_data[trend_data['Model'] == 'Decoder-Only']['MAE'].values[0]\n",
    "    encoder_trend = trend_data[trend_data['Model'] == 'Encoder-Only']['MAE'].values[0]\n",
    "\n",
    "    print(f\"Trend+Noise - Decoder-Only MAE: {decoder_trend:.4f}\")\n",
    "    print(f\"Trend+Noise - Encoder-Only MAE: {encoder_trend:.4f}\")\n",
    "\n",
    "    if decoder_trend > encoder_trend:\n",
    "        print(\"→ H2 SUPPORTED: Decoder-Only worse on noisy data (error accumulation)\")\n",
    "    else:\n",
    "        print(\"→ H2 PARTIALLY SUPPORTED or REJECTED\")\n",
    "\n",
    "    # H3: Architecture Efficiency Hypothesis\n",
    "    print(\"\\n[H3] Encoder-only efficiency vs accuracy\")\n",
    "    for dataset in results_df['Dataset'].unique():\n",
    "        subset = results_df[results_df['Dataset'] == dataset]\n",
    "        enc_dec = subset[subset['Model'] == 'Encoder-Decoder'].iloc[0]\n",
    "        enc_only = subset[subset['Model'] == 'Encoder-Only'].iloc[0]\n",
    "\n",
    "        mae_diff = abs(enc_dec['MAE'] - enc_only['MAE'])\n",
    "        param_ratio = enc_only['Parameters'] / enc_dec['Parameters']\n",
    "\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"  MAE difference: {mae_diff:.4f} ({mae_diff/enc_dec['MAE']*100:.1f}%)\")\n",
    "        print(f\"  Parameter ratio: {param_ratio:.2f}x fewer\")\n",
    "\n",
    "        if mae_diff < 0.1 * enc_dec['MAE']:  # Within 10%\n",
    "            print(f\"  → Encoder-Only competitive with {param_ratio:.2f}x fewer parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Generate Synthetic Datasets\n",
    "\n",
    "We create three different time series to test our models:\n",
    "\n",
    "- **Dataset A (Seasonal)**: Multiple seasonal frequencies with low noise\n",
    "- **Dataset B (Trend+Noise)**: Linear trend with high noise\n",
    "- **Dataset C (Mixed)**: Combination of trend and seasonal patterns\n",
    "\n",
    "**TODO**: Call the helper functions to generate and visualize the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING SYNTHETIC DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Generate the synthetic datasets using the provided helper function\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print dataset statistics\n",
    "for name, series in datasets.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Length: {len(series)}\")\n",
    "    print(f\"  Mean: {np.mean(series):.2f}\")\n",
    "    print(f\"  Std: {np.std(series):.2f}\")\n",
    "    print()\n",
    "\n",
    "# TODO: Visualize the datasets using the provided plotting helper\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check: Sliding Window Dimensions (Easy)\n",
    "\n",
    "Before training models, verify that you understand how the `TimeSeriesDataset` helper structures the data. Using the `datasets` dictionary created above, build a dataset for **Dataset A (Seasonal)** and inspect a single `(input, target)` pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUICK CHECK: WINDOW SHAPES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Instantiate TimeSeriesDataset with Dataset A using lookback=50 and forecast_horizon=10\n",
    "# Retrieve the first sample and print the shapes of x and y\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model Configurations\n",
    "\n",
    "Define the three Transformer architectures we'll compare:\n",
    "\n",
    "### Architecture A: Encoder-Decoder\n",
    "- Full Seq2Seq architecture with cross-attention\n",
    "- Best for capturing complex sequential dependencies\n",
    "- Most parameters\n",
    "\n",
    "### Architecture B: Encoder-Only\n",
    "- BERT-style with forecasting head\n",
    "- More parameter-efficient\n",
    "- Processes entire sequence at once\n",
    "\n",
    "### Architecture C: Decoder-Only\n",
    "- GPT-style with autoregressive generation\n",
    "- Generates predictions step-by-step\n",
    "- May accumulate errors in noisy data\n",
    "\n",
    "**TODO**: Create a dictionary mapping model names to their classes and count parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a dictionary with model names as keys and model classes as values\n",
    "# Format: {'Encoder-Decoder': EncoderDecoderTransformer, ...}\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Model Architectures:\")\n",
    "for name, ModelClass in models_config.items():\n",
    "    model = ModelClass()\n",
    "    # TODO: Count the parameters for this model using the count_parameters function\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    print(f\"  {name}: {n_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Run Ablation Study\n",
    "\n",
    "This is the main experimental loop. For each dataset and each model:\n",
    "\n",
    "1. Split data into train/val/test (70/15/15)\n",
    "2. Normalize using StandardScaler\n",
    "3. Train the model with early stopping\n",
    "4. Evaluate on test set\n",
    "5. Record MAE, RMSE, training time, and parameters\n",
    "\n",
    "**TODO**: Complete the ablation study loop by filling in the missing code.\n",
    "\n",
    "**Note**: This may take 10-15 minutes to complete all 9 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RUNNING ABLATION EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name, series in datasets.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # TODO: Prepare the data using the prepare_data function\n",
    "    # This should return train_loader, val_loader, test_loader, and scaler\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    for model_name, ModelClass in models_config.items():\n",
    "        print(f\"\\n[Training {model_name}]\")\n",
    "        \n",
    "        # TODO: Initialize the model and move it to the device\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        n_params = count_parameters(model)\n",
    "        print(f\"Parameters: {n_params:,}\")\n",
    "        \n",
    "        # TODO: Train the model using train_model function\n",
    "        # Use epochs=50, lr=0.001, patience=10\n",
    "        # This should return: model, train_losses, val_losses, training_time\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # TODO: Evaluate the model on test set using evaluate function\n",
    "        # Use nn.MSELoss() as the criterion\n",
    "        # This should return: test_loss, predictions, targets\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # TODO: Compute MAE and RMSE metrics\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        print(f\"Test MAE: {mae:.4f}, Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"Training time: {training_time:.2f}s\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'Parameters': n_params,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n✓ All experiments complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Results Summary\n",
    "\n",
    "Display the complete results table showing MAE, RMSE, parameters, and training time for each model-dataset combination.\n",
    "\n",
    "**TODO**: Display and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Print the results_df DataFrame as a string (without index)\n",
    "# Hint: Use .to_string(index=False)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('ablation_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved to 'ablation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Visualize Results\n",
    "\n",
    "Create comprehensive visualizations showing:\n",
    "\n",
    "1. **MAE Comparison**: Which model performs best on which dataset\n",
    "2. **RMSE Comparison**: Root Mean Squared Error across experiments\n",
    "3. **Complexity vs Accuracy**: Parameter count vs performance\n",
    "4. **Training Time**: Computational efficiency comparison\n",
    "\n",
    "**TODO**: Call the visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the results using the visualize_results function\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Hypothesis Testing\n",
    "\n",
    "Analyze the three research hypotheses based on experimental results:\n",
    "\n",
    "### H1: Attention Mechanism Hypothesis\n",
    "Does encoder-decoder architecture perform better on seasonal patterns due to its ability to attend bidirectionally?\n",
    "\n",
    "### H2: Autoregressive Complexity Hypothesis\n",
    "Does decoder-only architecture struggle with noisy data due to error accumulation during autoregressive generation?\n",
    "\n",
    "### H3: Architecture Efficiency Hypothesis\n",
    "Does encoder-only architecture provide a good accuracy/efficiency tradeoff with fewer parameters?\n",
    "\n",
    "**TODO**: Call the hypothesis analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the hypotheses using the analyze_hypotheses function\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Key Findings and Conclusions\n",
    "\n",
    "Summarize the main insights from the ablation study.\n",
    "\n",
    "**TODO**: Complete the analysis by finding the best model for each dataset and computing efficiency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Finding 1: Best model per dataset\n",
    "print(\"\\n1. BEST MODEL PER DATASET:\")\n",
    "for dataset in results_df['Dataset'].unique():\n",
    "    # TODO: Filter results_df for the current dataset\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # TODO: Find the row with minimum MAE\n",
    "    # Hint: Use .loc[] with .idxmin() on the 'MAE' column\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    print(f\"\\n   {dataset}:\")\n",
    "    print(f\"     Winner: {best['Model']}\")\n",
    "    print(f\"     MAE: {best['MAE']:.4f}\")\n",
    "    print(f\"     Parameters: {best['Parameters']:,}\")\n",
    "\n",
    "# Finding 2: Parameter efficiency\n",
    "print(\"\\n2. PARAMETER EFFICIENCY:\")\n",
    "# TODO: Group by 'Model', get first 'Parameters' value, and sort\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(param_counts)\n",
    "\n",
    "# Finding 3: Training speed\n",
    "print(\"\\n3. AVERAGE TRAINING TIME:\")\n",
    "# TODO: Group by 'Model', compute mean of 'Training Time (s)', and sort\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(time_avg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - synthetic_datasets.png\")\n",
    "print(\"  - ablation_results.csv\")\n",
    "print(\"  - ablation_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "After completing the assignment, answer the following questions:\n",
    "\n",
    "### Question 1: Hypothesis Validation\n",
    "Based on your results, were all three hypotheses (H1, H2, H3) supported? Explain why or why not using specific numbers from your results.\n",
    "\n",
    "**Your Answer:**\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Question 2: Architecture Selection\n",
    "If you had to choose ONE architecture to use for a new time series dataset with unknown characteristics, which would you choose and why?\n",
    "\n",
    "**Your Answer:**\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Question 3: Real-World Application\n",
    "How would you adapt this study to work with real-world financial time series data (e.g., stock prices)? What additional challenges might you face?\n",
    "\n",
    "**Your Answer:**\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Question 4: Experimental Design\n",
    "What are some limitations of this ablation study? What additional experiments would you run to make the conclusions more robust?\n",
    "\n",
    "**Your Answer:**\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Question 5: Quick Diagnostic\n",
    "If one of the models performs unexpectedly poorly, what is the first diagnostic check you would run, and why?\n",
    "\n",
    "**Your Answer:**\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
