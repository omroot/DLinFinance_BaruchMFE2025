{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: GNN for Emerging Market Alpha Prediction\n",
    "\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will build a **Graph Neural Network (GNN)** to predict **alpha (beta-adjusted returns)** for 10 emerging market equity indices. You will learn how to:\n",
    "\n",
    "1. Compute beta and alpha (CAPM-style risk adjustment)\n",
    "2. Engineer financial features for prediction\n",
    "3. Construct a graph from market correlations\n",
    "4. Implement a Graph Convolutional Network (GCN)\n",
    "5. Interpret model results in a financial context\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand beta-adjusted returns (alpha) and their financial interpretation\n",
    "- Implement rolling statistical calculations for financial features\n",
    "- Build adjacency matrices from correlation/covariance data\n",
    "- Implement GCN message passing layers\n",
    "- Analyze walk-forward backtesting results\n",
    "\n",
    "### Instructions:\n",
    "1. Complete all code sections marked with `# TODO: ...`\n",
    "2. Run all cells to generate results\n",
    "3. Answer the discussion questions in markdown cells\n",
    "4. Submit the completed notebook with all outputs\n",
    "\n",
    "### Grading:\n",
    "- Part 1 (Beta & Alpha): 15 points\n",
    "- Part 2 (Feature Engineering): 25 points\n",
    "- Part 3 (Graph Construction): 15 points\n",
    "- Part 4 (GNN Implementation): 25 points\n",
    "- Part 5 (Results Interpretation): 20 points\n",
    "- **Total: 100 points**\n",
    "- Bonus: 10 points\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup and Data Loading (Provided)\n",
    "\n",
    "Run these cells to import libraries and load the data. **Do not modify.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EM data\n",
    "df = pd.read_csv('equity_indicies.csv', low_memory=False)\n",
    "\n",
    "# Load MSCI World data\n",
    "df_world = pd.read_csv('msci_world.csv', low_memory=False)\n",
    "\n",
    "# Select our 10 EM indices\n",
    "EM_TICKERS = [\n",
    "    'I2CHN003',  # China - Shanghai Composite\n",
    "    'I2IND007',  # India - Nifty\n",
    "    'I4BRA002',  # Brazil - Bovespa\n",
    "    'I2KOR003',  # South Korea - KOSPI\n",
    "    'I2TWN002',  # Taiwan - TAIEX\n",
    "    'I4MEX005',  # Mexico - IPC\n",
    "    'I2IDN003',  # Indonesia - Jakarta\n",
    "    'I2THA002',  # Thailand - SET\n",
    "    'I3TUR002',  # Turkey - ISE 30\n",
    "    'I3RUS002',  # Russia - RTS\n",
    "]\n",
    "\n",
    "EM_NAMES = [\n",
    "    'China', 'India', 'Brazil', 'S.Korea', 'Taiwan',\n",
    "    'Mexico', 'Indonesia', 'Thailand', 'Turkey', 'Russia'\n",
    "]\n",
    "\n",
    "# Filter to our indices\n",
    "df_em = df[df['tic'].isin(EM_TICKERS)].copy()\n",
    "\n",
    "# Parse dates for EM\n",
    "df_em['date'] = pd.to_datetime(df_em['datadate'], format='%m/%d/%y', errors='coerce')\n",
    "df_em.loc[df_em['date'].dt.year > 2050, 'date'] = df_em.loc[df_em['date'].dt.year > 2050, 'date'] - pd.DateOffset(years=100)\n",
    "df_em = df_em[['tic', 'date', 'prccd']].dropna()\n",
    "df_em['prccd'] = pd.to_numeric(df_em['prccd'], errors='coerce')\n",
    "df_em = df_em.dropna()\n",
    "\n",
    "# Parse dates for MSCI World\n",
    "df_world['date'] = pd.to_datetime(df_world['datadate'], format='%m/%d/%y', errors='coerce')\n",
    "df_world.loc[df_world['date'].dt.year > 2050, 'date'] = df_world.loc[df_world['date'].dt.year > 2050, 'date'] - pd.DateOffset(years=100)\n",
    "df_world = df_world[['date', 'prccd']].dropna()\n",
    "df_world['prccd'] = pd.to_numeric(df_world['prccd'], errors='coerce')\n",
    "df_world = df_world.dropna()\n",
    "df_world = df_world.set_index('date').sort_index()\n",
    "df_world.columns = ['MSCI_World']\n",
    "\n",
    "# Pivot EM data to wide format\n",
    "df_pivot = df_em.pivot(index='date', columns='tic', values='prccd')\n",
    "df_pivot = df_pivot[EM_TICKERS]\n",
    "df_pivot.columns = EM_NAMES\n",
    "\n",
    "# Align EM and MSCI World on common dates\n",
    "common_dates = df_pivot.index.intersection(df_world.index)\n",
    "df_pivot = df_pivot.loc[common_dates]\n",
    "df_world = df_world.loc[common_dates]\n",
    "\n",
    "# Drop rows with missing values\n",
    "valid_rows = df_pivot.dropna().index.intersection(df_world.dropna().index)\n",
    "df_pivot = df_pivot.loc[valid_rows].sort_index()\n",
    "df_world = df_world.loc[valid_rows].sort_index()\n",
    "\n",
    "# Compute daily returns\n",
    "returns = df_pivot.pct_change().dropna()\n",
    "world_returns = df_world.pct_change().dropna()\n",
    "\n",
    "# Align returns\n",
    "common_ret_dates = returns.index.intersection(world_returns.index)\n",
    "returns = returns.loc[common_ret_dates]\n",
    "world_returns = world_returns.loc[common_ret_dates]\n",
    "\n",
    "# Get world return as Series\n",
    "world_ret = world_returns.iloc[:, 0]\n",
    "\n",
    "print(f\"EM Price data shape: {df_pivot.shape}\")\n",
    "print(f\"MSCI World data shape: {df_world.shape}\")\n",
    "print(f\"Returns shape: {returns.shape}\")\n",
    "print(f\"Date range: {returns.index.min()} to {returns.index.max()}\")\n",
    "print(f\"\\nCountries: {EM_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Normalized prices\n",
    "ax = axes[0]\n",
    "normalized = df_pivot / df_pivot.iloc[0] * 100\n",
    "for col in normalized.columns:\n",
    "    ax.plot(normalized.index, normalized[col], label=col, alpha=0.8)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Normalized Price (base=100)')\n",
    "ax.set_title('Emerging Market Indices (Normalized)')\n",
    "ax.legend(loc='upper left', ncol=2, fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "ax = axes[1]\n",
    "corr = returns.corr()\n",
    "im = ax.imshow(corr.values, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(EM_NAMES)))\n",
    "ax.set_yticks(range(len(EM_NAMES)))\n",
    "ax.set_xticklabels(EM_NAMES, rotation=45, ha='right')\n",
    "ax.set_yticklabels(EM_NAMES)\n",
    "ax.set_title('Return Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Beta and Alpha (15 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "In the CAPM framework:\n",
    "- **Beta (β)** measures a country's sensitivity to the global market (MSCI World)\n",
    "- **Alpha (α)** is the return unexplained by market exposure: `α = r_country - β × r_world`\n",
    "\n",
    "We compute **rolling beta** using:\n",
    "$$\\beta = \\frac{Cov(r_{country}, r_{world})}{Var(r_{world})}$$\n",
    "\n",
    "This isolates country-specific performance from global market movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Implement Rolling Beta Computation (5 points)\n",
    "\n",
    "Implement a function to compute rolling beta for a country vs the world index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_beta(country_returns: pd.Series, \n",
    "                         world_returns: pd.Series, \n",
    "                         window: int = 63) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute rolling beta of a country vs the world index.\n",
    "    \n",
    "    Beta = Cov(r_country, r_world) / Var(r_world)\n",
    "    \n",
    "    Args:\n",
    "        country_returns: Daily returns of a country index\n",
    "        world_returns: Daily returns of MSCI World index\n",
    "        window: Rolling window size (default 63 = ~3 months)\n",
    "    \n",
    "    Returns:\n",
    "        Series of rolling beta values\n",
    "    \"\"\"\n",
    "    # TODO: Compute rolling covariance between country and world returns\n",
    "    # Hint: Use .rolling(window).cov(other_series)\n",
    "    rolling_cov = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute rolling variance of world returns\n",
    "    # Hint: Use .rolling(window).var()\n",
    "    rolling_var = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute beta = cov / var (add small epsilon to avoid division by zero)\n",
    "    beta = None  # YOUR CODE HERE\n",
    "    \n",
    "    return beta\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_beta = compute_rolling_beta(returns['Brazil'], world_ret, window=63)\n",
    "test_beta_clean = test_beta.dropna()\n",
    "\n",
    "assert len(test_beta_clean) > 0, \"Beta should have non-NaN values after window period\"\n",
    "assert test_beta_clean.mean() > 0, \"Brazil beta should be positive on average\"\n",
    "assert 0.5 < test_beta_clean.mean() < 2.0, f\"Brazil beta mean ({test_beta_clean.mean():.2f}) seems unreasonable\"\n",
    "\n",
    "print(f\"✓ Rolling beta tests passed!\")\n",
    "print(f\"Brazil beta: mean={test_beta_clean.mean():.3f}, std={test_beta_clean.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Implement Alpha Computation (5 points)\n",
    "\n",
    "Implement a function to compute alpha (beta-adjusted return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(country_forward_returns: pd.Series,\n",
    "                  world_forward_returns: pd.Series,\n",
    "                  beta: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute alpha (beta-adjusted return).\n",
    "    \n",
    "    Alpha = r_country - beta * r_world\n",
    "    \n",
    "    Args:\n",
    "        country_forward_returns: Forward returns of country\n",
    "        world_forward_returns: Forward returns of world index\n",
    "        beta: Rolling beta of country vs world\n",
    "    \n",
    "    Returns:\n",
    "        Series of alpha values\n",
    "    \"\"\"\n",
    "    # TODO: Compute alpha = country_return - beta * world_return\n",
    "    alpha = None  # YOUR CODE HERE\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# Compute 21-day forward returns\n",
    "FORWARD_DAYS = 21\n",
    "forward_returns = df_pivot.pct_change(periods=FORWARD_DAYS).shift(-FORWARD_DAYS)\n",
    "world_forward = df_world.pct_change(periods=FORWARD_DAYS).shift(-FORWARD_DAYS).iloc[:, 0]\n",
    "\n",
    "# Compute beta for Brazil\n",
    "brazil_beta = compute_rolling_beta(returns['Brazil'], world_ret, window=63)\n",
    "\n",
    "# Compute alpha\n",
    "brazil_alpha = compute_alpha(forward_returns['Brazil'], world_forward, brazil_beta)\n",
    "brazil_alpha_clean = brazil_alpha.dropna()\n",
    "\n",
    "assert len(brazil_alpha_clean) > 0, \"Alpha should have non-NaN values\"\n",
    "# Alpha should have lower std than total return (market risk removed)\n",
    "total_return_std = forward_returns['Brazil'].dropna().std()\n",
    "alpha_std = brazil_alpha_clean.std()\n",
    "\n",
    "print(f\"✓ Alpha computation tests passed!\")\n",
    "print(f\"Brazil 21-day total return std: {total_return_std:.4f}\")\n",
    "print(f\"Brazil 21-day alpha std: {alpha_std:.4f}\")\n",
    "print(f\"Alpha std is {alpha_std/total_return_std:.1%} of total return std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Discussion Question (5 points)\n",
    "\n",
    "**Why might predicting alpha be more useful than predicting total returns for a portfolio manager?**\n",
    "\n",
    "Consider:\n",
    "- What does alpha capture vs total return?\n",
    "- How would a long-short strategy use alpha predictions?\n",
    "- Why might alpha be \"harder\" to predict than total return?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Betas and Alphas for All Countries (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling betas for all countries\n",
    "BETA_WINDOW = 63\n",
    "rolling_betas = pd.DataFrame(index=returns.index, columns=EM_NAMES)\n",
    "\n",
    "for col in EM_NAMES:\n",
    "    rolling_betas[col] = compute_rolling_beta(returns[col], world_ret, BETA_WINDOW)\n",
    "\n",
    "# Compute forward alphas for all countries\n",
    "forward_alpha = pd.DataFrame(index=forward_returns.index, columns=EM_NAMES)\n",
    "\n",
    "for col in EM_NAMES:\n",
    "    forward_alpha[col] = compute_alpha(forward_returns[col], world_forward, rolling_betas[col])\n",
    "\n",
    "print(\"Rolling Beta Statistics:\")\n",
    "print(rolling_betas.dropna().describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Visualize betas\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for col in EM_NAMES:\n",
    "    ax.plot(rolling_betas[col].dropna(), label=col, alpha=0.7)\n",
    "ax.axhline(y=1, color='black', linestyle='--', label='Beta=1')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Beta to MSCI World')\n",
    "ax.set_title('Rolling Beta (63-day) for Each Country')\n",
    "ax.legend(loc='upper right', ncol=2, fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Feature Engineering (25 points)\n",
    "\n",
    "We will create features to predict 21-day forward alpha. Features include:\n",
    "- **Momentum features**: Past returns at different horizons\n",
    "- **World features**: MSCI World momentum, volatility, beta\n",
    "- **Technical indicators**: RSI, volatility, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Implement Momentum Features (10 points)\n",
    "\n",
    "Implement momentum features for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_momentum_features(prices: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute momentum features for each country.\n",
    "    \n",
    "    Features to compute:\n",
    "    - mom_21: 21-day momentum (return over past 21 days)\n",
    "    - mom_63: 63-day momentum (return over past quarter)\n",
    "    - vol_21: 21-day volatility (std of daily returns)\n",
    "    \n",
    "    Args:\n",
    "        prices: DataFrame of prices (columns = countries)\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_name -> DataFrame with same columns as prices\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    daily_returns = prices.pct_change()\n",
    "    \n",
    "    # TODO: Compute 21-day momentum for each country\n",
    "    # Hint: Use prices.pct_change(periods=21)\n",
    "    features['mom_21'] = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute 63-day momentum for each country\n",
    "    features['mom_63'] = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute 21-day rolling volatility\n",
    "    # Hint: Use daily_returns.rolling(21).std()\n",
    "    features['vol_21'] = None  # YOUR CODE HERE\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "momentum_features = compute_momentum_features(df_pivot)\n",
    "\n",
    "assert 'mom_21' in momentum_features, \"Missing mom_21 feature\"\n",
    "assert 'mom_63' in momentum_features, \"Missing mom_63 feature\"\n",
    "assert 'vol_21' in momentum_features, \"Missing vol_21 feature\"\n",
    "assert momentum_features['mom_21'].shape == df_pivot.shape, \"mom_21 wrong shape\"\n",
    "assert momentum_features['vol_21'].dropna().min().min() >= 0, \"Volatility should be non-negative\"\n",
    "\n",
    "print(\"✓ Momentum features tests passed!\")\n",
    "print(f\"\\nSample mom_21 statistics:\")\n",
    "print(momentum_features['mom_21'].dropna().describe().T[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement World Features (10 points)\n",
    "\n",
    "Implement features based on MSCI World index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_world_features(world_prices: pd.DataFrame,\n",
    "                           country_returns: pd.DataFrame,\n",
    "                           world_returns: pd.Series) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute features based on MSCI World index.\n",
    "    \n",
    "    Features to compute:\n",
    "    - world_mom_21: 21-day MSCI World momentum (same for all countries)\n",
    "    - world_vol_21: 21-day MSCI World volatility (same for all countries)\n",
    "    - beta_to_world: Rolling 63-day beta to MSCI World (different per country)\n",
    "    \n",
    "    Args:\n",
    "        world_prices: DataFrame with MSCI World prices\n",
    "        country_returns: DataFrame of daily returns (columns = countries)\n",
    "        world_returns: Series of MSCI World daily returns\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_name -> DataFrame with same columns as country_returns\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    countries = country_returns.columns\n",
    "    world_ret_series = world_returns if isinstance(world_returns, pd.Series) else world_returns.iloc[:, 0]\n",
    "    world_price_series = world_prices.iloc[:, 0] if isinstance(world_prices, pd.DataFrame) else world_prices\n",
    "    \n",
    "    # TODO: Compute 21-day MSCI World momentum\n",
    "    # This is the same value for all countries on each date\n",
    "    # Hint: Compute for world, then broadcast to all countries\n",
    "    world_mom = None  # YOUR CODE HERE - compute pct_change(21) for world\n",
    "    \n",
    "    # Create DataFrame with same value for all countries\n",
    "    features['world_mom_21'] = pd.DataFrame(\n",
    "        {col: world_mom for col in countries},\n",
    "        index=country_returns.index\n",
    "    )\n",
    "    \n",
    "    # TODO: Compute 21-day MSCI World volatility\n",
    "    world_vol = None  # YOUR CODE HERE - compute rolling(21).std() for world returns\n",
    "    \n",
    "    features['world_vol_21'] = pd.DataFrame(\n",
    "        {col: world_vol for col in countries},\n",
    "        index=country_returns.index\n",
    "    )\n",
    "    \n",
    "    # TODO: Compute rolling beta to world for each country\n",
    "    # Use the compute_rolling_beta function from Part 1\n",
    "    beta_df = pd.DataFrame(index=country_returns.index, columns=countries)\n",
    "    for col in countries:\n",
    "        # YOUR CODE HERE - compute rolling beta for each country\n",
    "        beta_df[col] = None  # Use compute_rolling_beta\n",
    "    \n",
    "    features['beta_to_world'] = beta_df\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "world_features = compute_world_features(df_world, returns, world_ret)\n",
    "\n",
    "assert 'world_mom_21' in world_features, \"Missing world_mom_21 feature\"\n",
    "assert 'world_vol_21' in world_features, \"Missing world_vol_21 feature\"\n",
    "assert 'beta_to_world' in world_features, \"Missing beta_to_world feature\"\n",
    "\n",
    "# World features should be same across countries\n",
    "world_mom_std = world_features['world_mom_21'].std(axis=1).dropna()\n",
    "assert (world_mom_std < 1e-10).all(), \"world_mom_21 should be same for all countries\"\n",
    "\n",
    "# Beta should differ by country\n",
    "beta_var = world_features['beta_to_world'].var(axis=1).dropna()\n",
    "assert beta_var.mean() > 0.01, \"beta_to_world should differ across countries\"\n",
    "\n",
    "print(\"✓ World features tests passed!\")\n",
    "print(f\"\\nSample beta_to_world statistics:\")\n",
    "print(world_features['beta_to_world'].dropna().describe().T[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Feature Interpretation Question (5 points)\n",
    "\n",
    "Look at the beta statistics above.\n",
    "\n",
    "**Questions:**\n",
    "1. Which countries have the highest average beta to MSCI World?\n",
    "2. Which countries have the lowest average beta?\n",
    "3. What does high beta mean for a country's risk profile?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Features (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(prices: pd.Series, window: int = 14) -> pd.Series:\n",
    "    \"\"\"Compute Relative Strength Index (provided).\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def compute_all_features(prices: pd.DataFrame, \n",
    "                         returns: pd.DataFrame,\n",
    "                         world_prices: pd.DataFrame,\n",
    "                         world_returns: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute all features for all countries.\n",
    "    Returns DataFrame with MultiIndex columns: (country, feature)\n",
    "    \"\"\"\n",
    "    # Get momentum and world features from your implementations\n",
    "    mom_features = compute_momentum_features(prices)\n",
    "    world_feats = compute_world_features(world_prices, returns, world_returns)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for col in EM_NAMES:\n",
    "        feat = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        # Momentum features\n",
    "        feat['mom_21'] = mom_features['mom_21'][col]\n",
    "        feat['mom_63'] = mom_features['mom_63'][col]\n",
    "        feat['vol_21'] = mom_features['vol_21'][col]\n",
    "        \n",
    "        # World features\n",
    "        feat['world_mom_21'] = world_feats['world_mom_21'][col]\n",
    "        feat['world_vol_21'] = world_feats['world_vol_21'][col]\n",
    "        feat['beta_to_world'] = world_feats['beta_to_world'][col]\n",
    "        \n",
    "        # Additional technical features\n",
    "        feat['rsi_14'] = compute_rsi(prices[col], 14)\n",
    "        feat['ma_ratio'] = prices[col] / prices[col].rolling(50).mean()\n",
    "        \n",
    "        # Add column prefix\n",
    "        feat.columns = pd.MultiIndex.from_product([[col], feat.columns])\n",
    "        features_list.append(feat)\n",
    "    \n",
    "    all_features = pd.concat(features_list, axis=1)\n",
    "    return all_features.dropna()\n",
    "\n",
    "\n",
    "# Compute all features\n",
    "features = compute_all_features(df_pivot, returns, df_world, world_ret)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = features.columns.get_level_values(1).unique().tolist()\n",
    "N_FEATURES = len(feature_names)\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Number of features per country: {N_FEATURES}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Date range: {features.index.min()} to {features.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Graph Construction (15 points)\n",
    "\n",
    "We construct a graph where:\n",
    "- **Nodes** = Countries (10 nodes)\n",
    "- **Edge weights** = Correlation between country returns\n",
    "\n",
    "The intuition: correlated markets might share information for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Implement Correlation-Based Adjacency Matrix (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_adjacency(returns_window: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute adjacency matrix from return correlations.\n",
    "    \n",
    "    Steps:\n",
    "    1. Compute correlation matrix of returns\n",
    "    2. Take absolute value (both positive and negative correlation = linkage)\n",
    "    3. Set diagonal to 0 (no self-loops)\n",
    "    \n",
    "    Args:\n",
    "        returns_window: DataFrame of returns for a time window\n",
    "    \n",
    "    Returns:\n",
    "        Adjacency matrix (n_countries x n_countries)\n",
    "    \"\"\"\n",
    "    # TODO: Compute correlation matrix\n",
    "    # Hint: Use returns_window.corr()\n",
    "    corr = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Take absolute value (both positive and negative correlations matter)\n",
    "    adj = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Remove self-loops (set diagonal to 0)\n",
    "    # Hint: Use np.fill_diagonal(adj, 0)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return adj\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_window = returns.iloc[-63:]  # Last 63 days\n",
    "test_adj = compute_correlation_adjacency(test_window)\n",
    "\n",
    "assert test_adj.shape == (10, 10), f\"Adjacency shape should be (10,10), got {test_adj.shape}\"\n",
    "assert np.allclose(test_adj, test_adj.T), \"Adjacency should be symmetric\"\n",
    "assert np.allclose(np.diag(test_adj), 0), \"Diagonal should be zero (no self-loops)\"\n",
    "assert test_adj.min() >= 0, \"Adjacency values should be non-negative\"\n",
    "assert test_adj.max() <= 1, \"Adjacency values should be <= 1\"\n",
    "\n",
    "print(\"✓ Adjacency matrix tests passed!\")\n",
    "print(f\"Average edge weight: {test_adj.mean():.3f}\")\n",
    "print(f\"Max edge weight: {test_adj.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the adjacency matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Correlation matrix (with sign)\n",
    "ax = axes[0]\n",
    "corr_matrix = test_window.corr().values\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(EM_NAMES)))\n",
    "ax.set_yticks(range(len(EM_NAMES)))\n",
    "ax.set_xticklabels(EM_NAMES, rotation=45, ha='right')\n",
    "ax.set_yticklabels(EM_NAMES)\n",
    "ax.set_title('Correlation Matrix (with sign)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Adjacency matrix (absolute, no diagonal)\n",
    "ax = axes[1]\n",
    "im = ax.imshow(test_adj, cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(EM_NAMES)))\n",
    "ax.set_yticks(range(len(EM_NAMES)))\n",
    "ax.set_xticklabels(EM_NAMES, rotation=45, ha='right')\n",
    "ax.set_yticklabels(EM_NAMES)\n",
    "ax.set_title('Adjacency Matrix (|correlation|, no self-loops)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Discussion Question (5 points)\n",
    "\n",
    "Look at the adjacency matrix visualization above.\n",
    "\n",
    "**Questions:**\n",
    "1. Which pairs of countries have the strongest connections (highest correlation)?\n",
    "2. Are there regional patterns (e.g., Asian markets correlating with each other)?\n",
    "3. How might the GNN use these connections for prediction?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: GNN Implementation (25 points)\n",
    "\n",
    "We implement a Graph Convolutional Network (GCN) that aggregates information from connected countries to improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Implement GCN Layer (15 points)\n",
    "\n",
    "The GCN layer performs message passing:\n",
    "$$H' = \\tilde{D}^{-1} \\tilde{A} H W$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{A} = A + I$ (adjacency with self-loops)\n",
    "- $\\tilde{D}$ = degree matrix of $\\tilde{A}$\n",
    "- $H$ = node features\n",
    "- $W$ = learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer.\n",
    "    \n",
    "    Performs: H' = activation(D^{-1} * A_hat * H * W)\n",
    "    where A_hat = A + I (adjacency with self-loops)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of GCN layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features (batch_size, n_nodes, in_features)\n",
    "            adj: Adjacency matrix (batch_size, n_nodes, n_nodes)\n",
    "        \n",
    "        Returns:\n",
    "            Updated node features (batch_size, n_nodes, out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Step 1 - Add self-loops to adjacency matrix\n",
    "        # Hint: adj_with_self = adj + torch.eye(n_nodes)\n",
    "        # Note: adj.shape[-1] gives n_nodes, use .to(adj.device) for GPU compatibility\n",
    "        n_nodes = adj.shape[-1]\n",
    "        adj_with_self = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Step 2 - Compute degree and normalize\n",
    "        # degree = sum of each row, then D^{-1} * A\n",
    "        # Hint: deg = adj_with_self.sum(dim=-1, keepdim=True)\n",
    "        #       adj_norm = adj_with_self / (deg + 1e-10)\n",
    "        deg = None  # YOUR CODE HERE\n",
    "        adj_norm = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Step 3 - Message passing: aggregate neighbor features\n",
    "        # Hint: Use torch.bmm(adj_norm, x) for batch matrix multiplication\n",
    "        x_agg = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Step 4 - Apply linear transformation (provided)\n",
    "        out = self.linear(x_agg)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_layer = GCNLayer(in_features=8, out_features=16)\n",
    "test_x = torch.randn(2, 10, 8)  # batch=2, nodes=10, features=8\n",
    "test_adj = torch.rand(2, 10, 10)  # random adjacency\n",
    "test_adj = (test_adj + test_adj.transpose(-1, -2)) / 2  # make symmetric\n",
    "\n",
    "test_out = test_layer(test_x, test_adj)\n",
    "\n",
    "assert test_out.shape == (2, 10, 16), f\"Output shape should be (2,10,16), got {test_out.shape}\"\n",
    "assert not torch.isnan(test_out).any(), \"Output contains NaN\"\n",
    "\n",
    "print(\"✓ GCNLayer tests passed!\")\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"Output shape: {test_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Complete the GCN Model (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network for node-level regression.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: (batch, n_nodes, n_features)\n",
    "    - GCN Layer 1: n_features -> hidden_dim\n",
    "    - ReLU + Dropout\n",
    "    - GCN Layer 2: hidden_dim -> hidden_dim\n",
    "    - ReLU + Dropout  \n",
    "    - Linear: hidden_dim -> 1 (output per node)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32, dropout: float = 0.3):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # TODO: Create two GCN layers\n",
    "        self.gcn1 = None  # YOUR CODE HERE - GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = None  # YOUR CODE HERE - GCNLayer(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer (provided)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features (batch, n_nodes, input_dim)\n",
    "            adj: Adjacency matrix (batch, n_nodes, n_nodes)\n",
    "        \n",
    "        Returns:\n",
    "            Predictions (batch, n_nodes)\n",
    "        \"\"\"\n",
    "        # TODO: Apply first GCN layer + ReLU + dropout\n",
    "        x = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply second GCN layer + ReLU + dropout\n",
    "        x = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Output layer (provided)\n",
    "        out = self.fc_out(x).squeeze(-1)  # (batch, n_nodes)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_model = GCN(input_dim=8, hidden_dim=32, dropout=0.3)\n",
    "test_x = torch.randn(4, 10, 8)  # batch=4, nodes=10, features=8\n",
    "test_adj = torch.rand(4, 10, 10)\n",
    "test_adj = (test_adj + test_adj.transpose(-1, -2)) / 2\n",
    "\n",
    "test_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = test_model(test_x, test_adj)\n",
    "\n",
    "assert test_out.shape == (4, 10), f\"Output shape should be (4,10), got {test_out.shape}\"\n",
    "assert not torch.isnan(test_out).any(), \"Output contains NaN\"\n",
    "\n",
    "print(\"✓ GCN model tests passed!\")\n",
    "print(f\"Model parameters: {test_model.count_parameters()}\")\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"Output shape: {test_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Baseline (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP baseline - processes each node independently (no graph structure).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, adj=None):\n",
    "        # adj is ignored - MLP doesn't use graph structure\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x).squeeze(-1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"MLP parameters: {MLP(N_FEATURES, 32).count_parameters()}\")\n",
    "print(f\"GCN parameters: {GCN(N_FEATURES, 32).count_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training and Results (Provided + 20 points for interpretation)\n",
    "\n",
    "We use **walk-forward validation**: train on past data, predict future alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for walk-forward evaluation\n",
    "\n",
    "# Filter to 2010-2019 period\n",
    "START_DATE = '2010-01-01'\n",
    "END_DATE = '2019-12-31'\n",
    "\n",
    "# Align features and target\n",
    "common_dates = features.index.intersection(forward_alpha.dropna().index)\n",
    "common_dates = common_dates[(common_dates >= START_DATE) & (common_dates <= END_DATE)]\n",
    "\n",
    "features_aligned = features.loc[common_dates]\n",
    "target_alpha = forward_alpha.loc[common_dates]\n",
    "\n",
    "print(f\"Data period: {common_dates.min()} to {common_dates.max()}\")\n",
    "print(f\"Number of samples: {len(common_dates)}\")\n",
    "print(f\"Features per country: {N_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward configuration\n",
    "TRAIN_WINDOW = 504  # 2 years\n",
    "TEST_WINDOW = 21    # 1 month\n",
    "BUFFER = 21         # 1 month gap\n",
    "STEP = 21           # Retrain monthly\n",
    "COV_WINDOW = 63     # For adjacency matrix\n",
    "\n",
    "# Training settings\n",
    "HIDDEN_DIM = 32\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "DROPOUT = 0.3\n",
    "\n",
    "def prepare_fold_data(features, returns, target, train_start, train_end, test_start, test_end):\n",
    "    \"\"\"Prepare data for one walk-forward fold.\"\"\"\n",
    "    all_dates = features.index.tolist()\n",
    "    train_dates = all_dates[train_start:train_end]\n",
    "    test_dates = all_dates[test_start:test_end]\n",
    "    \n",
    "    n_train, n_test = len(train_dates), len(test_dates)\n",
    "    n_nodes, n_features = len(EM_NAMES), N_FEATURES\n",
    "    \n",
    "    # Prepare arrays\n",
    "    X_train = np.zeros((n_train, n_nodes, n_features))\n",
    "    y_train = np.zeros((n_train, n_nodes))\n",
    "    adj_train = np.zeros((n_train, n_nodes, n_nodes))\n",
    "    \n",
    "    X_test = np.zeros((n_test, n_nodes, n_features))\n",
    "    y_test = np.zeros((n_test, n_nodes))\n",
    "    adj_test = np.zeros((n_test, n_nodes, n_nodes))\n",
    "    \n",
    "    for i, date in enumerate(train_dates):\n",
    "        for j, name in enumerate(EM_NAMES):\n",
    "            X_train[i, j, :] = features.loc[date, name].values\n",
    "        y_train[i, :] = target.loc[date].values\n",
    "        date_loc = returns.index.get_loc(date)\n",
    "        if date_loc >= COV_WINDOW:\n",
    "            adj_train[i] = compute_correlation_adjacency(returns.iloc[date_loc-COV_WINDOW:date_loc])\n",
    "    \n",
    "    for i, date in enumerate(test_dates):\n",
    "        for j, name in enumerate(EM_NAMES):\n",
    "            X_test[i, j, :] = features.loc[date, name].values\n",
    "        y_test[i, :] = target.loc[date].values\n",
    "        date_loc = returns.index.get_loc(date)\n",
    "        if date_loc >= COV_WINDOW:\n",
    "            adj_test[i] = compute_correlation_adjacency(returns.iloc[date_loc-COV_WINDOW:date_loc])\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train, 'adj_train': adj_train,\n",
    "        'X_test': X_test, 'y_test': y_test, 'adj_test': adj_test,\n",
    "        'train_dates': train_dates, 'test_dates': test_dates\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, X, y, adj, n_epochs=30, lr=0.001, batch_size=32):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    adj_t = torch.tensor(adj, dtype=torch.float32)\n",
    "    \n",
    "    # Winsorize targets\n",
    "    y_flat = y_t.flatten()\n",
    "    low, high = torch.quantile(y_flat, 0.02), torch.quantile(y_flat, 0.98)\n",
    "    y_t = torch.clamp(y_t, low, high)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        perm = torch.randperm(len(X))\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_t[idx], adj_t[idx])\n",
    "            loss = criterion(pred, y_t[idx])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, adj):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X, dtype=torch.float32)\n",
    "        adj_t = torch.tensor(adj, dtype=torch.float32)\n",
    "        pred = model(X_t, adj_t).numpy()\n",
    "    return pred, y\n",
    "\n",
    "print(\"Training utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walk-forward folds\n",
    "n_total = len(features_aligned)\n",
    "all_dates = features_aligned.index.tolist()\n",
    "\n",
    "folds = []\n",
    "fold_idx = TRAIN_WINDOW\n",
    "\n",
    "while fold_idx + BUFFER + TEST_WINDOW <= n_total:\n",
    "    folds.append({\n",
    "        'train_start': fold_idx - TRAIN_WINDOW,\n",
    "        'train_end': fold_idx,\n",
    "        'test_start': fold_idx + BUFFER,\n",
    "        'test_end': min(fold_idx + BUFFER + TEST_WINDOW, n_total)\n",
    "    })\n",
    "    fold_idx += STEP\n",
    "\n",
    "print(f\"Number of walk-forward folds: {len(folds)}\")\n",
    "print(f\"First fold: train {all_dates[folds[0]['train_start']]} to {all_dates[folds[0]['train_end']-1]}\")\n",
    "print(f\"           test  {all_dates[folds[0]['test_start']]} to {all_dates[folds[0]['test_end']-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run walk-forward evaluation\n",
    "model_names = ['Ridge', 'MLP', 'GCN']\n",
    "results = {name: {'y_true': [], 'y_pred': []} for name in model_names}\n",
    "\n",
    "print(\"Running Walk-Forward Evaluation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Limit to first 20 folds for speed (can increase for full evaluation)\n",
    "MAX_FOLDS = 20\n",
    "\n",
    "for fold_num, fold in enumerate(folds[:MAX_FOLDS]):\n",
    "    if fold_num % 5 == 0:\n",
    "        print(f\"Fold {fold_num + 1}/{min(len(folds), MAX_FOLDS)}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    data = prepare_fold_data(\n",
    "        features_aligned, returns, target_alpha,\n",
    "        fold['train_start'], fold['train_end'],\n",
    "        fold['test_start'], fold['test_end']\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = data['X_train'].reshape(-1, N_FEATURES)\n",
    "    X_test_flat = data['X_test'].reshape(-1, N_FEATURES)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat).reshape(data['X_train'].shape)\n",
    "    X_test_scaled = scaler.transform(X_test_flat).reshape(data['X_test'].shape)\n",
    "    \n",
    "    # Ridge baseline\n",
    "    ridge = Ridge(alpha=10.0)\n",
    "    ridge.fit(X_train_scaled.reshape(-1, N_FEATURES), data['y_train'].flatten())\n",
    "    ridge_pred = ridge.predict(X_test_scaled.reshape(-1, N_FEATURES)).reshape(data['y_test'].shape)\n",
    "    results['Ridge']['y_pred'].append(ridge_pred)\n",
    "    results['Ridge']['y_true'].append(data['y_test'])\n",
    "    \n",
    "    # MLP\n",
    "    torch.manual_seed(SEED)\n",
    "    mlp = MLP(N_FEATURES, HIDDEN_DIM, DROPOUT)\n",
    "    train_model(mlp, X_train_scaled, data['y_train'], data['adj_train'], N_EPOCHS, LR, BATCH_SIZE)\n",
    "    mlp_pred, _ = evaluate_model(mlp, X_test_scaled, data['y_test'], data['adj_test'])\n",
    "    results['MLP']['y_pred'].append(mlp_pred)\n",
    "    results['MLP']['y_true'].append(data['y_test'])\n",
    "    \n",
    "    # GCN\n",
    "    torch.manual_seed(SEED)\n",
    "    gcn = GCN(N_FEATURES, HIDDEN_DIM, DROPOUT)\n",
    "    train_model(gcn, X_train_scaled, data['y_train'], data['adj_train'], N_EPOCHS, LR, BATCH_SIZE)\n",
    "    gcn_pred, _ = evaluate_model(gcn, X_test_scaled, data['y_test'], data['adj_test'])\n",
    "    results['GCN']['y_pred'].append(gcn_pred)\n",
    "    results['GCN']['y_true'].append(data['y_test'])\n",
    "\n",
    "# Concatenate results\n",
    "for name in model_names:\n",
    "    results[name]['y_true'] = np.vstack(results[name]['y_true'])\n",
    "    results[name]['y_pred'] = np.vstack(results[name]['y_pred'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Walk-forward evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY: Predicting 21-day Alpha\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<10} {'Test R2':>12} {'Test Corr':>12} {'Test RMSE':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "metrics = {}\n",
    "for name in model_names:\n",
    "    y_true = results[name]['y_true'].flatten()\n",
    "    y_pred = results[name]['y_pred'].flatten()\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    metrics[name] = {'r2': r2, 'corr': corr, 'rmse': rmse}\n",
    "    print(f\"{name:<10} {r2:>12.4f} {corr:>12.4f} {rmse:>12.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-country correlation\n",
    "print(\"\\nPer-Country Test Correlation:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Country':<12}\", end=\"\")\n",
    "for name in model_names:\n",
    "    print(f\"{name:>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\"*60)\n",
    "\n",
    "per_country_corr = {name: [] for name in model_names}\n",
    "for i, country in enumerate(EM_NAMES):\n",
    "    print(f\"{country:<12}\", end=\"\")\n",
    "    for name in model_names:\n",
    "        y_true = results[name]['y_true'][:, i]\n",
    "        y_pred = results[name]['y_pred'][:, i]\n",
    "        corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "        per_country_corr[name].append(corr)\n",
    "        print(f\"{corr:>12.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Average':<12}\", end=\"\")\n",
    "for name in model_names:\n",
    "    print(f\"{np.mean(per_country_corr[name]):>12.4f}\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "colors = {'Ridge': 'gray', 'MLP': 'blue', 'GCN': 'green'}\n",
    "\n",
    "for idx, name in enumerate(model_names):\n",
    "    ax = axes[idx]\n",
    "    y_true = results[name]['y_true'].flatten()\n",
    "    y_pred = results[name]['y_pred'].flatten()\n",
    "    \n",
    "    ax.scatter(y_pred, y_true, alpha=0.2, s=10, c=colors[name])\n",
    "    \n",
    "    # Add diagonal\n",
    "    lims = [min(y_pred.min(), y_true.min()), max(y_pred.max(), y_true.max())]\n",
    "    ax.plot(lims, lims, 'k--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Alpha')\n",
    "    ax.set_ylabel('Actual Alpha')\n",
    "    ax.set_title(f'{name}\\nCorr={metrics[name][\"corr\"]:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Predicted vs Actual Alpha', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Model Comparison (7 points)\n",
    "\n",
    "Based on the results above:\n",
    "\n",
    "1. Which model performed best in terms of correlation with actual alpha?\n",
    "2. Did the GCN outperform the MLP? Why or why not might this be the case?\n",
    "3. Why are R² values negative? What does this mean?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Per-Country Analysis (7 points)\n",
    "\n",
    "Look at the per-country correlation table.\n",
    "\n",
    "1. Which countries were easiest to predict (highest correlation)?\n",
    "2. Which countries were hardest to predict?\n",
    "3. Can you hypothesize why some countries might be more predictable than others?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Financial Interpretation (6 points)\n",
    "\n",
    "Consider using these predictions in a real trading strategy.\n",
    "\n",
    "1. How might you use alpha predictions to construct a portfolio?\n",
    "2. What are the limitations of this approach?\n",
    "3. What additional data or features might improve predictions?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Graph Attention Network (10 points)\n",
    "\n",
    "Implement a simple attention mechanism for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    BONUS: Implement a Graph Attention Layer.\n",
    "    \n",
    "    Instead of using fixed adjacency weights, learn attention weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features, out_features)\n",
    "        self.a = nn.Linear(2 * out_features, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO (BONUS): Implement attention-based message passing.\n",
    "        \n",
    "        Steps:\n",
    "        1. Transform features: h = W @ x\n",
    "        2. Compute attention scores for each pair (i,j)\n",
    "        3. Apply softmax over neighbors (masked by adjacency)\n",
    "        4. Aggregate: weighted sum of neighbor features\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE (BONUS)\n",
    "        pass\n",
    "\n",
    "\n",
    "# If you implement the bonus, test it here\n",
    "# test_gat_layer = GraphAttentionLayer(8, 16)\n",
    "# test_out = test_gat_layer(test_x, test_adj)\n",
    "# print(f\"GAT layer output shape: {test_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] All TODO sections are completed\n",
    "- [ ] All test assertions pass (you see ✓ messages)\n",
    "- [ ] All discussion questions are answered\n",
    "- [ ] Your name and student ID are at the top\n",
    "- [ ] The notebook is saved with all outputs visible\n",
    "\n",
    "**Submit**: Upload this completed notebook (.ipynb file) to the course portal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
