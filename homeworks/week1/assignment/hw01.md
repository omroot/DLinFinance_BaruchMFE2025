# Homework 1: Deep Learning Foundations

**Course**: Deep Learning for Finance
**Due Date**: October 27, 2025 (11:59 PM)
**Total Points**: 100 points

---

## Assignment Overview

This homework consists of three parts that explore fundamental concepts in deep learning theory and practice. You will investigate the approximation power of neural networks, understand the Universal Approximation Theorem, and gain hands-on experience with language modeling.

---

## Part 1: Approximation Power of Deep vs Shallow Networks (35 points)

**Notebook**: `hw01_part1_assignment.ipynb`

### Learning Objectives
- Understand compositional functions and their hierarchical structure
- Implement and compare shallow vs deep neural network architectures
- Analyze parameter efficiency across different architectures
- Investigate why depth matters for certain types of functions

### What You'll Do
1. Implement a hierarchical compositional function (3-level binary tree structure)
2. Build shallow neural networks (1 hidden layer) with varying widths
3. Build deep neural networks (3 hidden layers) with varying configurations
4. Train both types of networks on the compositional function
5. Compare performance, parameter efficiency, and training time
6. Answer analytical questions about your results

### Key Concepts
- **Compositional Functions**: Functions built by hierarchically combining simpler functions
- **Shallow Networks**: Networks with one hidden layer (Input → Hidden → Output)
- **Deep Networks**: Networks with multiple hidden layers (Input → H1 → H2 → H3 → Output)
- **Parameter Efficiency**: The relationship between number of parameters and approximation quality

### Expected Time
3-4 hours

---

## Part 2: Universal Approximation Theorem (35 points)

**Notebook**: `hw01_part2_assignment.ipynb`

### Learning Objectives
- Understand the Universal Approximation Theorem (UAT) and its implications
- Verify UAT empirically by approximating various target functions
- Explore the relationship between network width and approximation accuracy
- Analyze convergence properties and sample complexity

### What You'll Do
1. Implement diverse target functions:
   - Polynomial functions
   - Sinusoidal functions
   - Step functions
   - Gaussian bumps
   - Composite functions
2. Build shallow neural networks with varying widths (10 to 1000 neurons)
3. Train networks to approximate each target function
4. Measure approximation error using L2 distance
5. Visualize learned approximations vs target functions
6. Analyze how network width affects approximation quality

### Key Concepts
- **Universal Approximation Theorem**: A shallow network with enough neurons can approximate any continuous function
- **Width vs Accuracy**: More neurons generally improve approximation (but with diminishing returns)
- **Function Complexity**: Different functions require different network capacities
- **Overfitting vs Underfitting**: Trade-off between model complexity and generalization

### Expected Time
3-4 hours

---

## Part 3: Language Modeling with Transformers (30 points)

**Notebook**: `hw01_part3_assignment.ipynb`

### Learning Objectives
- Understand language modeling fundamentals
- Work with pre-trained transformer models (GPT-2 and DistilGPT2)
- Calculate and interpret perplexity metrics
- Analyze model predictions and confidence levels
- Compare model performance across different architectures

### What You'll Do
1. Load financial news text data
2. Implement perplexity calculation from scratch
3. Load and compare two pre-trained models:
   - GPT-2 (117M parameters)
   - DistilGPT2 (82M parameters - distilled version)
4. Calculate perplexity for both models on the same text
5. Analyze token-level predictions and probabilities
6. Explore model behavior on different types of text
7. Compare model size vs performance trade-offs

### Key Concepts
- **Language Modeling**: Predicting the next word given previous words
- **Perplexity**: A metric measuring how "surprised" a model is by the text (lower is better)
- **Transformers**: Modern architecture using self-attention mechanisms
- **Model Distillation**: Creating smaller, faster models that mimic larger ones

### Expected Time
2-3 hours

---

## Getting Started

### Prerequisites

Install required libraries:
```bash
pip install numpy pandas matplotlib seaborn
pip install torch torchvision
pip install transformers datasets
pip install scikit-learn jupyter
```

### Files in This Assignment

```
homeworks/week1/assignment/
├── hw01.md                          # This file
├── hw01_part1_assignment.ipynb      # Part 1: Deep vs Shallow
├── hw01_part2_assignment.ipynb      # Part 2: Universal Approximation
└── hw01_part3_assignment.ipynb      # Part 3: Language Modeling
```

### How to Work on the Assignment

1. **Start with Part 1** - It builds fundamental intuition
2. **Move to Part 2** - It extends Part 1 concepts
3. **Complete Part 3** - It applies concepts to real-world models
4. **Look for TODO markers** - Complete all sections marked with `# TODO:`
5. **Run all cells** - Make sure everything executes without errors
6. **Answer all questions** - Thoughtful responses earn full credit

---

## Grading Breakdown

| Component | Part 1 | Part 2 | Part 3 | Total |
|-----------|--------|--------|--------|-------|
| **Code Implementation** | 21 pts | 21 pts | 18 pts | 60 pts |
| **Analysis & Discussion** | 12 pts | 12 pts | 10 pts | 34 pts |
| **Code Quality** | 2 pts | 2 pts | 2 pts | 6 pts |
| **Subtotal** | 35 pts | 35 pts | 30 pts | **100 pts** |

---

## Submission Instructions

### What to Submit

Create a ZIP file named: `hw01_[YourLastName]_[YourFirstName].zip`

Include:
- `hw01_part1_assignment.ipynb` (with all outputs visible)
- `hw01_part2_assignment.ipynb` (with all outputs visible)
- `hw01_part3_assignment.ipynb` (with all outputs visible)
- Any generated PNG/CSV files (if not embedded in notebooks)

### Submission Checklist

Before submitting, verify:

- [ ] All three notebooks run completely without errors
- [ ] No `raise NotImplementedError()` statements remain
- [ ] All TODO sections are completed
- [ ] All plots and visualizations are generated
- [ ] All discussion questions are answered with 2-3 sentences minimum
- [ ] Your name and student ID appear at the top of each notebook
- [ ] All outputs are visible in the saved notebooks
- [ ] File names match the required format


**Extensions**: Contact instructor at least 48 hours before the deadline if you need an extension.

