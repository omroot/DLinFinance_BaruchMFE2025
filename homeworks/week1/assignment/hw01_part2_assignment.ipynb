{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Part 2: The Double Descent Phenomenon\n",
    "\n",
    "**Due Date**: [Add due date]\n",
    "\n",
    "**Name**: ___________________________\n",
    "\n",
    "**Student ID**: ___________________________\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will investigate the **double descent phenomenon** - a surprising discovery that challenges the classical bias-variance tradeoff.\n",
    "\n",
    "### The Phenomenon\n",
    "\n",
    "Classical machine learning theory predicts a U-shaped test error curve:\n",
    "- Few parameters → underfitting\n",
    "- Optimal parameters → sweet spot\n",
    "- Too many parameters → overfitting\n",
    "\n",
    "**But modern research shows**: Test error can **decrease again** when models become extremely overparameterized!\n",
    "\n",
    "### What You'll Do:\n",
    "1. Implement functions for linear regression double descent\n",
    "2. Build and train neural networks of varying sizes\n",
    "3. Observe and analyze the double descent curve\n",
    "4. Compare results across both settings\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the limitations of classical bias-variance theory\n",
    "- Implement minimum-norm interpolation\n",
    "- Analyze overparameterized models\n",
    "- Connect theory to modern deep learning practice\n",
    "\n",
    "### Grading:\n",
    "- **Part 1**: Linear Regression Implementation (35 points)\n",
    "- **Part 2**: Neural Network Implementation (25 points)\n",
    "- **Part 3**: Analysis Questions (30 points)\n",
    "- **Part 4**: Code Quality (10 points)\n",
    "- **Total**: 100 points\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Double Descent in Linear Regression (35 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "We study the linear model: $y = X\\beta + \\epsilon$\n",
    "\n",
    "**Two regimes:**\n",
    "1. **Underparameterized (p < n)**: Use standard least squares\n",
    "   $$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "2. **Overparameterized (p \\geq n)**: Use minimum-norm interpolator\n",
    "   $$\\hat{\\beta} = X^T (XX^T)^{-1} y$$\n",
    "\n",
    "The minimum-norm solution is the **smallest L2 norm** among all solutions that perfectly fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Implement Data Generation (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n: int = 100,\n",
    "                                   d: int = 200,\n",
    "                                   sigma: float = 0.2,\n",
    "                                   beta_decay: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate linear regression data: y = X @ beta + epsilon\n",
    "\n",
    "    TODO: Implement this function to generate:\n",
    "    1. Design matrix X from standard normal distribution\n",
    "    2. True coefficients beta with decay (if beta_decay=True: beta_j ~ 1/j^2)\n",
    "    3. Response y = X @ beta + Gaussian noise\n",
    "\n",
    "    Args:\n",
    "        n: Number of samples\n",
    "        d: Number of features\n",
    "        sigma: Noise standard deviation\n",
    "        beta_decay: If True, use decaying coefficients\n",
    "\n",
    "    Returns:\n",
    "        X: Design matrix (n x d)\n",
    "        y: Response vector (n,)\n",
    "        beta_true: True coefficients (d,)\n",
    "    \"\"\"\n",
    "    # TODO: Generate design matrix X from standard normal\n",
    "    # Hint: Use np.random.randn(n, d)\n",
    "    X = None  # YOUR CODE HERE\n",
    "\n",
    "    # TODO: Generate true coefficients\n",
    "    if beta_decay:\n",
    "        # Create decaying coefficients: beta_j ~ 1/j^2\n",
    "        # Hint: np.random.randn(d) / (np.arange(1, d + 1) ** 2)\n",
    "        beta_true = None  # YOUR CODE HERE\n",
    "    else:\n",
    "        # Standard normal coefficients\n",
    "        beta_true = None  # YOUR CODE HERE\n",
    "\n",
    "    # TODO: Generate response with Gaussian noise\n",
    "    # y = X @ beta_true + noise\n",
    "    # Hint: noise = np.random.randn(n) * sigma\n",
    "    y = None  # YOUR CODE HERE\n",
    "\n",
    "    return X, y, beta_true\n",
    "\n",
    "# Test your implementation\n",
    "X_test, y_test, beta_test = generate_linear_regression_data(n=50, d=100)\n",
    "assert X_test.shape == (50, 100), \"X shape incorrect\"\n",
    "assert y_test.shape == (50,), \"y shape incorrect\"\n",
    "assert beta_test.shape == (100,), \"beta shape incorrect\"\n",
    "print(\"✓ Data generation tests passed!\")\n",
    "print(f\"X range: [{X_test.min():.2f}, {X_test.max():.2f}]\")\n",
    "print(f\"y range: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "print(f\"beta range: [{beta_test.min():.4f}, {beta_test.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Implement Least Squares (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fit least squares: beta_hat = (X^T X)^{-1} X^T y\n",
    "    Works when X has full column rank (p < n)\n",
    "\n",
    "    TODO: Implement standard least squares estimation\n",
    "\n",
    "    Args:\n",
    "        X: Design matrix (n x p)\n",
    "        y: Response vector (n,)\n",
    "\n",
    "    Returns:\n",
    "        beta_hat: Estimated coefficients (p,)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement least squares\n",
    "        # Step 1: Compute X^T X\n",
    "        # Step 2: Compute X^T y\n",
    "        # Step 3: Solve (X^T X) beta = X^T y using np.linalg.solve\n",
    "        # Hint: beta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        beta_hat = None  # YOUR CODE HERE\n",
    "        return beta_hat\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Fallback if singular\n",
    "        return np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Test your implementation\n",
    "X_test, y_test, beta_true = generate_linear_regression_data(n=100, d=50)\n",
    "beta_hat = fit_least_squares(X_test, y_test)\n",
    "assert beta_hat.shape == (50,), \"beta_hat shape incorrect\"\n",
    "residuals = y_test - X_test @ beta_hat\n",
    "print(\"✓ Least squares tests passed!\")\n",
    "print(f\"Residual norm: {np.linalg.norm(residuals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Implement Minimum-Norm Interpolator (10 points)\n",
    "\n",
    "This is the **key method** for the overparameterized regime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_minimum_norm(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fit minimum-norm interpolator: beta_hat = X^T (X X^T)^{-1} y\n",
    "    This gives the minimum L2 norm solution among all interpolating solutions.\n",
    "    Works when p >= n (overparameterized regime)\n",
    "\n",
    "    TODO: Implement minimum-norm interpolation\n",
    "\n",
    "    Args:\n",
    "        X: Design matrix (n x p)\n",
    "        y: Response vector (n,)\n",
    "\n",
    "    Returns:\n",
    "        beta_hat: Estimated coefficients (p,)\n",
    "    \"\"\"\n",
    "    # TODO: Implement minimum-norm interpolator\n",
    "    # Step 1: Compute X @ X^T (this is n x n, smaller when p > n)\n",
    "    # Step 2: Solve (X @ X^T) @ alpha = y for alpha\n",
    "    # Step 3: Compute beta_hat = X^T @ alpha\n",
    "    # Hint: beta_hat = X.T @ np.linalg.solve(X @ X.T, y)\n",
    "    beta_hat = None  # YOUR CODE HERE\n",
    "\n",
    "    return beta_hat\n",
    "\n",
    "# Test your implementation\n",
    "X_test, y_test, beta_true = generate_linear_regression_data(n=50, d=100)\n",
    "beta_hat = fit_minimum_norm(X_test, y_test)\n",
    "assert beta_hat.shape == (100,), \"beta_hat shape incorrect\"\n",
    "\n",
    "# Check that it interpolates (perfectly fits training data)\n",
    "predictions = X_test @ beta_hat\n",
    "interpolation_error = np.max(np.abs(y_test - predictions))\n",
    "print(\"✓ Minimum-norm tests passed!\")\n",
    "print(f\"Max interpolation error: {interpolation_error:.6f} (should be ~0)\")\n",
    "print(f\"Beta norm: {np.linalg.norm(beta_hat):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Implement Risk Computation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk(X: np.ndarray, y: np.ndarray, beta_hat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean squared error: (1/n) ||y - X beta_hat||^2\n",
    "\n",
    "    TODO: Implement MSE calculation\n",
    "\n",
    "    Args:\n",
    "        X: Design matrix (n x p)\n",
    "        y: Response vector (n,)\n",
    "        beta_hat: Estimated coefficients (p,)\n",
    "\n",
    "    Returns:\n",
    "        MSE risk\n",
    "    \"\"\"\n",
    "    # TODO: Compute predictions and MSE\n",
    "    # Step 1: predictions = X @ beta_hat\n",
    "    # Step 2: risk = np.mean((y - predictions) ** 2)\n",
    "    predictions = None  # YOUR CODE HERE\n",
    "    risk = None  # YOUR CODE HERE\n",
    "\n",
    "    return risk\n",
    "\n",
    "# Test your implementation\n",
    "risk = compute_risk(X_test, y_test, beta_hat)\n",
    "assert isinstance(risk, (float, np.floating)), \"Risk should be a float\"\n",
    "assert risk >= 0, \"Risk should be non-negative\"\n",
    "print(\"✓ Risk computation tests passed!\")\n",
    "print(f\"Test risk: {risk:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Run Double Descent Experiment (5 points)\n",
    "\n",
    "Now we'll put it all together! The experiment function is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_descent_experiment(n: int = 100,\n",
    "                             d: int = 200,\n",
    "                             p_values: List[int] = None,\n",
    "                             sigma: float = 0.2,\n",
    "                             n_trials: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run double descent experiment (PROVIDED)\n",
    "    \"\"\"\n",
    "    if p_values is None:\n",
    "        p_values = list(range(10, 151, 5))\n",
    "\n",
    "    results = {\n",
    "        'p_values': p_values,\n",
    "        'train_risks': [],\n",
    "        'test_risks': [],\n",
    "        'train_risks_std': [],\n",
    "        'test_risks_std': []\n",
    "    }\n",
    "\n",
    "    print(f\"\\nRunning experiment with n={n}, d={d}, sigma={sigma}\")\n",
    "    print(f\"Testing {len(p_values)} different feature counts\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for p in p_values:\n",
    "        train_risks_trials = []\n",
    "        test_risks_trials = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            # Generate data\n",
    "            X_full_train, y_train, beta_true = generate_linear_regression_data(n, d, sigma)\n",
    "            X_full_test, y_test, _ = generate_linear_regression_data(n, d, sigma, beta_decay=False)\n",
    "            y_test = X_full_test @ beta_true + np.random.randn(n) * sigma\n",
    "\n",
    "            # Select p features\n",
    "            np.random.seed(SEED + trial)\n",
    "            selected_features = np.random.choice(d, size=p, replace=False)\n",
    "            X_train = X_full_train[:, selected_features]\n",
    "            X_test = X_full_test[:, selected_features]\n",
    "\n",
    "            # Fit model\n",
    "            if p < n:\n",
    "                beta_hat = fit_least_squares(X_train, y_train)\n",
    "            else:\n",
    "                beta_hat = fit_minimum_norm(X_train, y_train)\n",
    "\n",
    "            # Compute risks\n",
    "            train_risk = compute_risk(X_train, y_train, beta_hat)\n",
    "            test_risk = compute_risk(X_test, y_test, beta_hat)\n",
    "\n",
    "            train_risks_trials.append(train_risk)\n",
    "            test_risks_trials.append(test_risk)\n",
    "\n",
    "        results['train_risks'].append(np.mean(train_risks_trials))\n",
    "        results['test_risks'].append(np.mean(test_risks_trials))\n",
    "        results['train_risks_std'].append(np.std(train_risks_trials))\n",
    "        results['test_risks_std'].append(np.std(test_risks_trials))\n",
    "\n",
    "        if p % 20 == 0 or p == n:\n",
    "            print(f\"p={p:3d}: Train Risk={np.mean(train_risks_trials):.6f}, \"\n",
    "                  f\"Test Risk={np.mean(test_risks_trials):.6f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "print(\"=\"*80)\n",
    "print(\"PART 1: LINEAR REGRESSION DOUBLE DESCENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "p_values = list(range(10, 151, 5))\n",
    "results_linear = double_descent_experiment(\n",
    "    n=100,\n",
    "    d=200,\n",
    "    p_values=p_values,\n",
    "    sigma=0.2,\n",
    "    n_trials=50\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code (PROVIDED)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "p_vals = results_linear['p_values']\n",
    "train_risks = results_linear['train_risks']\n",
    "test_risks = results_linear['test_risks']\n",
    "train_std = results_linear['train_risks_std']\n",
    "test_std = results_linear['test_risks_std']\n",
    "\n",
    "# Plot 1: Log-log scale\n",
    "ax1 = axes[0]\n",
    "ax1.plot(p_vals, train_risks, 'o-', label='Train Risk',\n",
    "         linewidth=2.5, markersize=6, color='#e74c3c', alpha=0.8)\n",
    "ax1.plot(p_vals, test_risks, 's-', label='Test Risk',\n",
    "         linewidth=2.5, markersize=6, color='#3498db', alpha=0.8)\n",
    "ax1.fill_between(p_vals,\n",
    "                 np.array(train_risks) - np.array(train_std),\n",
    "                 np.array(train_risks) + np.array(train_std),\n",
    "                 alpha=0.2, color='#e74c3c')\n",
    "ax1.fill_between(p_vals,\n",
    "                 np.array(test_risks) - np.array(test_std),\n",
    "                 np.array(test_risks) + np.array(test_std),\n",
    "                 alpha=0.2, color='#3498db')\n",
    "ax1.axvline(x=100, color='black', linestyle='--', linewidth=2,\n",
    "            alpha=0.7, label='Interpolation Threshold (n=100)')\n",
    "ax1.set_xlabel('Number of Features (p)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Risk (MSE)', fontsize=14, fontweight='bold')\n",
    "ax1.set_title('Linear Regression Double Descent (Log-Log)', fontsize=16, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 2: Linear scale with annotations\n",
    "ax2 = axes[1]\n",
    "ax2.plot(p_vals, test_risks, 's-', label='Test Risk',\n",
    "         linewidth=2.5, markersize=6, color='#3498db', alpha=0.8)\n",
    "ax2.axvline(x=100, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
    "\n",
    "y_pos = ax2.get_ylim()[1] * 0.9\n",
    "ax2.annotate('Classical\\nRegime', xy=(50, y_pos),\n",
    "             fontsize=12, ha='center', style='italic',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax2.annotate('Peak', xy=(100, y_pos),\n",
    "             fontsize=12, ha='center', style='italic',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "ax2.annotate('Modern\\nRegime', xy=(130, y_pos),\n",
    "             fontsize=12, ha='center', style='italic',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "ax2.set_xlabel('Number of Features (p)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Test Risk (MSE)', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Linear Regression Double Descent', fontsize=16, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('linear_double_descent.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Double Descent in Neural Networks (25 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "We'll demonstrate double descent on Fashion-MNIST:\n",
    "- **Architecture**: Input (784) → Hidden (W) → Hidden (W) → Output (10)\n",
    "- **Variable**: Width W (from 5 to 2000)\n",
    "- **Goal**: Observe test accuracy improve in overparameterized regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist(subset_size: int = 1000, test_size: int = 500):\n",
    "    \"\"\"Load Fashion-MNIST (PROVIDED)\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    if subset_size < len(train_dataset):\n",
    "        indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n",
    "        train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "\n",
    "    if test_size < len(test_dataset):\n",
    "        indices = np.random.choice(len(test_dataset), test_size, replace=False)\n",
    "        test_dataset = torch.utils.data.Subset(test_dataset, indices)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 2: NEURAL NETWORK DOUBLE DESCENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_dataset, test_dataset = load_fashion_mnist(subset_size=1000, test_size=500)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Implement Neural Network Architecture (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected neural network with variable width.\n",
    "    Architecture: Input (784) -> Hidden1 -> Hidden2 -> Output (10)\n",
    "\n",
    "    TODO: Implement this network class\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 784,\n",
    "                 hidden_dims: List[int] = [100, 100],\n",
    "                 output_dim: int = 10,\n",
    "                 activation: str = 'relu'):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "\n",
    "        # TODO: Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = None  # YOUR CODE HERE (nn.ReLU())\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = None  # YOUR CODE HERE (nn.Tanh())\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "        # TODO: Build layers\n",
    "        # Create a sequential network:\n",
    "        # Input -> Linear -> Activation -> Linear -> Activation -> ... -> Output\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Loop through hidden_dims:\n",
    "        #   - Add nn.Linear(in_dim, hidden_dim)\n",
    "        #   - Add activation\n",
    "        #   - Update in_dim = hidden_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            pass  # YOUR CODE HERE\n",
    "\n",
    "        # Add final output layer\n",
    "        # YOUR CODE HERE: layers.append(nn.Linear(in_dim, output_dim))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement forward pass\n",
    "        Flatten input and pass through network\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Step 1: Flatten: x = x.view(x.size(0), -1)\n",
    "        # Step 2: Pass through network: return self.network(x)\n",
    "        pass\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count parameters (PROVIDED)\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test your implementation\n",
    "test_model = FullyConnectedNet(input_dim=784, hidden_dims=[50, 50], output_dim=10)\n",
    "test_input = torch.randn(5, 1, 28, 28)\n",
    "test_output = test_model(test_input)\n",
    "assert test_output.shape == (5, 10), f\"Expected shape (5, 10), got {test_output.shape}\"\n",
    "print(\"✓ Network tests passed!\")\n",
    "print(f\"Parameters: {test_model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement Training Function (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(model: nn.Module,\n",
    "                        train_dataset,\n",
    "                        test_dataset,\n",
    "                        epochs: int = 100,\n",
    "                        batch_size: int = 32,\n",
    "                        lr: float = 0.001,\n",
    "                        patience: int = 20,\n",
    "                        verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Train a neural network.\n",
    "\n",
    "    TODO: Complete the training loop\n",
    "    \"\"\"\n",
    "    # Create data loaders (PROVIDED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # TODO: Define loss and optimizer\n",
    "    criterion = None  # YOUR CODE HERE (nn.CrossEntropyLoss())\n",
    "    optimizer = None  # YOUR CODE HERE (optim.Adam(model.parameters(), lr=lr))\n",
    "\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    best_test_acc = 0\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # TODO: Training step\n",
    "            # 1. Zero gradients\n",
    "            # 2. Forward pass\n",
    "            # 3. Compute loss\n",
    "            # 4. Backward pass\n",
    "            # 5. Optimizer step\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "\n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        # Test phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "                test_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += batch_y.size(0)\n",
    "                test_correct += predicted.eq(batch_y).sum().item()\n",
    "\n",
    "        test_loss /= len(test_dataset)\n",
    "        test_acc = 100. * test_correct / test_total\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        # Early stopping\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'best_test_acc': best_test_acc,\n",
    "        'final_train_acc': train_acc,\n",
    "        'final_test_acc': test_acc,\n",
    "        'training_time': training_time,\n",
    "        'num_parameters': model.count_parameters(),\n",
    "        'epochs_trained': len(history['train_loss'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network Experiment (Provided)\n",
    "\n",
    "**Note**: This will take 10-20 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning Neural Network Experiment...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "widths = [5, 10, 15, 20, 30, 50, 75, 100, 150, 200, 300, 500, 750, 1000, 1500, 2000]\n",
    "nn_results = []\n",
    "\n",
    "for width in widths:\n",
    "    print(f\"\\n[{widths.index(width)+1}/{len(widths)}] Training width={width}...\")\n",
    "    \n",
    "    model = FullyConnectedNet(\n",
    "        input_dim=784,\n",
    "        hidden_dims=[width, width],\n",
    "        output_dim=10,\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Parameters: {model.count_parameters():,}\")\n",
    "    \n",
    "    results = train_neural_network(\n",
    "        model, train_dataset, test_dataset,\n",
    "        epochs=100, batch_size=32, lr=0.001,\n",
    "        patience=20, verbose=False\n",
    "    )\n",
    "    \n",
    "    nn_results.append(results)\n",
    "    \n",
    "    print(f\"  Train Acc: {results['final_train_acc']:.2f}%\")\n",
    "    print(f\"  Test Acc:  {results['final_test_acc']:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Neural network experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Neural Network Results (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "nn_params = [r['num_parameters'] for r in nn_results]\n",
    "nn_train_acc = [r['final_train_acc'] for r in nn_results]\n",
    "nn_test_acc = [r['final_test_acc'] for r in nn_results]\n",
    "nn_test_error = [100 - acc for acc in nn_test_acc]\n",
    "\n",
    "# Find interpolation threshold\n",
    "interpolation_threshold_idx = None\n",
    "for i, acc in enumerate(nn_train_acc):\n",
    "    if acc >= 99.0:\n",
    "        interpolation_threshold_idx = i\n",
    "        break\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.plot(nn_params, nn_test_error, 's-', linewidth=2.5, markersize=8,\n",
    "        color='#e74c3c', label='Test Error')\n",
    "\n",
    "if interpolation_threshold_idx is not None:\n",
    "    ax.axvline(x=nn_params[interpolation_threshold_idx], color='black',\n",
    "               linestyle='--', linewidth=2, alpha=0.7,\n",
    "               label=f'Interpolation (~{nn_params[interpolation_threshold_idx]:,} params)')\n",
    "\n",
    "ax.set_xlabel('Number of Parameters', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Test Error (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Neural Network: Double Descent', fontsize=16, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('neural_network_double_descent.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analysis Questions (30 points)\n",
    "\n",
    "Answer the following questions based on your experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (10 points)\n",
    "\n",
    "**Describe the double descent phenomenon observed in linear regression.**\n",
    "\n",
    "a) What happens to test risk as p increases from 10 to 100?\n",
    "\n",
    "b) What happens at p = n = 100 (the interpolation threshold)?\n",
    "\n",
    "c) What happens to test risk as p increases beyond 100?\n",
    "\n",
    "d) Why is this surprising from a classical bias-variance perspective?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (10 points)\n",
    "\n",
    "**Explain the minimum-norm interpolator.**\n",
    "\n",
    "a) When p > n, why are there infinitely many solutions that perfectly fit the training data?\n",
    "\n",
    "b) What does \"minimum-norm\" mean? Why do we choose this particular solution?\n",
    "\n",
    "c) Why does the minimum-norm solution tend to generalize better than other interpolating solutions?\n",
    "\n",
    "d) How does gradient descent implicitly find this solution in neural networks?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (10 points)\n",
    "\n",
    "**Connect to modern deep learning practice.**\n",
    "\n",
    "a) Large language models (like GPT) have billions of parameters but are trained on relatively smaller datasets. Based on double descent, why might this work?\n",
    "\n",
    "b) In your neural network experiments, did you observe double descent? Compare the test accuracy at the interpolation threshold vs. the largest models.\n",
    "\n",
    "c) What are the practical implications for practitioners? Should we:\n",
    "   - Use small models and tune carefully?\n",
    "   - Use very large models and rely on overparameterization?\n",
    "   - Something in between?\n",
    "\n",
    "d) How does this change our understanding of regularization (L2 penalty, dropout, etc.)?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (PROVIDED)\n",
    "linear_df = pd.DataFrame({\n",
    "    'p': results_linear['p_values'],\n",
    "    'train_risk': results_linear['train_risks'],\n",
    "    'test_risk': results_linear['test_risks'],\n",
    "    'train_risk_std': results_linear['train_risks_std'],\n",
    "    'test_risk_std': results_linear['test_risks_std']\n",
    "})\n",
    "linear_df.to_csv('linear_double_descent_results.csv', index=False)\n",
    "\n",
    "nn_df = pd.DataFrame({\n",
    "    'width': widths,\n",
    "    'parameters': nn_params,\n",
    "    'train_acc': nn_train_acc,\n",
    "    'test_acc': nn_test_acc,\n",
    "    'training_time': [r['training_time'] for r in nn_results],\n",
    "    'epochs': [r['epochs_trained'] for r in nn_results]\n",
    "})\n",
    "nn_df.to_csv('neural_network_double_descent_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Assignment Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - linear_double_descent.png\")\n",
    "print(\"  - neural_network_double_descent.png\")\n",
    "print(\"  - linear_double_descent_results.csv\")\n",
    "print(\"  - neural_network_double_descent_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting, ensure:\n",
    "\n",
    "- [ ] All TODO sections are completed\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] All plots are generated\n",
    "- [ ] All analysis questions are answered\n",
    "- [ ] Your name and student ID are at the top\n",
    "- [ ] The notebook is saved with all outputs visible\n",
    "\n",
    "**Submit**: Upload this completed notebook (.ipynb file) with all outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
