{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Part 1: Approximation Power of Deep vs Shallow Networks\n",
    "\n",
    "**Due Date**: [Add due date]\n",
    "\n",
    "**Name**: ___________________________\n",
    "\n",
    "**Student ID**: ___________________________\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will investigate a fundamental theoretical insight in deep learning: **deep networks can approximate compositional functions more efficiently than shallow networks**.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand compositional functions and their hierarchical structure\n",
    "- Implement neural network architectures (shallow and deep)\n",
    "- Compare parameter efficiency across different architectures\n",
    "- Analyze experimental results and draw conclusions\n",
    "\n",
    "### Instructions:\n",
    "1. Complete all code sections marked with `# TODO: ...`\n",
    "2. Run all cells to generate results\n",
    "3. Answer the discussion questions in markdown cells\n",
    "4. Submit the completed notebook with all outputs\n",
    "\n",
    "### Grading:\n",
    "- Code Implementation: 60 points\n",
    "- Analysis and Discussion: 30 points\n",
    "- Code Quality and Documentation: 10 points\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup (Provided)\n",
    "\n",
    "Run this cell to import all necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Compositional Function (15 points)\n",
    "\n",
    "### Background:\n",
    "\n",
    "A **compositional function** is built by hierarchically combining simpler functions. We'll create a 3-level binary tree structure.\n",
    "\n",
    "**Building block**: $h(a, b) = \\tanh(a + b^2)$\n",
    "\n",
    "**Hierarchical structure**:\n",
    "```\n",
    "Level 1 (Leaf):    h11(x1,x2)   h12(x3,x4)   h13(x5,x6)   h14(x7,x8)\n",
    "                        \\         /                \\         /\n",
    "Level 2 (Internal):     h21                         h22\n",
    "                              \\                   /\n",
    "Level 3 (Root):                     h3 (output)\n",
    "```\n",
    "\n",
    "### Task 1.1: Implement the basic h function (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_function(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Basic non-linear function for composition.\n",
    "    \n",
    "    TODO: Implement h(a,b) = tanh(a + b^2)\n",
    "    \n",
    "    Args:\n",
    "        a, b: Input values (can be scalars or numpy arrays)\n",
    "    \n",
    "    Returns:\n",
    "        Non-linear combination\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "assert np.isclose(h_function(0, 0), 0.0), \"Test failed for h(0,0)\"\n",
    "assert np.isclose(h_function(1, 1), np.tanh(2)), \"Test failed for h(1,1)\"\n",
    "print(\"✓ h_function tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Implement the hierarchical compositional function (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compositional_function(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hierarchical compositional function following a binary tree structure.\n",
    "    \n",
    "    TODO: Implement the 3-level tree structure:\n",
    "    - Level 1: Compute h11(x1,x2), h12(x3,x4), h13(x5,x6), h14(x7,x8)\n",
    "    - Level 2: Compute h21(h11, h12) and h22(h13, h14)\n",
    "    - Level 3: Compute h3(h21, h22) as the final output\n",
    "    \n",
    "    Args:\n",
    "        x: Input array of shape (n_samples, 8)\n",
    "    \n",
    "    Returns:\n",
    "        Output array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    assert x.shape[1] == 8, \"Input must have 8 features\"\n",
    "    \n",
    "    # TODO: Level 1 - Leaf computations (4 nodes)\n",
    "    # Hint: Use x[:, i] to access the i-th feature for all samples\n",
    "    h11 = None  # YOUR CODE HERE\n",
    "    h12 = None  # YOUR CODE HERE\n",
    "    h13 = None  # YOUR CODE HERE\n",
    "    h14 = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Level 2 - Internal nodes (2 nodes)\n",
    "    h21 = None  # YOUR CODE HERE\n",
    "    h22 = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Level 3 - Root (1 node)\n",
    "    h3 = None  # YOUR CODE HERE\n",
    "    \n",
    "    return h3\n",
    "\n",
    "# Test your implementation\n",
    "test_x = np.random.randn(10, 8)\n",
    "test_y = compositional_function(test_x)\n",
    "assert test_y.shape == (10,), \"Output shape should be (n_samples,)\"\n",
    "assert np.all(np.abs(test_y) <= 1.0), \"Output should be in range [-1, 1] due to tanh\"\n",
    "print(\"✓ compositional_function tests passed!\")\n",
    "print(f\"Sample output range: [{test_y.min():.3f}, {test_y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Generate training data (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compositional_data(n_samples: int = 1000, \n",
    "                               noise_std: float = 0.05) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset based on compositional function.\n",
    "    \"\"\"\n",
    "    X = np.random.randn(n_samples, 8)\n",
    "    y = compositional_function(X)\n",
    "    y += np.random.randn(n_samples) * noise_std\n",
    "    return X, y\n",
    "\n",
    "# Generate training and test data\n",
    "print(\"Generating compositional dataset...\")\n",
    "X_train, y_train = generate_compositional_data(n_samples=1000, noise_std=0.05)\n",
    "X_test, y_test = generate_compositional_data(n_samples=500, noise_std=0.05)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target range: [{y_train.min():.3f}, {y_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X_train[:, i], y_train, alpha=0.5, s=10)\n",
    "    ax.set_xlabel(f'x{i+1}')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Target vs Feature {i+1}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('compositional_data_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Neural Network Architectures (25 points)\n",
    "\n",
    "### Task 2.1: Implement Shallow Network (12 points)\n",
    "\n",
    "A shallow network has: **Input (8) → Hidden → Output (1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Shallow neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 8, hidden_dim: int = 100, \n",
    "                 activation: str = 'tanh'):\n",
    "        super(ShallowNetwork, self).__init__()\n",
    "        \n",
    "        # TODO: Define the layers\n",
    "        # Hint: Use nn.Linear(in_features, out_features)\n",
    "        self.fc1 = None  # YOUR CODE HERE - First layer: input_dim -> hidden_dim\n",
    "        self.fc2 = None  # YOUR CODE HERE - Second layer: hidden_dim -> 1\n",
    "        \n",
    "        # TODO: Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = None  # YOUR CODE HERE\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = None  # YOUR CODE HERE\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement the forward pass\n",
    "        1. Apply first linear layer\n",
    "        2. Apply activation function\n",
    "        3. Apply second linear layer\n",
    "        4. Squeeze output to shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters (provided)\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test your implementation\n",
    "test_model = ShallowNetwork(input_dim=8, hidden_dim=100, activation='tanh')\n",
    "test_input = torch.randn(5, 8)\n",
    "test_output = test_model(test_input)\n",
    "assert test_output.shape == (5,), f\"Expected shape (5,), got {test_output.shape}\"\n",
    "print(f\"✓ ShallowNetwork tests passed!\")\n",
    "print(f\"Parameters: {test_model.count_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement Deep Network (13 points)\n",
    "\n",
    "A deep network has: **Input (8) → Hidden1 → Hidden2 → Hidden3 → Output (1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep neural network with multiple hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 8, hidden_dims: List[int] = [16, 8, 4], \n",
    "                 activation: str = 'tanh'):\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        \n",
    "        # TODO: Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = None  # YOUR CODE HERE\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = None  # YOUR CODE HERE\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # TODO: Build sequential layers\n",
    "        # Hint: Loop through hidden_dims and create layers with activations\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            # YOUR CODE HERE\n",
    "            # Add a linear layer: in_dim -> hidden_dim\n",
    "            # Add activation function\n",
    "            # Update in_dim = hidden_dim for next iteration\n",
    "            pass\n",
    "        \n",
    "        # YOUR CODE HERE - Add final output layer: in_dim -> 1\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement forward pass\n",
    "        Hint: Just pass through self.network and squeeze\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters (provided)\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test your implementation\n",
    "test_model = DeepNetwork(input_dim=8, hidden_dims=[16, 8, 4], activation='tanh')\n",
    "test_input = torch.randn(5, 8)\n",
    "test_output = test_model(test_input)\n",
    "assert test_output.shape == (5,), f\"Expected shape (5,), got {test_output.shape}\"\n",
    "print(f\"✓ DeepNetwork tests passed!\")\n",
    "print(f\"Parameters: {test_model.count_parameters()}\")\n",
    "print(f\"Architecture: {test_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Training Function (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model: nn.Module, \n",
    "                 X_train: np.ndarray, \n",
    "                 y_train: np.ndarray,\n",
    "                 X_test: np.ndarray,\n",
    "                 y_test: np.ndarray,\n",
    "                 epochs: int = 500,\n",
    "                 batch_size: int = 32,\n",
    "                 lr: float = 0.001,\n",
    "                 verbose: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Train a neural network and track performance.\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # TODO: Define loss function and optimizer\n",
    "    criterion = None  # YOUR CODE HERE - Use MSELoss\n",
    "    optimizer = None  # YOUR CODE HERE - Use Adam optimizer\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'test_loss': []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # TODO: Implement training loop\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # YOUR CODE HERE\n",
    "            # 1. Zero gradients: optimizer.zero_grad()\n",
    "            # 2. Forward pass: outputs = model(batch_X)\n",
    "            # 3. Compute loss: loss = criterion(outputs, batch_y)\n",
    "            # 4. Backward pass: loss.backward()\n",
    "            # 5. Update weights: optimizer.step()\n",
    "            # 6. Accumulate loss: train_loss += loss.item() * batch_X.size(0)\n",
    "            pass\n",
    "        \n",
    "        train_loss /= len(X_train)\n",
    "        \n",
    "        # TODO: Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # YOUR CODE HERE\n",
    "            # 1. Get predictions: test_outputs = model(X_test_t)\n",
    "            # 2. Compute test loss: test_loss = criterion(test_outputs, y_test_t).item()\n",
    "            test_loss = 0.0  # Replace this\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_train_loss = criterion(model(X_train_t), y_train_t).item()\n",
    "        final_test_loss = criterion(model(X_test_t), y_test_t).item()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_loss': final_train_loss,\n",
    "        'test_loss': final_test_loss,\n",
    "        'training_time': training_time,\n",
    "        'num_parameters': model.count_parameters()\n",
    "    }\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing training function...\")\n",
    "test_model = ShallowNetwork(input_dim=8, hidden_dim=10, activation='tanh')\n",
    "test_results = train_network(test_model, X_train[:100], y_train[:100], \n",
    "                            X_test[:50], y_test[:50], epochs=10, verbose=False)\n",
    "print(f\"✓ Training function works!\")\n",
    "print(f\"Final test loss: {test_results['test_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train and Compare Networks (Provided)\n",
    "\n",
    "Now we'll train multiple networks and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Shallow Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_widths = [10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "shallow_results = []\n",
    "\n",
    "print(\"Training Shallow Networks...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for width in shallow_widths:\n",
    "    print(f\"Training shallow network with width {width}...\")\n",
    "    model = ShallowNetwork(input_dim=8, hidden_dim=width, activation='tanh')\n",
    "    results = train_network(model, X_train, y_train, X_test, y_test,\n",
    "                           epochs=500, batch_size=32, lr=0.001, verbose=False)\n",
    "    shallow_results.append(results)\n",
    "    print(f\"  Parameters: {results['num_parameters']} | Test MSE: {results['test_loss']:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Shallow network training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_configs = [\n",
    "    [4, 2, 2],\n",
    "    [8, 4, 2],\n",
    "    [16, 8, 4],\n",
    "    [24, 12, 6],\n",
    "    [32, 16, 8],\n",
    "    [48, 24, 12],\n",
    "    [64, 32, 16],\n",
    "    [96, 48, 24],\n",
    "]\n",
    "\n",
    "deep_results = []\n",
    "\n",
    "print(\"Training Deep Networks...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for config in deep_configs:\n",
    "    print(f\"Training deep network with structure {config}...\")\n",
    "    model = DeepNetwork(input_dim=8, hidden_dims=config, activation='tanh')\n",
    "    results = train_network(model, X_train, y_train, X_test, y_test,\n",
    "                           epochs=500, batch_size=32, lr=0.001, verbose=False)\n",
    "    deep_results.append(results)\n",
    "    print(f\"  Parameters: {results['num_parameters']} | Test MSE: {results['test_loss']:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Deep network training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "shallow_params = [r['num_parameters'] for r in shallow_results]\n",
    "shallow_test_mse = [r['test_loss'] for r in shallow_results]\n",
    "shallow_train_time = [r['training_time'] for r in shallow_results]\n",
    "\n",
    "deep_params = [r['num_parameters'] for r in deep_results]\n",
    "deep_test_mse = [r['test_loss'] for r in deep_results]\n",
    "deep_train_time = [r['training_time'] for r in deep_results]\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Test Error vs Parameters (log-log)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.loglog(shallow_params, shallow_test_mse, 'o-', label='Shallow', \n",
    "           linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.loglog(deep_params, deep_test_mse, 's-', label='Deep', \n",
    "           linewidth=2, markersize=8, color='#3498db')\n",
    "ax1.set_xlabel('Number of Parameters', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Test MSE', fontsize=14, fontweight='bold')\n",
    "ax1.set_title('Test Error vs Model Size (Log-Log)', fontsize=16, fontweight='bold')\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test Error vs Parameters (linear)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(shallow_params, shallow_test_mse, 'o-', label='Shallow', \n",
    "         linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax2.plot(deep_params, deep_test_mse, 's-', label='Deep', \n",
    "         linewidth=2, markersize=8, color='#3498db')\n",
    "ax2.set_xlabel('Number of Parameters', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Test MSE', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Test Error vs Model Size (Linear)', fontsize=16, fontweight='bold')\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(shallow_params, shallow_train_time, 'o-', label='Shallow', \n",
    "         linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax3.plot(deep_params, deep_train_time, 's-', label='Deep', \n",
    "         linewidth=2, markersize=8, color='#3498db')\n",
    "ax3.set_xlabel('Number of Parameters', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Training Time (seconds)', fontsize=14, fontweight='bold')\n",
    "ax3.set_title('Training Time vs Model Size', fontsize=16, fontweight='bold')\n",
    "ax3.legend(fontsize=12)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Parameter Efficiency\n",
    "ax4 = axes[1, 1]\n",
    "shallow_efficiency = [mse / params for mse, params in zip(shallow_test_mse, shallow_params)]\n",
    "deep_efficiency = [mse / params for mse, params in zip(deep_test_mse, deep_params)]\n",
    "ax4.semilogx(shallow_params, shallow_efficiency, 'o-', label='Shallow', \n",
    "             linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax4.semilogx(deep_params, deep_efficiency, 's-', label='Deep', \n",
    "             linewidth=2, markersize=8, color='#3498db')\n",
    "ax4.set_xlabel('Number of Parameters', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Efficiency (MSE / Parameters)', fontsize=14, fontweight='bold')\n",
    "ax4.set_title('Parameter Efficiency', fontsize=16, fontweight='bold')\n",
    "ax4.legend(fontsize=12)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shallow_vs_deep_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis Questions (30 points total)\n",
    "\n",
    "Answer the following questions based on your experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (10 points)\n",
    "\n",
    "**Compare the test error of shallow vs deep networks at similar parameter counts (around 300-400 parameters).**\n",
    "\n",
    "a) Which architecture achieves lower test error?\n",
    "\n",
    "b) Calculate the percentage improvement.\n",
    "\n",
    "c) Explain why you think one architecture outperforms the other.\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (10 points)\n",
    "\n",
    "**Examine the parameter efficiency plot (bottom-right).**\n",
    "\n",
    "a) Which network type is more parameter-efficient (lower efficiency score is better)?\n",
    "\n",
    "b) How does the efficiency trend differ between shallow and deep networks as parameters increase?\n",
    "\n",
    "c) What does this tell you about the relationship between network depth and the compositional function?\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (10 points)\n",
    "\n",
    "**Consider the hierarchical structure of the compositional function and the deep network architecture.**\n",
    "\n",
    "a) How does the 3-layer structure of the compositional function relate to the deep network architecture?\n",
    "\n",
    "b) Why might a shallow network struggle to approximate this function efficiently?\n",
    "\n",
    "c) In what types of real-world problems (e.g., finance, computer vision, NLP) might you expect deep networks to have similar advantages? Explain your reasoning.\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Bonus - Non-Compositional Function (10 bonus points)\n",
    "\n",
    "Implement and test a non-compositional function to verify that the deep network advantage is specific to compositional structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_compositional_function(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Implement a non-compositional function using random Fourier features.\n",
    "    f(x) = sum_i w_i * sin(omega_i * x_i)\n",
    "    \n",
    "    Hint: Use fixed random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Generate non-compositional data\n",
    "X_train_nc = np.random.randn(1000, 8)\n",
    "y_train_nc = non_compositional_function(X_train_nc) + np.random.randn(1000) * 0.05\n",
    "\n",
    "X_test_nc = np.random.randn(500, 8)\n",
    "y_test_nc = non_compositional_function(X_test_nc) + np.random.randn(500) * 0.05\n",
    "\n",
    "# Train models\n",
    "test_shallow = ShallowNetwork(input_dim=8, hidden_dim=200, activation='tanh')\n",
    "test_deep = DeepNetwork(input_dim=8, hidden_dims=[32, 16, 8], activation='tanh')\n",
    "\n",
    "shallow_nc_results = train_network(test_shallow, X_train_nc, y_train_nc, \n",
    "                                   X_test_nc, y_test_nc, epochs=500, verbose=False)\n",
    "deep_nc_results = train_network(test_deep, X_train_nc, y_train_nc, \n",
    "                               X_test_nc, y_test_nc, epochs=500, verbose=False)\n",
    "\n",
    "print(f\"Shallow Network: Test MSE = {shallow_nc_results['test_loss']:.6f}\")\n",
    "print(f\"Deep Network: Test MSE = {deep_nc_results['test_loss']:.6f}\")\n",
    "print(f\"\\nAre the results similar? What does this tell you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question\n",
    "\n",
    "**Explain the results from the non-compositional function experiment. Why do you think shallow and deep networks perform similarly (or differently) on this function compared to the compositional function?**\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "*Double-click to edit this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] All TODO sections are completed\n",
    "- [ ] All plots are generated and saved\n",
    "- [ ] All analysis questions are answered\n",
    "- [ ] Your name and student ID are at the top\n",
    "- [ ] The notebook is saved with all outputs visible\n",
    "\n",
    "**Submit**: Upload this completed notebook (.ipynb file) with all outputs to the course portal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
