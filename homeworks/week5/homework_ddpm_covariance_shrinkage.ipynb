{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: DDPM for Covariance Shrinkage in Portfolio Optimization\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will implement a **Denoising Diffusion Probabilistic Model (DDPM)** and use it for covariance matrix estimation in portfolio optimization. You will compare three covariance estimation methods:\n",
    "\n",
    "1. **Empirical Covariance**: Sample covariance (baseline)\n",
    "2. **Ledoit-Wolf Shrinkage**: Classic shrinkage estimator\n",
    "3. **DDPM Diffusion**: Train a diffusion model, generate samples, compute covariance\n",
    "\n",
    "For each method, we compute both **Minimum Variance (MVP)** and **Maximum Sharpe** portfolios.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this homework, you will:\n",
    "- Understand the forward and reverse diffusion processes in DDPM\n",
    "- Implement key components of a diffusion model\n",
    "- Apply diffusion models to financial covariance estimation\n",
    "- Compare different covariance estimators in a portfolio optimization context\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Fill in the code where you see `### YOUR CODE HERE ###`\n",
    "- Answer the conceptual and interpretation questions in the markdown cells\n",
    "- Run all cells to verify your implementation\n",
    "- The backtest may take 10-30 minutes depending on your hardware\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|---------|--------|\n",
    "| Exercise 1 (ScoreNet) | 10 |\n",
    "| Exercise 2 (add_noise) | 15 |\n",
    "| Exercise 3 (sample) | 15 |\n",
    "| Exercise 4 (MVP weights) | 10 |\n",
    "| Exercise 5 (MaxSharpe) | 10 |\n",
    "| Exercise 6 (DiffusionCov) | 15 |\n",
    "| Conceptual Q1-Q3 | 10 |\n",
    "| Interpretation Q4-Q7 | 15 |\n",
    "| **Total** | **100** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Data Loading\n",
    "\n",
    "Run the following cells to set up the environment and load the ETF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sklearn covariance estimators\n",
    "from sklearn.covariance import LedoitWolf, EmpiricalCovariance\n",
    "\n",
    "# Scipy for portfolio optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean ETF data\n",
    "df_raw = pd.read_csv('etf_db.csv')\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"ETFs in dataset: {df_raw['etf'].unique().tolist()}\")\n",
    "\n",
    "# Select ETFs (excluding SPY as benchmark)\n",
    "etf_names = ['XLK', 'XLI', 'XLV', 'XLE', 'XLP', 'XLY', 'XTL', 'XLRE', 'XLB', 'XLF', 'XLU']\n",
    "df_raw = df_raw[df_raw['etf'].isin(etf_names)].copy()\n",
    "\n",
    "# Parse dates and clean data\n",
    "df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
    "df_raw = df_raw[df_raw['Date'].dt.dayofweek < 5]  # Remove weekends\n",
    "\n",
    "# Remove March 2020 (COVID crash)\n",
    "march_2020_mask = (df_raw['Date'].dt.year == 2020) & (df_raw['Date'].dt.month == 3)\n",
    "df_raw = df_raw[~march_2020_mask]\n",
    "\n",
    "# Compute returns\n",
    "df_raw = df_raw.sort_values(['etf', 'Date']).reset_index(drop=True)\n",
    "df_raw['pct_return'] = df_raw.groupby('etf')['Adj Close'].transform(lambda x: x.pct_change())\n",
    "df_raw = df_raw.dropna(subset=['pct_return'])\n",
    "\n",
    "# Pivot to returns matrix\n",
    "returns_df = df_raw.pivot(index='Date', columns='etf', values='pct_return')\n",
    "returns_df = returns_df.sort_index().dropna()\n",
    "\n",
    "print(f\"\\nReturns matrix shape: {returns_df.shape}\")\n",
    "print(f\"Date range: {returns_df.index.min().date()} to {returns_df.index.max().date()}\")\n",
    "print(f\"Number of assets: {returns_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Diffusion Models (Conceptual Questions)\n",
    "\n",
    "Before implementing, let's make sure you understand the key concepts behind DDPM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (3 points)\n",
    "\n",
    "In the **forward diffusion process** of DDPM, which of the following is TRUE?\n",
    "\n",
    "A) We gradually remove noise from data to recover the original signal  \n",
    "B) We gradually add Gaussian noise to data until it becomes pure noise  \n",
    "C) We train a neural network to predict the next timestep  \n",
    "D) We use a deterministic transformation to encode data  \n",
    "\n",
    "**Your Answer:** [Write A, B, C, or D]\n",
    "\n",
    "**Explanation (1-2 sentences):**\n",
    "\n",
    "[Your explanation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (3 points)\n",
    "\n",
    "The noise schedule in DDPM is defined by $\\beta_t$ values that typically increase from $\\beta_1 \\approx 10^{-4}$ to $\\beta_T \\approx 0.02$.\n",
    "\n",
    "**Why do we use small $\\beta_t$ values at the beginning and larger values at the end?**\n",
    "\n",
    "[Your answer here - 2-3 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (4 points)\n",
    "\n",
    "In DDPM, the neural network is trained to predict the **noise** $\\epsilon$ that was added, rather than predicting the clean data $x_0$ directly.\n",
    "\n",
    "**What is the advantage of predicting noise instead of the clean data?**\n",
    "\n",
    "[Your answer here - 2-3 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implement the Score Network\n",
    "\n",
    "The ScoreNet is a neural network that predicts the noise added at each timestep. It takes:\n",
    "- `x`: The noisy data at timestep t\n",
    "- `t`: The timestep\n",
    "\n",
    "And outputs the predicted noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (10 points)\n",
    "\n",
    "Complete the `forward()` method of the ScoreNet class.\n",
    "\n",
    "**Steps:**\n",
    "1. Normalize the timestep `t` by dividing by 1000.0 and add a dimension\n",
    "2. Pass the normalized timestep through `self.time_embed` to get the time embedding\n",
    "3. Concatenate `x` and the time embedding along the last dimension\n",
    "4. Pass the concatenated tensor through `self.net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP for noise prediction in DDPM.\n",
    "    \n",
    "    Takes noisy data x_t and timestep t, predicts the noise that was added.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 2, hidden_dim: int = 128, time_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding network\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, time_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # Main network: takes concatenated [x, time_embedding]\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Noisy data [batch_size, input_dim]\n",
    "            t: Timesteps [batch_size] (integer values from 0 to T-1)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted noise [batch_size, input_dim]\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Step 1: Normalize timestep and add dimension for the linear layer\n",
    "        # Hint: t is shape [batch_size], we need [batch_size, 1]\n",
    "        t_normalized = None  # TODO\n",
    "        \n",
    "        # Step 2: Get time embedding\n",
    "        t_emb = None  # TODO\n",
    "        \n",
    "        # Step 3: Concatenate x and time embedding\n",
    "        # Hint: use torch.cat with dim=-1\n",
    "        x_with_time = None  # TODO\n",
    "        \n",
    "        # Step 4: Pass through main network\n",
    "        output = None  # TODO\n",
    "        \n",
    "        return output\n",
    "        ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your ScoreNet implementation\n",
    "test_model = ScoreNet(input_dim=11, hidden_dim=64, time_dim=16).to(device)\n",
    "test_x = torch.randn(32, 11).to(device)\n",
    "test_t = torch.randint(0, 1000, (32,)).to(device)\n",
    "\n",
    "try:\n",
    "    test_output = test_model(test_x, test_t)\n",
    "    assert test_output.shape == (32, 11), f\"Expected shape (32, 11), got {test_output.shape}\"\n",
    "    print(\"ScoreNet test PASSED!\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ScoreNet test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implement the DDPM Forward Process\n",
    "\n",
    "The forward process gradually adds noise to the data according to:\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
    "\n",
    "This can be reparameterized as:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s = \\prod_{s=1}^{t} (1 - \\beta_s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (15 points)\n",
    "\n",
    "Complete the `add_noise()` method of the DDPM class.\n",
    "\n",
    "**Steps:**\n",
    "1. Sample random noise $\\epsilon$ with the same shape as $x_0$\n",
    "2. Get $\\sqrt{\\bar{\\alpha}_t}$ for the given timesteps (already precomputed)\n",
    "3. Get $\\sqrt{1 - \\bar{\\alpha}_t}$ for the given timesteps\n",
    "4. Compute $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n",
    "\n",
    "**Hint:** Use `unsqueeze(-1)` to add a dimension for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM:\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model.\n",
    "    \n",
    "    Implements the forward (noising) and reverse (denoising) diffusion processes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Linear noise schedule: beta_t increases linearly from beta_start to beta_end\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        \n",
    "        # alpha_t = 1 - beta_t\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        \n",
    "        # alpha_bar_t = cumulative product of alphas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Precomputed values for the forward process\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "    \n",
    "    def add_noise(self, x_0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to data.\n",
    "        \n",
    "        Implements: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean data [batch_size, dim]\n",
    "            t: Timesteps [batch_size] (integer indices)\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Noisy data [batch_size, dim]\n",
    "            noise: The noise that was added [batch_size, dim]\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Step 1: Sample random Gaussian noise with same shape as x_0\n",
    "        noise = None  # TODO\n",
    "        \n",
    "        # Step 2: Get sqrt(alpha_bar_t) for the given timesteps\n",
    "        # Hint: Index into self.sqrt_alpha_cumprod using t, then unsqueeze for broadcasting\n",
    "        sqrt_alpha_cumprod_t = None  # TODO\n",
    "        \n",
    "        # Step 3: Get sqrt(1 - alpha_bar_t) for the given timesteps\n",
    "        sqrt_one_minus_alpha_cumprod_t = None  # TODO\n",
    "        \n",
    "        # Step 4: Compute x_t using the reparameterization\n",
    "        x_t = None  # TODO\n",
    "        \n",
    "        return x_t, noise\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    def sample(self, model: nn.Module, n_samples: int, input_dim: int, \n",
    "               show_progress: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse diffusion: generate samples from noise.\n",
    "        This method will be completed in Exercise 3.\n",
    "        \"\"\"\n",
    "        # Placeholder - will be implemented in Exercise 3\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your add_noise implementation\n",
    "test_ddpm = DDPM(timesteps=1000)\n",
    "test_x0 = torch.randn(32, 11).to(device)\n",
    "test_t = torch.randint(0, 1000, (32,)).to(device)\n",
    "\n",
    "try:\n",
    "    x_t, noise = test_ddpm.add_noise(test_x0, test_t)\n",
    "    assert x_t.shape == test_x0.shape, f\"x_t shape mismatch: {x_t.shape} vs {test_x0.shape}\"\n",
    "    assert noise.shape == test_x0.shape, f\"noise shape mismatch: {noise.shape} vs {test_x0.shape}\"\n",
    "    \n",
    "    # Check that noise is approximately standard normal\n",
    "    noise_mean = noise.mean().item()\n",
    "    noise_std = noise.std().item()\n",
    "    assert abs(noise_mean) < 0.5, f\"Noise mean too far from 0: {noise_mean}\"\n",
    "    assert 0.5 < noise_std < 1.5, f\"Noise std unexpected: {noise_std}\"\n",
    "    \n",
    "    print(\"add_noise test PASSED!\")\n",
    "    print(f\"x_t shape: {x_t.shape}\")\n",
    "    print(f\"Noise mean: {noise_mean:.4f}, std: {noise_std:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"add_noise test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Implement the DDPM Reverse Process\n",
    "\n",
    "The reverse process iteratively denoises from $x_T$ (pure noise) back to $x_0$ (clean data).\n",
    "\n",
    "At each step, we compute:\n",
    "\n",
    "$$\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)$$\n",
    "\n",
    "Then sample:\n",
    "$$x_{t-1} = \\mu_\\theta(x_t, t) + \\sqrt{\\beta_t} \\cdot z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ (except for $t=0$ where we don't add noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (15 points)\n",
    "\n",
    "Complete the `sample()` method of the DDPM class.\n",
    "\n",
    "The skeleton is provided. You need to fill in the computation of `x_mean` (the denoised estimate at each step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM:\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model - Complete Implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Linear noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Precomputed values\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "    \n",
    "    def add_noise(self, x_0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward diffusion - copy your implementation from Exercise 2.\"\"\"\n",
    "        ### YOUR CODE HERE (copy from Exercise 2) ###\n",
    "        noise = None  # TODO\n",
    "        sqrt_alpha_cumprod_t = None  # TODO\n",
    "        sqrt_one_minus_alpha_cumprod_t = None  # TODO\n",
    "        x_t = None  # TODO\n",
    "        return x_t, noise\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    def sample(self, model: nn.Module, n_samples: int, input_dim: int, \n",
    "               show_progress: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse diffusion: generate samples from noise.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained noise prediction model (ScoreNet)\n",
    "            n_samples: Number of samples to generate\n",
    "            input_dim: Dimension of each sample\n",
    "            show_progress: Whether to show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            Generated samples [n_samples, input_dim]\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Start from pure noise x_T ~ N(0, I)\n",
    "        x = torch.randn(n_samples, input_dim).to(device)\n",
    "        \n",
    "        # Iterate from t = T-1 down to t = 0\n",
    "        iterator = reversed(range(self.timesteps))\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Sampling\", total=self.timesteps)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t in iterator:\n",
    "                # Create timestep tensor\n",
    "                t_tensor = torch.full((n_samples,), t, dtype=torch.long).to(device)\n",
    "                \n",
    "                # Predict the noise using the model\n",
    "                predicted_noise = model(x, t_tensor)\n",
    "                \n",
    "                # Get the coefficients for this timestep\n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "                beta_t = self.betas[t]\n",
    "                \n",
    "                ### YOUR CODE HERE ###\n",
    "                # Compute x_mean (the denoised estimate)\n",
    "                # Formula: x_mean = (1/sqrt(alpha_t)) * (x - (beta_t/sqrt(1-alpha_bar_t)) * predicted_noise)\n",
    "                #\n",
    "                # Hint: Use torch.sqrt() for square roots\n",
    "                x_mean = None  # TODO\n",
    "                ### END YOUR CODE ###\n",
    "                \n",
    "                if t > 0:\n",
    "                    # Add noise for all steps except the last\n",
    "                    noise = torch.randn_like(x)\n",
    "                    x = x_mean + torch.sqrt(beta_t) * noise\n",
    "                else:\n",
    "                    # Last step: no noise added\n",
    "                    x = x_mean\n",
    "        \n",
    "        model.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training Function\n",
    "\n",
    "This function is provided for you. It trains the DDPM by:\n",
    "1. Sampling random timesteps\n",
    "2. Adding noise to the data\n",
    "3. Predicting the noise with the model\n",
    "4. Computing MSE loss between predicted and actual noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpm(\n",
    "    model: nn.Module,\n",
    "    data: np.ndarray,\n",
    "    ddpm: DDPM,\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-3,\n",
    "    verbose: bool = False\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Train the DDPM model.\n",
    "    \n",
    "    Args:\n",
    "        model: ScoreNet model\n",
    "        data: Training data [n_samples, dim]\n",
    "        ddpm: DDPM instance\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        verbose: Print progress\n",
    "        \n",
    "    Returns:\n",
    "        List of losses per epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    data_tensor = torch.from_numpy(data.astype(np.float32)).to(device)\n",
    "    n_samples = len(data_tensor)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Mini-batch training\n",
    "        indices = torch.randperm(n_samples)\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            batch = data_tensor[batch_idx]\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, ddpm.timesteps, (len(batch),)).to(device)\n",
    "            \n",
    "            # Add noise\n",
    "            x_t, noise = ddpm.add_noise(batch, t)\n",
    "            \n",
    "            # Predict noise\n",
    "            predicted_noise = model(x_t, t)\n",
    "            \n",
    "            # MSE loss\n",
    "            loss = nn.MSELoss()(predicted_noise, noise)\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Portfolio Optimization\n",
    "\n",
    "Now let's implement the portfolio optimization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (10 points)\n",
    "\n",
    "Implement the **Minimum Variance Portfolio** weights.\n",
    "\n",
    "The analytical solution is:\n",
    "$$w_{MVP} = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^T \\Sigma^{-1} \\mathbf{1}}$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix and $\\mathbf{1}$ is a vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum_variance_weights(cov_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute minimum variance portfolio weights.\n",
    "    \n",
    "    Args:\n",
    "        cov_matrix: Covariance matrix [n_assets, n_assets]\n",
    "        \n",
    "    Returns:\n",
    "        Portfolio weights [n_assets, 1]\n",
    "    \"\"\"\n",
    "    n_assets = cov_matrix.shape[0]\n",
    "    ones = np.ones((n_assets, 1))\n",
    "    \n",
    "    # Add small regularization for numerical stability\n",
    "    cov_reg = cov_matrix + np.eye(n_assets) * 1e-8\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    # Step 1: Compute the inverse of the covariance matrix\n",
    "    # Hint: Use np.linalg.inv() or np.linalg.pinv() for pseudo-inverse\n",
    "    try:\n",
    "        cov_inv = None  # TODO\n",
    "    except np.linalg.LinAlgError:\n",
    "        cov_inv = np.linalg.pinv(cov_reg)\n",
    "    \n",
    "    # Step 2: Compute unnormalized weights: Sigma^{-1} @ ones\n",
    "    weights_unnorm = None  # TODO\n",
    "    \n",
    "    # Step 3: Normalize so weights sum to 1\n",
    "    # Formula: weights = weights_unnorm / (ones.T @ Sigma^{-1} @ ones)\n",
    "    weights = None  # TODO\n",
    "    \n",
    "    return weights\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MVP weights\n",
    "test_cov = np.array([[0.04, 0.01], [0.01, 0.09]])  # 2-asset example\n",
    "mvp_weights = get_minimum_variance_weights(test_cov)\n",
    "\n",
    "try:\n",
    "    assert mvp_weights.shape == (2, 1), f\"Shape mismatch: {mvp_weights.shape}\"\n",
    "    assert abs(mvp_weights.sum() - 1.0) < 1e-6, f\"Weights don't sum to 1: {mvp_weights.sum()}\"\n",
    "    # Expected: higher weight on lower variance asset\n",
    "    assert mvp_weights[0] > mvp_weights[1], \"Lower variance asset should have higher weight\"\n",
    "    print(\"MVP weights test PASSED!\")\n",
    "    print(f\"Weights: {mvp_weights.flatten()}\")\n",
    "except Exception as e:\n",
    "    print(f\"MVP weights test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (10 points)\n",
    "\n",
    "Implement the **Maximum Sharpe Portfolio** optimization.\n",
    "\n",
    "We maximize the Sharpe ratio:\n",
    "$$\\text{Sharpe} = \\frac{w^T \\mu - r_f}{\\sqrt{w^T \\Sigma w}}$$\n",
    "\n",
    "Subject to: $\\sum_i w_i = 1$\n",
    "\n",
    "We'll use scipy.optimize.minimize to minimize the **negative** Sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_sharpe_weights(\n",
    "    cov_matrix: np.ndarray, \n",
    "    expected_returns: np.ndarray,\n",
    "    risk_free_rate: float = 0.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute maximum Sharpe ratio portfolio weights using optimization.\n",
    "    \n",
    "    Args:\n",
    "        cov_matrix: Covariance matrix [n_assets, n_assets]\n",
    "        expected_returns: Expected returns [n_assets, 1] or [n_assets,]\n",
    "        risk_free_rate: Annual risk-free rate (daily scale will be computed)\n",
    "        \n",
    "    Returns:\n",
    "        Portfolio weights [n_assets, 1]\n",
    "    \"\"\"\n",
    "    n_assets = cov_matrix.shape[0]\n",
    "    expected_returns = expected_returns.flatten()\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    def neg_sharpe(weights):\n",
    "        \"\"\"\n",
    "        Compute negative Sharpe ratio (we minimize this).\n",
    "        \n",
    "        Args:\n",
    "            weights: Portfolio weights [n_assets,]\n",
    "        Returns:\n",
    "            Negative Sharpe ratio (scalar)\n",
    "        \"\"\"\n",
    "        # Step 1: Compute portfolio return: w^T @ mu\n",
    "        port_return = None  # TODO\n",
    "        \n",
    "        # Step 2: Compute portfolio volatility: sqrt(w^T @ Sigma @ w)\n",
    "        # Hint: Use np.dot for matrix multiplication\n",
    "        port_vol = None  # TODO\n",
    "        \n",
    "        # Handle edge case of zero volatility\n",
    "        if port_vol < 1e-10:\n",
    "            return 1e10\n",
    "        \n",
    "        # Step 3: Compute Sharpe ratio and return its negative\n",
    "        # Note: risk_free_rate is annual, divide by 252 for daily\n",
    "        sharpe = None  # TODO\n",
    "        \n",
    "        return -sharpe  # Return negative because we're minimizing\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    # Constraint: weights sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    \n",
    "    # Bounds: allow short selling (-1 to 1)\n",
    "    bounds = [(-1, 1) for _ in range(n_assets)]\n",
    "    \n",
    "    # Initial guess: equal weights\n",
    "    w0 = np.ones(n_assets) / n_assets\n",
    "    \n",
    "    result = minimize(\n",
    "        neg_sharpe, \n",
    "        w0, \n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'maxiter': 1000}\n",
    "    )\n",
    "    \n",
    "    return result.x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MaxSharpe weights\n",
    "test_cov = np.array([[0.04, 0.01], [0.01, 0.09]])\n",
    "test_returns = np.array([[0.10], [0.05]])  # First asset has higher return\n",
    "\n",
    "try:\n",
    "    ms_weights = get_maximum_sharpe_weights(test_cov, test_returns, risk_free_rate=0.02)\n",
    "    assert ms_weights.shape == (2, 1), f\"Shape mismatch: {ms_weights.shape}\"\n",
    "    assert abs(ms_weights.sum() - 1.0) < 1e-4, f\"Weights don't sum to 1: {ms_weights.sum()}\"\n",
    "    # Higher return & lower vol asset should have higher weight\n",
    "    assert ms_weights[0] > ms_weights[1], \"Better risk-adjusted asset should have higher weight\"\n",
    "    print(\"MaxSharpe weights test PASSED!\")\n",
    "    print(f\"Weights: {ms_weights.flatten()}\")\n",
    "except Exception as e:\n",
    "    print(f\"MaxSharpe weights test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Diffusion Covariance Estimator\n",
    "\n",
    "Now let's put it all together into a covariance estimator that:\n",
    "1. Trains a DDPM on return data\n",
    "2. Generates synthetic samples\n",
    "3. Computes covariance from the generated samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (15 points)\n",
    "\n",
    "Complete the `fit()` method of the DiffusionCovarianceEstimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionCovarianceEstimator:\n",
    "    \"\"\"\n",
    "    Covariance estimator using DDPM diffusion model.\n",
    "    \n",
    "    The approach:\n",
    "    1. Train a generative diffusion model on historical returns\n",
    "    2. Generate many synthetic samples from the learned distribution\n",
    "    3. Compute covariance from the generated samples\n",
    "    \n",
    "    This can help \"denoise\" the sample covariance by learning the underlying distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        timesteps: int = 1000,\n",
    "        hidden_dim: int = 128,\n",
    "        time_dim: int = 32,\n",
    "        epochs: int = 200,\n",
    "        batch_size: int = 64,\n",
    "        lr: float = 1e-3,\n",
    "        n_gen_samples: int = 1000\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_gen_samples = n_gen_samples\n",
    "        \n",
    "        self.model = None\n",
    "        self.ddpm = None\n",
    "        self.covariance_ = None\n",
    "        self.input_dim = None\n",
    "        self.training_mean = None\n",
    "        self.training_std = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the diffusion model on returns data.\n",
    "        \n",
    "        Args:\n",
    "            X: Returns data [n_samples, n_assets]\n",
    "            \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.input_dim = X.shape[1]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Step 1: Standardize the data for better training\n",
    "        # Compute mean and std, then standardize: X_std = (X - mean) / std\n",
    "        self.training_mean = None  # TODO: X.mean(axis=0)\n",
    "        self.training_std = None  # TODO: X.std(axis=0) + 1e-8 (add small epsilon)\n",
    "        X_standardized = None  # TODO\n",
    "        \n",
    "        # Step 2: Initialize the ScoreNet model\n",
    "        self.model = None  # TODO: Create ScoreNet with appropriate dimensions\n",
    "        \n",
    "        # Step 3: Initialize the DDPM\n",
    "        self.ddpm = None  # TODO: Create DDPM with self.timesteps\n",
    "        \n",
    "        # Step 4: Train the model (use train_ddpm function)\n",
    "        # TODO: Call train_ddpm with appropriate arguments\n",
    "        \n",
    "        # Step 5: Generate samples using the trained model\n",
    "        # TODO: Use self.ddpm.sample() to generate self.n_gen_samples samples\n",
    "        generated = None  # TODO\n",
    "        \n",
    "        # Step 6: Convert to numpy and unstandardize\n",
    "        generated_np = None  # TODO: Convert to numpy\n",
    "        generated_unstd = None  # TODO: Reverse the standardization\n",
    "        \n",
    "        # Step 7: Compute covariance from generated samples\n",
    "        self.covariance_ = None  # TODO: Use np.cov with rowvar=False\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DiffusionCovarianceEstimator with a small example\n",
    "print(\"Testing DiffusionCovarianceEstimator (this may take a minute)...\")\n",
    "\n",
    "# Use a small subset of data for testing\n",
    "test_returns = returns_df.iloc[:100].values  # 100 days, 11 assets\n",
    "\n",
    "try:\n",
    "    estimator = DiffusionCovarianceEstimator(\n",
    "        timesteps=100,  # Reduced for testing\n",
    "        hidden_dim=64,\n",
    "        epochs=50,      # Reduced for testing\n",
    "        n_gen_samples=200\n",
    "    )\n",
    "    estimator.fit(test_returns)\n",
    "    \n",
    "    assert estimator.covariance_ is not None, \"Covariance not computed\"\n",
    "    assert estimator.covariance_.shape == (11, 11), f\"Wrong shape: {estimator.covariance_.shape}\"\n",
    "    \n",
    "    # Check symmetry\n",
    "    assert np.allclose(estimator.covariance_, estimator.covariance_.T), \"Covariance not symmetric\"\n",
    "    \n",
    "    print(\"DiffusionCovarianceEstimator test PASSED!\")\n",
    "    print(f\"Covariance shape: {estimator.covariance_.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"DiffusionCovarianceEstimator test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Run the Backtest\n",
    "\n",
    "Now we'll run the full backtest comparing all three covariance estimation methods. This may take 10-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest configuration\n",
    "LOOKBACK_WINDOW = 252  # 1 year of trading days\n",
    "REBALANCE_FREQ = 21    # Monthly rebalancing\n",
    "RISK_FREE_RATE = 0.02  # Annual risk-free rate\n",
    "\n",
    "# Weight constraints\n",
    "MAX_WEIGHT = 0.30\n",
    "MIN_WEIGHT = -0.10\n",
    "\n",
    "# DDPM parameters (reduced for faster execution)\n",
    "DDPM_TIMESTEPS = 500\n",
    "DDPM_HIDDEN_DIM = 128\n",
    "DDPM_TIME_DIM = 32\n",
    "DDPM_EPOCHS = 100\n",
    "DDPM_BATCH_SIZE = 64\n",
    "DDPM_LR = 1e-3\n",
    "DDPM_GEN_SAMPLES = 500\n",
    "\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW} trading days\")\n",
    "print(f\"Rebalance frequency: {REBALANCE_FREQ} trading days\")\n",
    "print(f\"Weight bounds: [{MIN_WEIGHT:.0%}, {MAX_WEIGHT:.0%}]\")\n",
    "print(f\"\\nBacktest period: {returns_df.index[LOOKBACK_WINDOW].date()} to {returns_df.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(\n",
    "    returns_df: pd.DataFrame,\n",
    "    lookback_window: int,\n",
    "    rebalance_freq: int,\n",
    "    max_weight: float = 0.5,\n",
    "    min_weight: float = -0.5,\n",
    "    ddpm_config: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run backtest comparing Empirical, Ledoit-Wolf, and Diffusion covariance estimators.\n",
    "    \"\"\"\n",
    "    n_assets = returns_df.shape[1]\n",
    "    dates = returns_df.index[lookback_window:]\n",
    "    returns_array = returns_df.values\n",
    "    \n",
    "    # Methods and strategies\n",
    "    methods = ['Empirical', 'Ledoit-Wolf', 'Diffusion']\n",
    "    strategies = ['MVP', 'MaxSharpe']\n",
    "    portfolio_names = [f\"{m}_{s}\" for m in methods for s in strategies]\n",
    "    \n",
    "    # Initialize\n",
    "    portfolio_values = {name: [1.0] for name in portfolio_names}\n",
    "    portfolio_values['EqualWeight'] = [1.0]\n",
    "    \n",
    "    equal_weights = np.ones((n_assets, 1)) / n_assets\n",
    "    current_weights = {name: equal_weights.copy() for name in portfolio_names}\n",
    "    \n",
    "    weights_history = {name: [] for name in portfolio_names}\n",
    "    rebalance_dates = []\n",
    "    \n",
    "    days_since_rebalance = rebalance_freq\n",
    "    \n",
    "    def clip_and_normalize(weights, min_w, max_w):\n",
    "        clipped = np.clip(weights, min_w, max_w)\n",
    "        return clipped / clipped.sum()\n",
    "    \n",
    "    empirical_est = EmpiricalCovariance()\n",
    "    lw_est = LedoitWolf()\n",
    "    \n",
    "    if ddpm_config is None:\n",
    "        ddpm_config = {}\n",
    "    \n",
    "    for t in tqdm(range(lookback_window, len(returns_df)), desc='Backtesting'):\n",
    "        if days_since_rebalance >= rebalance_freq:\n",
    "            trailing_returns = returns_array[t - lookback_window:t, :]\n",
    "            mean_returns = np.mean(trailing_returns, axis=0).reshape(-1, 1)\n",
    "            \n",
    "            # Empirical\n",
    "            empirical_est.fit(trailing_returns)\n",
    "            cov_empirical = empirical_est.covariance_\n",
    "            \n",
    "            # Ledoit-Wolf\n",
    "            lw_est.fit(trailing_returns)\n",
    "            cov_lw = lw_est.covariance_\n",
    "            \n",
    "            # Diffusion\n",
    "            diffusion_est = DiffusionCovarianceEstimator(**ddpm_config)\n",
    "            diffusion_est.fit(trailing_returns)\n",
    "            cov_diffusion = diffusion_est.covariance_\n",
    "            \n",
    "            covariances = {\n",
    "                'Empirical': cov_empirical,\n",
    "                'Ledoit-Wolf': cov_lw,\n",
    "                'Diffusion': cov_diffusion\n",
    "            }\n",
    "            \n",
    "            for method, cov_matrix in covariances.items():\n",
    "                try:\n",
    "                    mvp_weights = get_minimum_variance_weights(cov_matrix)\n",
    "                    current_weights[f\"{method}_MVP\"] = clip_and_normalize(\n",
    "                        mvp_weights, min_weight, max_weight\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    ms_weights = get_maximum_sharpe_weights(cov_matrix, mean_returns)\n",
    "                    current_weights[f\"{method}_MaxSharpe\"] = clip_and_normalize(\n",
    "                        ms_weights, min_weight, max_weight\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            for name in portfolio_names:\n",
    "                weights_history[name].append(current_weights[name].flatten())\n",
    "            \n",
    "            rebalance_dates.append(returns_df.index[t])\n",
    "            days_since_rebalance = 0\n",
    "        \n",
    "        today_returns = returns_array[t, :].reshape(-1, 1)\n",
    "        \n",
    "        for name in portfolio_names:\n",
    "            port_return = (current_weights[name].T @ today_returns).item()\n",
    "            portfolio_values[name].append(\n",
    "                portfolio_values[name][-1] * (1 + port_return)\n",
    "            )\n",
    "        \n",
    "        eq_return = (equal_weights.T @ today_returns).item()\n",
    "        portfolio_values['EqualWeight'].append(\n",
    "            portfolio_values['EqualWeight'][-1] * (1 + eq_return)\n",
    "        )\n",
    "        \n",
    "        days_since_rebalance += 1\n",
    "    \n",
    "    all_dates = [returns_df.index[lookback_window - 1]] + list(dates)\n",
    "    portfolio_df = pd.DataFrame(portfolio_values, index=all_dates)\n",
    "    \n",
    "    weights_dfs = {}\n",
    "    for name in portfolio_names:\n",
    "        weights_dfs[name] = pd.DataFrame(\n",
    "            weights_history[name],\n",
    "            index=rebalance_dates,\n",
    "            columns=returns_df.columns\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'portfolio_values': portfolio_df,\n",
    "        'weights': weights_dfs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the backtest\n",
    "ddpm_config = {\n",
    "    'timesteps': DDPM_TIMESTEPS,\n",
    "    'hidden_dim': DDPM_HIDDEN_DIM,\n",
    "    'time_dim': DDPM_TIME_DIM,\n",
    "    'epochs': DDPM_EPOCHS,\n",
    "    'batch_size': DDPM_BATCH_SIZE,\n",
    "    'lr': DDPM_LR,\n",
    "    'n_gen_samples': DDPM_GEN_SAMPLES\n",
    "}\n",
    "\n",
    "n_rebalances = (len(returns_df) - LOOKBACK_WINDOW) // REBALANCE_FREQ + 1\n",
    "print(f\"Running backtest...\")\n",
    "print(f\"Number of rebalancing periods: {n_rebalances}\")\n",
    "print(f\"Estimated time: 10-30 minutes\\n\")\n",
    "\n",
    "results = run_backtest(\n",
    "    returns_df,\n",
    "    LOOKBACK_WINDOW,\n",
    "    REBALANCE_FREQ,\n",
    "    max_weight=MAX_WEIGHT,\n",
    "    min_weight=MIN_WEIGHT,\n",
    "    ddpm_config=ddpm_config\n",
    ")\n",
    "\n",
    "portfolio_values = results['portfolio_values']\n",
    "weights_dfs = results['weights']\n",
    "\n",
    "print(f\"\\nBacktest complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(portfolio_values: pd.DataFrame, risk_free_rate: float = 0.02):\n",
    "    \"\"\"Compute performance metrics for all portfolios.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for col in portfolio_values.columns:\n",
    "        values = portfolio_values[col]\n",
    "        returns = values.pct_change().dropna()\n",
    "        \n",
    "        total_return = values.iloc[-1] / values.iloc[0] - 1\n",
    "        n_years = len(returns) / 252\n",
    "        ann_return = (1 + total_return) ** (1 / n_years) - 1\n",
    "        ann_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = (ann_return - risk_free_rate) / ann_vol\n",
    "        \n",
    "        cummax = values.cummax()\n",
    "        drawdown = (values - cummax) / cummax\n",
    "        max_drawdown = drawdown.min()\n",
    "        calmar = ann_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
    "        \n",
    "        metrics[col] = {\n",
    "            'Total Return': total_return,\n",
    "            'Ann. Return': ann_return,\n",
    "            'Ann. Volatility': ann_vol,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Max Drawdown': max_drawdown,\n",
    "            'Calmar Ratio': calmar\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "metrics_df = compute_performance_metrics(portfolio_values, RISK_FREE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "mvp_cols = [c for c in metrics_df.columns if 'MVP' in c]\n",
    "maxsharpe_cols = [c for c in metrics_df.columns if 'MaxSharpe' in c]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MINIMUM VARIANCE PORTFOLIOS\")\n",
    "print(\"=\" * 80)\n",
    "display_df = metrics_df[mvp_cols + ['EqualWeight']].copy()\n",
    "for col in display_df.columns:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.2%}\" if abs(x) < 10 else f\"{x:.2f}\")\n",
    "print(display_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MAXIMUM SHARPE PORTFOLIOS\")\n",
    "print(\"=\" * 80)\n",
    "display_df = metrics_df[maxsharpe_cols + ['EqualWeight']].copy()\n",
    "for col in display_df.columns:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.2%}\" if abs(x) < 10 else f\"{x:.2f}\")\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "# MVP portfolios\n",
    "ax = axes[0]\n",
    "colors = {'Empirical_MVP': 'blue', 'Ledoit-Wolf_MVP': 'green', 'Diffusion_MVP': 'red', 'EqualWeight': 'gray'}\n",
    "for col in mvp_cols + ['EqualWeight']:\n",
    "    style = '--' if col == 'EqualWeight' else '-'\n",
    "    lw = 2.5 if 'Diffusion' in col else 1.5\n",
    "    portfolio_values[col].plot(ax=ax, label=col, linestyle=style, linewidth=lw, color=colors.get(col))\n",
    "ax.set_title('Minimum Variance Portfolios', fontsize=14)\n",
    "ax.set_ylabel('Portfolio Value')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# MaxSharpe portfolios\n",
    "ax = axes[1]\n",
    "colors = {'Empirical_MaxSharpe': 'blue', 'Ledoit-Wolf_MaxSharpe': 'green', 'Diffusion_MaxSharpe': 'red', 'EqualWeight': 'gray'}\n",
    "for col in maxsharpe_cols + ['EqualWeight']:\n",
    "    style = '--' if col == 'EqualWeight' else '-'\n",
    "    lw = 2.5 if 'Diffusion' in col else 1.5\n",
    "    portfolio_values[col].plot(ax=ax, label=col, linestyle=style, linewidth=lw, color=colors.get(col))\n",
    "ax.set_title('Maximum Sharpe Portfolios', fontsize=14)\n",
    "ax.set_ylabel('Portfolio Value')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpe ratio ranking\n",
    "print(\"=\" * 60)\n",
    "print(\"RANKING BY SHARPE RATIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMinimum Variance Portfolios:\")\n",
    "print(\"-\" * 40)\n",
    "mvp_sharpe = metrics_df.loc['Sharpe Ratio', mvp_cols].sort_values(ascending=False)\n",
    "for i, (name, sharpe) in enumerate(mvp_sharpe.items(), 1):\n",
    "    print(f\"{i}. {name}: {sharpe:.3f}\")\n",
    "\n",
    "print(\"\\nMaximum Sharpe Portfolios:\")\n",
    "print(\"-\" * 40)\n",
    "maxsharpe_sharpe = metrics_df.loc['Sharpe Ratio', maxsharpe_cols].sort_values(ascending=False)\n",
    "for i, (name, sharpe) in enumerate(maxsharpe_sharpe.items(), 1):\n",
    "    print(f\"{i}. {name}: {sharpe:.3f}\")\n",
    "\n",
    "print(f\"\\nEqual Weight: {metrics_df.loc['Sharpe Ratio', 'EqualWeight']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Interpretation Questions\n",
    "\n",
    "Based on the backtest results above, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (4 points)\n",
    "\n",
    "Compare the performance of the **Diffusion** method between MVP and MaxSharpe strategies.\n",
    "\n",
    "**Questions:**\n",
    "1. Which strategy performed better with Diffusion covariance estimation?\n",
    "2. What is the Sharpe ratio difference between the two strategies?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "[Your answer here - 2-3 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (4 points)\n",
    "\n",
    "The Diffusion method shows different relative performance for MVP vs MaxSharpe compared to traditional methods.\n",
    "\n",
    "**Why might the Diffusion-based covariance perform differently for these two optimization objectives?**\n",
    "\n",
    "Hint: Think about what each optimization objective is sensitive to (variance structure vs. mean-variance tradeoff).\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "[Your answer here - 3-4 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (4 points)\n",
    "\n",
    "Look at the weight evolution plots for the different methods.\n",
    "\n",
    "**What do you observe about the stability of weights over time for each covariance estimation method? Which method produces more stable allocations?**\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "[Your answer here - 2-3 sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight evolution for comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "comparisons = [\n",
    "    ('Empirical_MVP', 'Empirical - MVP'),\n",
    "    ('Ledoit-Wolf_MVP', 'Ledoit-Wolf - MVP'),\n",
    "    ('Diffusion_MVP', 'Diffusion - MVP'),\n",
    "    ('Empirical_MaxSharpe', 'Empirical - MaxSharpe'),\n",
    "    ('Ledoit-Wolf_MaxSharpe', 'Ledoit-Wolf - MaxSharpe'),\n",
    "    ('Diffusion_MaxSharpe', 'Diffusion - MaxSharpe'),\n",
    "]\n",
    "\n",
    "for ax, (key, title) in zip(axes.flat, comparisons):\n",
    "    weights_dfs[key].plot(ax=ax, linewidth=1, legend=False)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (3 points)\n",
    "\n",
    "**What are the main trade-offs of using diffusion-based covariance estimation compared to traditional methods like Ledoit-Wolf shrinkage?**\n",
    "\n",
    "Consider: computational cost, sample efficiency, assumptions, interpretability.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "[Your answer here - 3-4 sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Bonus Challenge (Optional - 10 extra points)\n",
    "\n",
    "Experiment with different DDPM hyperparameters and analyze their impact on portfolio performance.\n",
    "\n",
    "**Choose ONE of the following experiments:**\n",
    "\n",
    "A) Increase `n_gen_samples` from 500 to 2000. Does generating more samples improve covariance estimation?\n",
    "\n",
    "B) Increase `epochs` from 100 to 300. Does longer training help?\n",
    "\n",
    "C) Try a different `timesteps` value (e.g., 200 vs 1000). How does the noise schedule length affect results?\n",
    "\n",
    "**Document your findings below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Your experiment code here\n",
    "# Example: Re-run with different parameters\n",
    "\n",
    "# ddpm_config_bonus = {\n",
    "#     'timesteps': DDPM_TIMESTEPS,\n",
    "#     'hidden_dim': DDPM_HIDDEN_DIM,\n",
    "#     'time_dim': DDPM_TIME_DIM,\n",
    "#     'epochs': 300,  # Changed\n",
    "#     'batch_size': DDPM_BATCH_SIZE,\n",
    "#     'lr': DDPM_LR,\n",
    "#     'n_gen_samples': DDPM_GEN_SAMPLES\n",
    "# }\n",
    "\n",
    "# results_bonus = run_backtest(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Findings:**\n",
    "\n",
    "[Describe your experiment and findings here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this homework, you implemented:\n",
    "\n",
    "1. **ScoreNet**: A neural network for noise prediction in DDPM\n",
    "2. **Forward diffusion**: Adding noise to data according to a schedule\n",
    "3. **Reverse diffusion**: Iteratively denoising to generate samples\n",
    "4. **Portfolio optimization**: MVP and Maximum Sharpe portfolios\n",
    "5. **Diffusion covariance estimator**: Using generated samples for covariance estimation\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Diffusion models can learn the underlying distribution of financial returns\n",
    "- Generated samples can be used to compute \"denoised\" covariance estimates\n",
    "- The effectiveness depends on the optimization objective (MVP vs MaxSharpe)\n",
    "- There are computational trade-offs compared to analytical shrinkage methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
