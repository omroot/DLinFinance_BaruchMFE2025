{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Conditional GAN for Bond Pricing\n",
    "\n",
    "**Course**: Deep Learning in Finance (Baruch MFE 2025)  \n",
    "**Total Points**: 100\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this homework, you will be able to:\n",
    "\n",
    "1. **Compare** point predictions from conditional GANs vs traditional regression\n",
    "2. **Calculate** and interpret calibrated prediction intervals\n",
    "3. **Generate** correlated market scenarios using GANs\n",
    "4. **Compute** portfolio risk metrics (VaR, CVaR) from distributional predictions\n",
    "5. **Analyze** correlation structures and diversification opportunities\n",
    "6. **Articulate** when to use distributional vs point prediction models\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- **All model training code is provided** - you do NOT need to implement the GAN\n",
    "- **Your task**: Complete the analysis questions and provide interpretations\n",
    "- **Coding**: Some questions require simple calculations (means, percentiles, etc.)\n",
    "- **Interpretation**: Write 3-5 sentences explaining your findings\n",
    "- **Run all cells** in order before starting the questions\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "- **Section 1 (Model Comparison)**: 20 points\n",
    "- **Section 2 (Distributional Predictions)**: 25 points\n",
    "- **Section 3 (Portfolio Risk Analysis)**: 30 points\n",
    "- **Section 4 (Correlation & Diversification)**: 25 points\n",
    "\n",
    "**Total**: 100 points\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Setup & Model Training (Provided - Run All Cells)\n",
    "\n",
    "The following sections load data, train both ElasticNet and cGAN models.  \n",
    "**Simply run all cells below** until you reach the homework questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data/output directory\n",
    "DATA_DIR = Path.cwd()\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Verify file exists\n",
    "trace_file = DATA_DIR / 'trace_hy_merged.csv'\n",
    "if not trace_file.exists():\n",
    "    raise FileNotFoundError(f\"trace_hy_merged.csv not found in {DATA_DIR}\")\n",
    "\n",
    "print(f\"✓ Input file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TRACE data\n",
    "trace_hy = pd.read_csv(\n",
    "    DATA_DIR / \"trace_hy_merged.csv\",\n",
    "    parse_dates=[\"date\", \"maturity\", \"offering_date\"]\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Records: {len(trace_hy):,}\")\n",
    "print(f\"Unique bonds: {trace_hy['cusip_9'].nunique():,}\")\n",
    "print(f\"Trading days: {trace_hy['date'].nunique():,}\")\n",
    "print(f\"Date range: {trace_hy['date'].min()} to {trace_hy['date'].max()}\")\n",
    "\n",
    "# Chronological train/test split - ONE MONTH EACH\n",
    "date_range = sorted(trace_hy['date'].unique())\n",
    "train_size = 22  # ~1 month\n",
    "test_size = 22   # ~1 month\n",
    "\n",
    "train_dates = date_range[:train_size]\n",
    "test_dates = date_range[train_size:train_size + test_size]\n",
    "\n",
    "print(f\"\\nTRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train: {train_dates[0].date()} to {train_dates[-1].date()} ({len(train_dates)} days)\")\n",
    "print(f\"Test:  {test_dates[0].date()} to {test_dates[-1].date()} ({len(test_dates)} days)\")\n",
    "\n",
    "# Split data\n",
    "train_data = trace_hy[trace_hy['date'].isin(train_dates)].copy()\n",
    "test_data = trace_hy[trace_hy['date'].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "print(f\"Train bonds: {train_data['cusip_9'].nunique()}\")\n",
    "print(f\"Test bonds: {test_data['cusip_9'].nunique()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Universe for scenario analysis\n",
    "universe_df = trace_hy.drop_duplicates('cusip_9', keep='last').copy()\n",
    "print(f\"\\nUniverse size: {len(universe_df)} unique bonds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(bonds_df, reference_date, include_lagged_price=False, lagged_prices=None, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix for regression.\n",
    "    \"\"\"\n",
    "    df = bonds_df.copy()\n",
    "    \n",
    "    # Time-varying features\n",
    "    df['days_to_maturity'] = (df['maturity'] - reference_date).dt.days\n",
    "    df['years_to_maturity'] = df['days_to_maturity'] / 365.25\n",
    "    df['days_since_issue'] = (reference_date - df['offering_date']).dt.days\n",
    "    df['years_since_issue'] = df['days_since_issue'] / 365.25\n",
    "    \n",
    "    # Handle missing values\n",
    "    if 'amount_outstanding' in df.columns:\n",
    "        sector_rating_median = df.groupby(['sector', 'rating_broad'])['amount_outstanding'].transform('median')\n",
    "        df['amount_outstanding'] = df['amount_outstanding'].fillna(sector_rating_median)\n",
    "        df['amount_outstanding'] = df['amount_outstanding'].fillna(df['amount_outstanding'].median())\n",
    "    \n",
    "    if 'offering_yield' in df.columns:\n",
    "        rating_median = df.groupby('rating_broad')['offering_yield'].transform('median')\n",
    "        df['offering_yield'] = df['offering_yield'].fillna(rating_median)\n",
    "        df['offering_yield'] = df['offering_yield'].fillna(df['offering_yield'].median())\n",
    "    \n",
    "    # Numerical features\n",
    "    numerical_features = []\n",
    "    if 'amount_outstanding' in df.columns:\n",
    "        numerical_features.append('amount_outstanding')\n",
    "    if 'offering_yield' in df.columns:\n",
    "        numerical_features.append('offering_yield')\n",
    "    numerical_features.extend(['years_to_maturity', 'years_since_issue'])\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_features = []\n",
    "    if 'sector' in df.columns:\n",
    "        categorical_features.append('sector')\n",
    "    if 'rating_broad' in df.columns:\n",
    "        categorical_features.append('rating_broad')\n",
    "    if 'seniority' in df.columns:\n",
    "        categorical_features.append('seniority')\n",
    "    if 'coupon_type_label' in df.columns:\n",
    "        categorical_features.append('coupon_type_label')\n",
    "    \n",
    "    # Binary features\n",
    "    binary_features = []\n",
    "    for feat in ['putable', 'redeemable', 'convertible', 'rule_144a']:\n",
    "        if feat in df.columns:\n",
    "            df[feat] = (df[feat] == 'Y').astype(int)\n",
    "            binary_features.append(feat)\n",
    "    \n",
    "    # Build feature matrix\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for feat in numerical_features:\n",
    "        X[feat] = df[feat]\n",
    "    \n",
    "    for feat in binary_features:\n",
    "        X[feat] = df[feat]\n",
    "    \n",
    "    for feat in categorical_features:\n",
    "        dummies = pd.get_dummies(df[feat], prefix=feat, drop_first=True)\n",
    "        X = pd.concat([X, dummies], axis=1)\n",
    "    \n",
    "    if include_lagged_price and lagged_prices is not None:\n",
    "        X['lagged_price'] = df['cusip_9'].map(lagged_prices)\n",
    "        X['lagged_price'] = X['lagged_price'].fillna(X['lagged_price'].median())\n",
    "    \n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    if feature_columns is not None:\n",
    "        for col in feature_columns:\n",
    "            if col not in X.columns:\n",
    "                X[col] = 0\n",
    "        X = X[feature_columns]\n",
    "    \n",
    "    return X, list(X.columns)\n",
    "\n",
    "print(\"Feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train ElasticNet Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ElasticNet on training period...\\n\")\n",
    "\n",
    "# Prepare features for first training day\n",
    "first_date = train_dates[0]\n",
    "day1_data = train_data[train_data['date'] == first_date]\n",
    "X_day1, feature_names = prepare_features(day1_data, first_date, include_lagged_price=False)\n",
    "y_day1 = day1_data['price'].values\n",
    "\n",
    "# Train initial model\n",
    "scaler_enet = StandardScaler()\n",
    "X_day1_scaled = scaler_enet.fit_transform(X_day1)\n",
    "\n",
    "model_enet = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n",
    "    cv=5,\n",
    "    max_iter=10000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_enet.fit(X_day1_scaled, y_day1)\n",
    "\n",
    "print(f\"ElasticNet trained on {len(train_data)} samples\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Best alpha: {model_enet.alpha_:.6f}\")\n",
    "print(f\"Best l1_ratio: {model_enet.l1_ratio_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define & Train Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator: G(z, features) -> price\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_dim, condition_dim, hidden_dim=128, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        input_dim = noise_dim + condition_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, features):\n",
    "        gen_input = torch.cat([z, features], dim=1)\n",
    "        return self.net(gen_input)\n",
    "\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator: D(features, price) -> [0, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, condition_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        input_dim = condition_dim + 1\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, price):\n",
    "        disc_input = torch.cat([features, price], dim=1)\n",
    "        return self.net(disc_input).view(-1)\n",
    "\n",
    "\n",
    "print(\"Conditional GAN architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for train_date in train_dates:\n",
    "    day_data = train_data[train_data['date'] == train_date]\n",
    "    if len(day_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    X_day, _ = prepare_features(day_data, train_date, include_lagged_price=False, feature_columns=feature_names)\n",
    "    y_day = day_data['price'].values\n",
    "    \n",
    "    X_train_list.append(X_day.values)\n",
    "    y_train_list.append(y_day)\n",
    "\n",
    "X_train_cgan = np.vstack(X_train_list)\n",
    "y_train_cgan = np.concatenate(y_train_list).reshape(-1, 1)\n",
    "\n",
    "print(f\"Training samples: {len(X_train_cgan):,}\")\n",
    "\n",
    "# Normalize\n",
    "scaler_cgan = StandardScaler()\n",
    "X_train_cgan_scaled = scaler_cgan.fit_transform(X_train_cgan)\n",
    "\n",
    "price_mean = y_train_cgan.mean()\n",
    "price_std = y_train_cgan.std()\n",
    "y_train_cgan_normalized = (y_train_cgan - price_mean) / price_std\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(X_train_cgan_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train_cgan_normalized, dtype=torch.float32)\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"DataLoader: {len(dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Conditional GAN...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 20\n",
    "condition_dim = X_train_cgan_scaled.shape[1]\n",
    "hidden_dim = 128\n",
    "n_epochs = 2000\n",
    "lr = 1e-4\n",
    "\n",
    "# Initialize models\n",
    "G = ConditionalGenerator(noise_dim, condition_dim, hidden_dim).to(device)\n",
    "D = ConditionalDiscriminator(condition_dim, hidden_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "print(f\"Epochs: {n_epochs}\")\n",
    "print(\"Training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for features_real, prices_real in dataloader:\n",
    "        batch_size = features_real.size(0)\n",
    "        features_real = features_real.to(device)\n",
    "        prices_real = prices_real.to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, noise_dim, device=device)\n",
    "        prices_fake = G(z, features_real).detach()\n",
    "        \n",
    "        d_real = D(features_real, prices_real)\n",
    "        d_fake = D(features_real, prices_fake)\n",
    "        \n",
    "        d_loss = criterion(d_real, torch.ones_like(d_real)) + \\\n",
    "                 criterion(d_fake, torch.zeros_like(d_fake))\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, noise_dim, device=device)\n",
    "        prices_fake = G(z, features_real)\n",
    "        \n",
    "        g_loss = criterion(D(features_real, prices_fake), torch.ones_like(D(features_real, prices_fake)))\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    g_losses.append(epoch_g_loss / n_batches)\n",
    "    d_losses.append(epoch_d_loss / n_batches)\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch [{epoch+1:4d}/{n_epochs}] | D_loss: {d_losses[-1]:.4f} | G_loss: {g_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"cGAN TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cgan_samples(G, features, n_samples=1000, noise_dim=20, device=device):\n",
    "    \"\"\"\n",
    "    Generate multiple price samples for given features.\n",
    "    \"\"\"\n",
    "    n_bonds = features.shape[0]\n",
    "    samples = np.zeros((n_bonds, n_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            z = torch.randn(n_bonds, noise_dim, device=device)\n",
    "            prices_normalized = G(z, features_tensor).cpu().numpy().squeeze()\n",
    "            # Denormalize\n",
    "            prices = prices_normalized * price_std + price_mean\n",
    "            samples[:, i] = prices\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "# Storage\n",
    "test_results = {\n",
    "    'date': [],\n",
    "    'cusip': [],\n",
    "    'actual': [],\n",
    "    'enet_pred': [],\n",
    "    'cgan_mean': [],\n",
    "    'cgan_p05': [],\n",
    "    'cgan_p25': [],\n",
    "    'cgan_p50': [],\n",
    "    'cgan_p75': [],\n",
    "    'cgan_p95': []\n",
    "}\n",
    "\n",
    "# Store all cGAN samples for later analysis\n",
    "all_cgan_samples = {}  # cusip -> array of 1000 samples\n",
    "\n",
    "for test_date in test_dates:\n",
    "    day_data = test_data[test_data['date'] == test_date]\n",
    "    \n",
    "    if len(day_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Prepare features\n",
    "    X_day, _ = prepare_features(day_data, test_date, include_lagged_price=False, feature_columns=feature_names)\n",
    "    X_day_scaled = scaler_cgan.transform(X_day.values)\n",
    "    y_actual = day_data['price'].values\n",
    "    cusips = day_data['cusip_9'].values\n",
    "    \n",
    "    # ElasticNet predictions\n",
    "    enet_preds = model_enet.predict(scaler_enet.transform(X_day.values))\n",
    "    \n",
    "    # cGAN predictions (1000 samples per bond)\n",
    "    cgan_samples = generate_cgan_samples(G, X_day_scaled, n_samples=1000, noise_dim=noise_dim, device=device)\n",
    "    \n",
    "    # Store results\n",
    "    for j in range(len(day_data)):\n",
    "        cusip = cusips[j]\n",
    "        test_results['date'].append(test_date)\n",
    "        test_results['cusip'].append(cusip)\n",
    "        test_results['actual'].append(y_actual[j])\n",
    "        test_results['enet_pred'].append(enet_preds[j])\n",
    "        test_results['cgan_mean'].append(cgan_samples[j].mean())\n",
    "        test_results['cgan_p05'].append(np.percentile(cgan_samples[j], 5))\n",
    "        test_results['cgan_p25'].append(np.percentile(cgan_samples[j], 25))\n",
    "        test_results['cgan_p50'].append(np.percentile(cgan_samples[j], 50))\n",
    "        test_results['cgan_p75'].append(np.percentile(cgan_samples[j], 75))\n",
    "        test_results['cgan_p95'].append(np.percentile(cgan_samples[j], 95))\n",
    "        \n",
    "        # Store samples for this bond (last date only to save memory)\n",
    "        all_cgan_samples[cusip] = cgan_samples[j]\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(f\"Test predictions complete: {len(test_df):,} samples\")\n",
    "print(f\"Generated 1000 price samples per bond-day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Homework Questions (Complete These)\n",
    "\n",
    "Now that both models are trained and test predictions are generated, complete the following analysis questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Model Comparison (20 points)\n",
    "\n",
    "### Question 1.1: Generate Predictions and Calculate Metrics (10 points)\n",
    "\n",
    "**Task:**\n",
    "- Extract actual prices and predictions from `test_df`\n",
    "- Calculate RMSE (Root Mean Squared Error) for both models\n",
    "- Calculate MAE (Mean Absolute Error) for both models\n",
    "- Create a scatter plot comparing actual vs predicted prices for both models\n",
    "\n",
    "**Hints:**\n",
    "- Use `test_df['actual']`, `test_df['enet_pred']`, `test_df['cgan_mean']`\n",
    "- RMSE formula: `np.sqrt(mean_squared_error(actual, predicted))`\n",
    "- MAE formula: `mean_absolute_error(actual, predicted)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract actual prices and predictions\n",
    "actual = test_df['actual'].values\n",
    "enet_pred = # TODO: Extract ElasticNet predictions\n",
    "cgan_pred = # TODO: Extract cGAN mean predictions\n",
    "\n",
    "# TODO: Calculate metrics\n",
    "enet_rmse = # TODO: Calculate RMSE for ElasticNet\n",
    "cgan_rmse = # TODO: Calculate RMSE for cGAN\n",
    "\n",
    "enet_mae = # TODO: Calculate MAE for ElasticNet\n",
    "cgan_mae = # TODO: Calculate MAE for cGAN\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"POINT PREDICTION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ElasticNet - RMSE: {enet_rmse:.4f}, MAE: {enet_mae:.4f}\")\n",
    "print(f\"cGAN       - RMSE: {cgan_rmse:.4f}, MAE: {cgan_mae:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Create scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ElasticNet\n",
    "axes[0].scatter(# TODO: Add actual vs predicted for ElasticNet\n",
    "axes[0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price')\n",
    "axes[0].set_ylabel('Predicted Price')\n",
    "axes[0].set_title(f'ElasticNet (RMSE: {enet_rmse:.2f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# cGAN\n",
    "axes[1].scatter(# TODO: Add actual vs predicted for cGAN\n",
    "axes[1].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price')\n",
    "axes[1].set_ylabel('Predicted Price')\n",
    "axes[1].set_title(f'cGAN Mean (RMSE: {cgan_rmse:.2f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Interpretation (10 points)\n",
    "\n",
    "**Answer the following questions in 3-5 sentences each:**\n",
    "\n",
    "1. Which model has better point prediction accuracy (lower RMSE/MAE)? By how much?\n",
    "\n",
    "2. Looking at the scatter plots, do you see any systematic patterns in the errors (e.g., underprediction for high prices, overprediction for low prices)?\n",
    "\n",
    "3. What advantage does the cGAN offer beyond point predictions that ElasticNet cannot provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "1. [Your interpretation of which model is better and by how much]\n",
    "\n",
    "2. [Your analysis of the scatter plot patterns]\n",
    "\n",
    "3. [Your explanation of cGAN's advantages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Distributional Predictions (25 points)\n",
    "\n",
    "### Question 2.1: Prediction Intervals and Coverage (15 points)\n",
    "\n",
    "**Task:**\n",
    "- Calculate empirical coverage for the 90% prediction interval (5th to 95th percentile)\n",
    "- Calculate empirical coverage for the 50% prediction interval (25th to 75th percentile)\n",
    "- Visualize prediction intervals vs actual prices for 20 randomly selected bonds\n",
    "\n",
    "**Hints:**\n",
    "- Coverage = fraction of actual prices that fall within the interval\n",
    "- For 90% interval: check if `actual >= p05` AND `actual <= p95`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate coverage\n",
    "within_90 = # TODO: Calculate fraction of actuals within [p05, p95]\n",
    "within_50 = # TODO: Calculate fraction of actuals within [p25, p75]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION INTERVAL COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"50% Interval Coverage: {within_50:.2%} (expected: 50%)\")\n",
    "print(f\"90% Interval Coverage: {within_90:.2%} (expected: 90%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Visualize intervals for 20 random bonds\n",
    "# Select most frequently traded bonds\n",
    "top_bonds = test_df.groupby('cusip').size().nlargest(20).index\n",
    "sample_data = test_df[test_df['cusip'].isin(top_bonds)].copy()\n",
    "sample_data = sample_data.sort_values(['cusip', 'date'])\n",
    "\n",
    "# Pick one bond to visualize in detail\n",
    "sample_cusip = top_bonds[0]\n",
    "bond_data = sample_data[sample_data['cusip'] == sample_cusip]\n",
    "\n",
    "# TODO: Create time series plot with intervals\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# TODO: Plot actual prices\n",
    "# TODO: Plot cGAN mean\n",
    "# TODO: Fill between for 90% interval\n",
    "# TODO: Fill between for 50% interval\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_title(f'Prediction Intervals: {sample_cusip}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Interpretation (10 points)\n",
    "\n",
    "**Answer the following questions in 3-5 sentences each:**\n",
    "\n",
    "1. What does it mean if the empirical coverage is 92% for a 90% prediction interval? Is this good or bad?\n",
    "\n",
    "2. Looking at your time series plot, do the actual prices mostly fall within the prediction intervals? Are there any periods where the model is overconfident or underconfident?\n",
    "\n",
    "3. Why is this capability valuable for risk management? Think about how a trader or risk manager might use these intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "1. [Your interpretation of calibration and coverage]\n",
    "\n",
    "2. [Your analysis of the time series plot]\n",
    "\n",
    "3. [Your explanation of practical applications]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Portfolio Risk Analysis (30 points)\n",
    "\n",
    "### Question 3.1: Generate Correlated Scenarios (10 points)\n",
    "\n",
    "**Task:**\n",
    "- Generate 1000 correlated market scenarios for all bonds in the universe\n",
    "- Create an equal-weighted portfolio\n",
    "- Calculate portfolio value in each scenario\n",
    "- Calculate 95% VaR and CVaR\n",
    "\n",
    "**Starter code provided below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test date for scenario analysis\n",
    "analysis_date = test_dates[len(test_dates)//2]\n",
    "\n",
    "print(f\"Generating scenarios for {analysis_date.date()}...\")\n",
    "\n",
    "# Get ALL bonds in universe with features\n",
    "X_all, _ = prepare_features(universe_df, analysis_date, include_lagged_price=False, feature_columns=feature_names)\n",
    "X_all_scaled = scaler_cgan.transform(X_all.values)\n",
    "n_bonds = len(universe_df)\n",
    "\n",
    "print(f\"Universe size: {n_bonds} bonds\")\n",
    "\n",
    "# Generate correlated scenarios\n",
    "n_scenarios = 1000\n",
    "scenarios = np.zeros((n_scenarios, n_bonds))\n",
    "\n",
    "with torch.no_grad():\n",
    "    features_tensor = torch.tensor(X_all_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    for s in range(n_scenarios):\n",
    "        # KEY: Use same z for all bonds → captures correlations!\n",
    "        z = torch.randn(n_bonds, noise_dim, device=device)\n",
    "        prices_normalized = G(z, features_tensor).cpu().numpy().squeeze()\n",
    "        prices = prices_normalized * price_std + price_mean\n",
    "        scenarios[s] = prices\n",
    "\n",
    "print(f\"Scenario matrix shape: {scenarios.shape} (scenarios × bonds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create equal-weighted portfolio\n",
    "weights = # TODO: Create array of equal weights (1/n_bonds for each bond)\n",
    "\n",
    "# TODO: Calculate portfolio value in each scenario\n",
    "portfolio_values = # TODO: Matrix multiply scenarios with weights\n",
    "\n",
    "# TODO: Calculate VaR and CVaR\n",
    "var_95 = # TODO: Calculate 5th percentile of portfolio values\n",
    "cvar_95 = # TODO: Calculate mean of values below VaR_95\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PORTFOLIO RISK METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Portfolio Mean: {portfolio_values.mean():.4f}\")\n",
    "print(f\"Portfolio Std:  {portfolio_values.std():.4f}\")\n",
    "print(f\"VaR (95%):      {var_95:.4f}\")\n",
    "print(f\"CVaR (95%):     {cvar_95:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Create histogram of portfolio values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# TODO: Plot histogram\n",
    "# TODO: Add vertical line for VaR\n",
    "# TODO: Add vertical line for CVaR\n",
    "ax.set_xlabel('Portfolio Average Price')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Portfolio Values Across 1000 Scenarios')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Interpretation (20 points)\n",
    "\n",
    "**Answer the following questions in 3-5 sentences each:**\n",
    "\n",
    "1. What is the difference between VaR (Value-at-Risk) and CVaR (Conditional VaR)? Which is more informative for risk management?\n",
    "\n",
    "2. Based on your histogram, what does the CVaR value mean in practical terms? If you're a portfolio manager, how would you use this information?\n",
    "\n",
    "3. Why is it important that the scenarios are correlated (i.e., using the same noise vector z for all bonds)? What would happen if we generated independent scenarios for each bond?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "1. [Your explanation of VaR vs CVaR]\n",
    "\n",
    "2. [Your interpretation of the portfolio risk metrics]\n",
    "\n",
    "3. [Your explanation of why correlation matters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Correlation & Diversification (25 points)\n",
    "\n",
    "### Question 4.1: Sector Correlations (10 points)\n",
    "\n",
    "**Task:**\n",
    "- Calculate average price by sector across all scenarios\n",
    "- Compute correlation matrix between sectors\n",
    "- Create a correlation heatmap\n",
    "- Identify most/least correlated sector pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate sector averages across scenarios\n",
    "sectors = universe_df['sector'].unique()\n",
    "sector_scenarios = {}  # sector -> array of 1000 scenario averages\n",
    "\n",
    "for sector in sectors:\n",
    "    # TODO: For each sector, calculate average price across all bonds in that sector\n",
    "    # TODO: Do this for each of the 1000 scenarios\n",
    "    sector_mask = universe_df['sector'] == sector\n",
    "    sector_avg = # TODO: Calculate mean across bonds in this sector for each scenario\n",
    "    sector_scenarios[sector] = sector_avg\n",
    "\n",
    "# Convert to DataFrame for correlation\n",
    "sector_df = pd.DataFrame(sector_scenarios)\n",
    "\n",
    "# TODO: Calculate correlation matrix\n",
    "sector_corr = # TODO: Calculate correlation matrix of sector_df\n",
    "\n",
    "# TODO: Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# TODO: Use seaborn to create correlation heatmap\n",
    "plt.title('Sector Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Find most/least correlated pairs\n",
    "# Hint: Extract upper triangle of correlation matrix, find max/min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Diversification Analysis (5 points)\n",
    "\n",
    "**Task:**\n",
    "- Compare portfolio risk under equal-weight vs single-sector (concentrated) strategy\n",
    "- Calculate standard deviation for both portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-weight portfolio (already calculated above)\n",
    "equal_weight_std = portfolio_values.std()\n",
    "\n",
    "# TODO: Pick the largest sector and create concentrated portfolio\n",
    "largest_sector = # TODO: Find sector with most bonds\n",
    "sector_mask = universe_df['sector'] == largest_sector\n",
    "sector_bonds_idx = np.where(sector_mask)[0]\n",
    "\n",
    "# TODO: Calculate concentrated portfolio values\n",
    "concentrated_scenarios = # TODO: Extract only bonds in this sector from scenarios\n",
    "concentrated_values = # TODO: Calculate equal-weighted portfolio within this sector\n",
    "concentrated_std = # TODO: Calculate standard deviation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIVERSIFICATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Equal-weight portfolio std:  {equal_weight_std:.4f}\")\n",
    "print(f\"Concentrated ({largest_sector}) std: {concentrated_std:.4f}\")\n",
    "print(f\"Risk reduction from diversification: {(1 - equal_weight_std/concentrated_std)*100:.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3: Interpretation (10 points)\n",
    "\n",
    "**Answer the following questions in 3-5 sentences each:**\n",
    "\n",
    "1. Which two sectors are most correlated? Least correlated? What might explain these relationships?\n",
    "\n",
    "2. How much risk reduction do you achieve through diversification (equal-weight vs concentrated)? Is this a meaningful difference?\n",
    "\n",
    "3. How would you use this correlation information to construct a better diversified portfolio? Be specific about what you would overweight/underweight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "1. [Your analysis of sector correlations]\n",
    "\n",
    "2. [Your interpretation of diversification benefits]\n",
    "\n",
    "3. [Your portfolio construction recommendations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Reflection (Bonus - not graded, but recommended)\n",
    "\n",
    "**Question**: When would you recommend using a conditional GAN approach vs traditional regression (like ElasticNet) for bond pricing in a real-world setting? Consider factors like:\n",
    "- Computational cost\n",
    "- Interpretability\n",
    "- Risk management needs\n",
    "- Regulatory requirements\n",
    "\n",
    "Write 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR REFLECTION HERE:**\n",
    "\n",
    "[Your thoughtful reflection on when to use cGAN vs regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "1. Complete all TODO sections in the code cells\n",
    "2. Fill in all interpretation sections with your answers\n",
    "3. Make sure all cells run without errors (Run All)\n",
    "4. Export notebook as PDF or HTML\n",
    "5. Submit via Coursera\n",
    "\n",
    "**Checklist before submission:**\n",
    "- [ ] All code cells run successfully\n",
    "- [ ] All TODO comments replaced with working code\n",
    "- [ ] All interpretation questions answered\n",
    "- [ ] All plots generated and visible\n",
    "- [ ] Answers are in your own words (no copy-paste from internet)\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
