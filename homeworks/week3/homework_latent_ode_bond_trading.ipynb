{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Latent Neural ODE for Bond Trading Volume Prediction\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will implement a **Latent Neural ODE** model to predict the trading volume for Apple corporate bonds. Bond trades occur at irregular intervals, making this an ideal application for Neural ODEs that can handle continuous-time dynamics.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how to preprocess irregular time series data\n",
    "- Implement key components of a Latent ODE model\n",
    "- Train and evaluate the model against baselines\n",
    "- Interpret results in a financial context\n",
    "\n",
    "**Data:** Apple (AAPL) corporate bonds TRACE prints since 2025-01-01\n",
    "\n",
    "**Task:** Given historical trading volumes at irregular time points, predict the volume that will be traded the next time this bond trades.\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "| Part | Description | Points |\n",
    "|------|-------------|--------|\n",
    "| 1 | Data Loading & Exploration | 10 |\n",
    "| 2 | Data Preprocessing | 15 |\n",
    "| 3 | Model Architecture | 25 |\n",
    "| 4 | Training | 20 |\n",
    "| 5 | Evaluation & Baselines | 15 |\n",
    "| 6 | Interpretation Questions | 15 |\n",
    "| **Total** | | **100** |\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "- Fill in code where you see `# TODO: Your code here`\n",
    "- Do not modify provided code unless instructed\n",
    "- Answer all interpretation questions in the designated markdown cells\n",
    "- Run all cells to ensure your code works before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports (DO NOT MODIFY)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Loading & Exploration (10 points)\n",
    "\n",
    "### 1.1 Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Apple bonds TRACE data (PROVIDED)\n",
    "df = pd.read_csv('apple_ bonds_trace_prints_since_20250101.csv', index_col=0)\n",
    "\n",
    "print(f\"Total trades loaded: {len(df)}\")\n",
    "print(f\"\\nDate range: {df['trd_exctn_dt'].min()} to {df['trd_exctn_dt'].max()}\")\n",
    "print(f\"\\nUnique CUSIPs: {df['cusip_id'].nunique()}\")\n",
    "\n",
    "# Display relevant columns\n",
    "display_cols = ['cusip_id', 'trd_exctn_dt', 'trd_exctn_tm', 'ascii_rptd_vol_tx', \n",
    "                'rptg_party_type', 'contra_party_type', 'side', 'rptd_pr']\n",
    "df[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filter to Dealer-to-Customer (D2C) Trades (5 points)\n",
    "\n",
    "In TRACE data:\n",
    "- `rptg_party_type`: Type of reporting party (D=Dealer, C=Customer)\n",
    "- `contra_party_type`: Type of counterparty (D=Dealer, C=Customer)\n",
    "\n",
    "A D2C trade has one party as Dealer and the other as Customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a boolean mask for D2C trades (5 points)\n",
    "# A D2C trade is when:\n",
    "# - rptg_party_type is 'D' AND contra_party_type is 'C', OR\n",
    "# - rptg_party_type is 'C' AND contra_party_type is 'D'\n",
    "\n",
    "d2c_mask = # TODO: Your code here\n",
    "\n",
    "df_d2c = df[d2c_mask].copy()\n",
    "print(f\"D2C trades: {len(df_d2c)} ({len(df_d2c)/len(df)*100:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove Odd Lots (5 points)\n",
    "\n",
    "Odd lots are trades with par value less than $100,000. These are typically retail trades and may have different dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert volume to numeric\n",
    "df_d2c['volume'] = pd.to_numeric(df_d2c['ascii_rptd_vol_tx'], errors='coerce')\n",
    "\n",
    "# TODO: Remove odd lots (trades with volume < 100,000) (5 points)\n",
    "df_filtered = # TODO: Your code here\n",
    "\n",
    "print(f\"After removing odd lots (<100k): {len(df_filtered)} trades\")\n",
    "print(f\"Removed {len(df_d2c) - len(df_filtered)} odd lot trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Preprocessing (15 points)\n",
    "\n",
    "### 2.1 Aggregate at Daily Level (10 points)\n",
    "\n",
    "We need to aggregate multiple trades on the same day for the same bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df_filtered['date'] = pd.to_datetime(df_filtered['trd_exctn_dt'])\n",
    "\n",
    "# TODO: Aggregate by CUSIP and date (10 points)\n",
    "# Group by 'cusip_id' and 'date'\n",
    "# Compute:\n",
    "# - 'volume': sum of volumes\n",
    "# - 'rptd_pr': mean of prices\n",
    "# - 'msg_seq_nb': count of trades\n",
    "\n",
    "daily_trades = # TODO: Your code here\n",
    "\n",
    "# Rename columns\n",
    "daily_trades.columns = ['cusip_id', 'date', 'total_volume', 'avg_price', 'num_trades']\n",
    "\n",
    "print(f\"Daily aggregated trades: {len(daily_trades)}\")\n",
    "daily_trades.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Select CUSIPs for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 2 most active CUSIPs (PROVIDED)\n",
    "cusip_activity = daily_trades.groupby('cusip_id').agg({\n",
    "    'date': 'count',\n",
    "    'total_volume': 'sum'\n",
    "}).reset_index()\n",
    "cusip_activity.columns = ['cusip_id', 'trading_days', 'total_volume']\n",
    "cusip_activity = cusip_activity.sort_values('trading_days', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Active CUSIPs:\")\n",
    "print(cusip_activity.head(10))\n",
    "\n",
    "selected_cusips = cusip_activity.head(2)['cusip_id'].tolist()\n",
    "print(f\"\\nSelected CUSIPs: {selected_cusips}\")\n",
    "\n",
    "# Filter to selected CUSIPs\n",
    "df_selected = daily_trades[daily_trades['cusip_id'].isin(selected_cusips)].copy()\n",
    "df_selected = df_selected.sort_values(['cusip_id', 'date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Trading Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization (PROVIDED)\n",
    "fig, axes = plt.subplots(len(selected_cusips), 2, figsize=(14, 5*len(selected_cusips)))\n",
    "if len(selected_cusips) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, cusip in enumerate(selected_cusips):\n",
    "    cusip_data = df_selected[df_selected['cusip_id'] == cusip].copy()\n",
    "    \n",
    "    ax = axes[i, 0]\n",
    "    ax.bar(cusip_data['date'], cusip_data['total_volume'] / 1e6, alpha=0.7)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Volume (Millions $)')\n",
    "    ax.set_title(f'{cusip} - Daily Trading Volume')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[i, 1]\n",
    "    time_gaps = cusip_data['date'].diff().dt.days.dropna()\n",
    "    ax.hist(time_gaps, bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(time_gaps.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {time_gaps.mean():.1f} days')\n",
    "    ax.set_xlabel('Days Between Trades')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{cusip} - Trade Interval Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Apple Bond Trading Patterns', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Prepare Sequences for Training (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, cusip, lookback=10, horizon=1):\n",
    "    \"\"\"\n",
    "    Prepare sequences for Latent ODE training.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with date and total_volume\n",
    "        cusip: CUSIP to filter\n",
    "        lookback: Number of past observations to use\n",
    "        horizon: Number of future points to predict\n",
    "    Returns:\n",
    "        sequences: List of dictionaries\n",
    "        scaler: Fitted StandardScaler\n",
    "    \"\"\"\n",
    "    cusip_data = df[df['cusip_id'] == cusip].copy()\n",
    "    cusip_data = cusip_data.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Convert dates to numeric (days since first date)\n",
    "    first_date = cusip_data['date'].min()\n",
    "    cusip_data['days'] = (cusip_data['date'] - first_date).dt.days\n",
    "    \n",
    "    # Normalize volumes\n",
    "    scaler = StandardScaler()\n",
    "    cusip_data['volume_scaled'] = scaler.fit_transform(cusip_data[['total_volume']])\n",
    "    \n",
    "    sequences = []\n",
    "    n = len(cusip_data)\n",
    "    \n",
    "    # TODO: Create sequences (5 points)\n",
    "    # For each valid starting position i (from lookback to n-horizon):\n",
    "    # 1. Get observation data: cusip_data.iloc[i-lookback:i]\n",
    "    # 2. Get target data: cusip_data.iloc[i:i+horizon]\n",
    "    # 3. Normalize time to [0, 1] within the sequence\n",
    "    # 4. Append dictionary with t_obs, x_obs, t_target, x_target, volume_raw, date\n",
    "    \n",
    "    for i in range(lookback, n - horizon + 1):\n",
    "        obs_data = cusip_data.iloc[i-lookback:i]\n",
    "        target_data = cusip_data.iloc[i:i+horizon]\n",
    "        \n",
    "        # Normalize time to [0, 1]\n",
    "        t_start = obs_data['days'].iloc[0]\n",
    "        t_end = target_data['days'].iloc[-1]\n",
    "        t_range = t_end - t_start\n",
    "        \n",
    "        if t_range > 0:\n",
    "            # TODO: Compute normalized times\n",
    "            t_obs = # TODO: Your code here\n",
    "            t_target = # TODO: Your code here\n",
    "            \n",
    "            sequences.append({\n",
    "                't_obs': t_obs.astype(np.float32),\n",
    "                'x_obs': obs_data['volume_scaled'].values.astype(np.float32),\n",
    "                't_target': t_target.astype(np.float32),\n",
    "                'x_target': target_data['volume_scaled'].values.astype(np.float32),\n",
    "                'volume_raw': target_data['total_volume'].values,\n",
    "                'date': target_data['date'].values\n",
    "            })\n",
    "    \n",
    "    return sequences, scaler\n",
    "\n",
    "# Prepare sequences\n",
    "lookback = 10\n",
    "all_sequences = {}\n",
    "scalers = {}\n",
    "\n",
    "for cusip in selected_cusips:\n",
    "    sequences, scaler = prepare_sequences(df_selected, cusip, lookback=lookback)\n",
    "    all_sequences[cusip] = sequences\n",
    "    scalers[cusip] = scaler\n",
    "    print(f\"{cusip}: Created {len(sequences)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (Part of 15 points):** Why do we aggregate trades at the daily level instead of using individual trade-level data? What information might we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:** \n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Model Architecture (25 points)\n",
    "\n",
    "### 3.1 ODE Function (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent dynamics function (PROVIDED)\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"Defines the latent dynamics dz/dt = f(z, t).\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encoder (10 points)\n",
    "\n",
    "The encoder processes the irregular time series and outputs parameters of the latent distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"RNN encoder for irregular time series.\"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, latent_dim=8):\n",
    "        super().__init__()\n",
    "        # GRU takes input_dim + 1 (for time delta)\n",
    "        self.gru = nn.GRU(input_dim + 1, hidden_dim, batch_first=True)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Encode observations into latent distribution.\n",
    "        \n",
    "        Args:\n",
    "            x: Observations [batch, seq_len, 1]\n",
    "            t: Time points [batch, seq_len]\n",
    "        Returns:\n",
    "            mean: Mean of latent distribution [batch, latent_dim]\n",
    "            logvar: Log variance [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        # TODO: Compute time deltas (10 points)\n",
    "        # Time delta at position i is t[i] - t[i-1]\n",
    "        # First delta should be 0\n",
    "        t_delta = torch.zeros_like(t)\n",
    "        # TODO: Fill in t_delta[:, 1:] with differences\n",
    "        t_delta[:, 1:] = # TODO: Your code here\n",
    "        \n",
    "        # Concatenate observations with time deltas\n",
    "        # x shape: [batch, seq_len, 1]\n",
    "        # t_delta shape: [batch, seq_len]\n",
    "        # Result should be [batch, seq_len, 2]\n",
    "        x_with_time = # TODO: Your code here\n",
    "        \n",
    "        # Pass through GRU\n",
    "        _, h = self.gru(x_with_time)  # h shape: [1, batch, hidden_dim]\n",
    "        h = h.squeeze(0)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Map to latent distribution parameters\n",
    "        mean = self.fc_mean(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decoder (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder (PROVIDED)\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decode latent state to observation.\"\"\"\n",
    "    def __init__(self, latent_dim=8, hidden_dim=32, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Complete Latent ODE Model (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODEForecaster(nn.Module):\n",
    "    \"\"\"Latent ODE for time series forecasting.\"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, latent_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.ode_func = ODEFunc(latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick for VAE.\n",
    "        Sample z = mean + std * epsilon, where epsilon ~ N(0, I)\n",
    "        \"\"\"\n",
    "        # TODO: Implement reparameterization trick (5 points)\n",
    "        # 1. Compute std from logvar: std = exp(0.5 * logvar)\n",
    "        # 2. Sample epsilon from standard normal\n",
    "        # 3. Return mean + std * epsilon\n",
    "        \n",
    "        std = # TODO: Your code here\n",
    "        eps = # TODO: Your code here\n",
    "        return # TODO: Your code here\n",
    "    \n",
    "    def forward(self, x_obs, t_obs, t_pred):\n",
    "        \"\"\"\n",
    "        Forward pass: encode, evolve latent state, decode.\n",
    "        \n",
    "        Args:\n",
    "            x_obs: Observations [batch, obs_len, 1]\n",
    "            t_obs: Observation times [batch, obs_len]\n",
    "            t_pred: Prediction times [batch, pred_len]\n",
    "        Returns:\n",
    "            predictions: [batch, pred_len, 1]\n",
    "            mean, logvar: Latent distribution parameters\n",
    "        \"\"\"\n",
    "        # Encode observations\n",
    "        mean, logvar = self.encoder(x_obs, t_obs)\n",
    "        \n",
    "        # Sample initial latent state\n",
    "        z0 = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # Get the last observation time for each sample\n",
    "        t_last_obs = t_obs[:, -1]  # [batch]\n",
    "        batch_size = x_obs.shape[0]\n",
    "        pred_len = t_pred.shape[1]\n",
    "        \n",
    "        # TODO: Evolve latent state and decode (10 points)\n",
    "        # For each sample in batch:\n",
    "        # 1. Create time grid from t_last_obs to t_pred times\n",
    "        # 2. Solve ODE using odeint\n",
    "        # 3. Decode the predicted latent states\n",
    "        \n",
    "        predictions = torch.zeros(batch_size, pred_len, 1).to(x_obs.device)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Create time grid: start from last observation, then prediction times\n",
    "            t_grid = # TODO: Your code here (concatenate t_last_obs[b:b+1] with t_pred[b])\n",
    "            \n",
    "            # Solve ODE\n",
    "            z_traj = odeint(self.ode_func, z0[b:b+1], t_grid, method='dopri5')\n",
    "            # z_traj shape: [len(t_grid), 1, latent_dim]\n",
    "            \n",
    "            # Skip first time point (t_last_obs), keep predictions\n",
    "            z_pred = z_traj[1:].squeeze(1)  # [pred_len, latent_dim]\n",
    "            \n",
    "            # Decode\n",
    "            predictions[b] = # TODO: Your code here\n",
    "        \n",
    "        return predictions, mean, logvar\n",
    "    \n",
    "    def predict(self, x_obs, t_obs, t_pred):\n",
    "        \"\"\"Deterministic prediction using mean of latent distribution.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, _ = self.encoder(x_obs, t_obs)\n",
    "            z0 = mean  # Use mean instead of sampling\n",
    "            \n",
    "            batch_size = x_obs.shape[0]\n",
    "            t_last_obs = t_obs[:, -1]\n",
    "            pred_len = t_pred.shape[1]\n",
    "            \n",
    "            predictions = torch.zeros(batch_size, pred_len, 1).to(x_obs.device)\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                t_grid = torch.cat([t_last_obs[b:b+1], t_pred[b]])\n",
    "                z_traj = odeint(self.ode_func, z0[b:b+1], t_grid, method='dopri5')\n",
    "                z_pred = z_traj[1:].squeeze(1)\n",
    "                predictions[b] = self.decoder(z_pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (Part of 25 points):** Why do we include time deltas as input features to the encoder? What information does this provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Training (20 points)\n",
    "\n",
    "### 4.1 Batch Preparation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(sequences, device='cpu'):\n",
    "    \"\"\"\n",
    "    Prepare a batch of sequences for training.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequence dictionaries\n",
    "        device: Device to put tensors on\n",
    "    Returns:\n",
    "        x_obs, t_obs, t_target, x_target as tensors\n",
    "    \"\"\"\n",
    "    batch_size = len(sequences)\n",
    "    obs_len = len(sequences[0]['t_obs'])\n",
    "    \n",
    "    # Initialize tensors\n",
    "    x_obs = torch.zeros(batch_size, obs_len, 1)\n",
    "    t_obs = torch.zeros(batch_size, obs_len)\n",
    "    t_target = torch.zeros(batch_size, 1)\n",
    "    x_target = torch.zeros(batch_size, 1, 1)\n",
    "    \n",
    "    # TODO: Fill tensors from sequences (5 points)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        x_obs[i, :, 0] = # TODO: Your code here\n",
    "        t_obs[i, :] = # TODO: Your code here\n",
    "        t_target[i, 0] = # TODO: Your code here\n",
    "        x_target[i, 0, 0] = # TODO: Your code here\n",
    "    \n",
    "    return (x_obs.to(device), t_obs.to(device), \n",
    "            t_target.to(device), x_target.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loss Function (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vae_loss(pred, target, mean, logvar, kl_weight=0.01):\n",
    "    \"\"\"\n",
    "    Compute VAE loss: reconstruction + KL divergence.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predictions [batch, pred_len, 1]\n",
    "        target: Targets [batch, pred_len, 1]\n",
    "        mean: Latent mean [batch, latent_dim]\n",
    "        logvar: Latent log variance [batch, latent_dim]\n",
    "        kl_weight: Weight for KL term\n",
    "    Returns:\n",
    "        total_loss, mse_loss, kl_loss\n",
    "    \"\"\"\n",
    "    # TODO: Compute MSE reconstruction loss (5 points)\n",
    "    mse_loss = # TODO: Your code here (use nn.functional.mse_loss)\n",
    "    \n",
    "    # TODO: Compute KL divergence (5 points)\n",
    "    # KL(q(z) || p(z)) where p(z) = N(0, I)\n",
    "    # Formula: -0.5 * sum(1 + logvar - mean^2 - exp(logvar))\n",
    "    # Average over batch\n",
    "    kl_loss = # TODO: Your code here\n",
    "    \n",
    "    total_loss = mse_loss + kl_weight * kl_loss\n",
    "    \n",
    "    return total_loss, mse_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_seqs, val_seqs, epochs=100, batch_size=16, lr=0.001, kl_weight=0.01):\n",
    "    \"\"\"Train the Latent ODE model.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    n_train_batches = len(train_seqs) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        indices = np.random.permutation(len(train_seqs))\n",
    "        \n",
    "        for i in range(n_train_batches):\n",
    "            batch_idx = indices[i*batch_size:(i+1)*batch_size]\n",
    "            batch_seqs = [train_seqs[j] for j in batch_idx]\n",
    "            \n",
    "            x_obs, t_obs, t_target, x_target = prepare_batch(batch_seqs, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # TODO: Forward pass and compute loss (5 points)\n",
    "            pred, mean, logvar = # TODO: Your code here\n",
    "            loss, mse, kl = # TODO: Your code here\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / n_train_batches)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_obs, t_obs, t_target, x_target = prepare_batch(val_seqs, device)\n",
    "            pred = model.predict(x_obs, t_obs, t_target)\n",
    "            val_mse = nn.functional.mse_loss(pred, x_target).item()\n",
    "        val_losses.append(val_mse)\n",
    "        \n",
    "        scheduler.step(val_mse)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_losses[-1]:.6f} | Val MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on first CUSIP (PROVIDED setup)\n",
    "cusip = selected_cusips[0]\n",
    "sequences = all_sequences[cusip]\n",
    "\n",
    "# Split data\n",
    "n_total = len(sequences)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "train_seqs = sequences[:n_train]\n",
    "val_seqs = sequences[n_train:n_train+n_val]\n",
    "test_seqs = sequences[n_train+n_val:]\n",
    "\n",
    "print(f\"Training on {cusip}\")\n",
    "print(f\"Train: {len(train_seqs)}, Val: {len(val_seqs)}, Test: {len(test_seqs)}\")\n",
    "\n",
    "# Create model\n",
    "model = LatentODEForecaster(input_dim=1, hidden_dim=32, latent_dim=8).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Train\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_seqs, val_seqs,\n",
    "    epochs=150, batch_size=8, lr=0.005, kl_weight=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves (PROVIDED)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_losses, color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('Validation MSE')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Latent ODE Training for {cusip}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 (Part of 20 points):** What is the role of the KL weight (kl_weight) in the loss function? What happens if we set it too high or too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Evaluation & Baselines (15 points)\n",
    "\n",
    "### 5.1 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_seqs, scaler):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq in test_seqs:\n",
    "            x_obs = torch.tensor(seq['x_obs']).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "            t_obs = torch.tensor(seq['t_obs']).unsqueeze(0).to(device)\n",
    "            t_target = torch.tensor(seq['t_target']).unsqueeze(0).to(device)\n",
    "            \n",
    "            pred = model.predict(x_obs, t_obs, t_target)\n",
    "            pred_scaled = pred.squeeze().cpu().numpy()\n",
    "            \n",
    "            pred_original = scaler.inverse_transform([[pred_scaled]])[0, 0]\n",
    "            actual_original = seq['volume_raw'][0]\n",
    "            \n",
    "            predictions.append(pred_original)\n",
    "            actuals.append(actual_original)\n",
    "    \n",
    "    return np.array(predictions), np.array(actuals)\n",
    "\n",
    "# Evaluate Latent ODE\n",
    "predictions, actuals = evaluate_model(model, test_seqs, scalers[cusip])\n",
    "print(f\"Latent ODE - Test samples: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compute Metrics (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, actuals):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \n",
    "    Returns dict with MSE, RMSE, MAE, MAPE\n",
    "    \"\"\"\n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    # TODO: Compute MAPE (Mean Absolute Percentage Error) (5 points)\n",
    "    # MAPE = mean(|pred - actual| / |actual|) * 100\n",
    "    mape = # TODO: Your code here\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "metrics_ode = compute_metrics(predictions, actuals)\n",
    "print(f\"Latent ODE Metrics for {cusip}:\")\n",
    "for metric, value in metrics_ode.items():\n",
    "    if metric in ['MSE', 'RMSE', 'MAE']:\n",
    "        print(f\"  {metric}: ${value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 GRU Baseline (10 points)\n",
    "\n",
    "Implement a standard GRU model (without ODE) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard GRU baseline without ODE dynamics.\n",
    "    Takes observations and directly predicts the next value.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1):\n",
    "        super().__init__()\n",
    "        # TODO: Define GRU and output layer (10 points)\n",
    "        # GRU should take input_dim + 1 (for time delta) similar to encoder\n",
    "        self.gru = # TODO: Your code here\n",
    "        self.fc_out = # TODO: Your code here\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Observations [batch, seq_len, 1]\n",
    "            t: Time points [batch, seq_len]\n",
    "        Returns:\n",
    "            Prediction [batch, 1]\n",
    "        \"\"\"\n",
    "        # Compute time deltas\n",
    "        t_delta = torch.zeros_like(t)\n",
    "        t_delta[:, 1:] = t[:, 1:] - t[:, :-1]\n",
    "        \n",
    "        # Concatenate\n",
    "        x_with_time = torch.cat([x, t_delta.unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "        # TODO: Pass through GRU and output layer\n",
    "        _, h = # TODO: Your code here\n",
    "        h = h.squeeze(0)\n",
    "        out = # TODO: Your code here\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU baseline (PROVIDED training loop)\n",
    "gru_model = GRUBaseline(input_dim=1, hidden_dim=32, output_dim=1).to(device)\n",
    "optimizer_gru = torch.optim.Adam(gru_model.parameters(), lr=0.005)\n",
    "\n",
    "print(\"Training GRU Baseline...\")\n",
    "for epoch in range(150):\n",
    "    gru_model.train()\n",
    "    indices = np.random.permutation(len(train_seqs))\n",
    "    epoch_loss = 0\n",
    "    n_batches = len(train_seqs) // 8\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batch_idx = indices[i*8:(i+1)*8]\n",
    "        batch_seqs = [train_seqs[j] for j in batch_idx]\n",
    "        x_obs, t_obs, _, x_target = prepare_batch(batch_seqs, device)\n",
    "        \n",
    "        optimizer_gru.zero_grad()\n",
    "        pred = gru_model(x_obs, t_obs)\n",
    "        loss = nn.functional.mse_loss(pred, x_target.squeeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer_gru.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/150 | Loss: {epoch_loss/n_batches:.6f}\")\n",
    "\n",
    "# Evaluate GRU baseline\n",
    "gru_model.eval()\n",
    "gru_preds = []\n",
    "with torch.no_grad():\n",
    "    for seq in test_seqs:\n",
    "        x_obs = torch.tensor(seq['x_obs']).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        t_obs = torch.tensor(seq['t_obs']).unsqueeze(0).to(device)\n",
    "        pred = gru_model(x_obs, t_obs)\n",
    "        pred_scaled = pred.squeeze().cpu().numpy()\n",
    "        pred_original = scalers[cusip].inverse_transform([[pred_scaled]])[0, 0]\n",
    "        gru_preds.append(pred_original)\n",
    "\n",
    "gru_preds = np.array(gru_preds)\n",
    "metrics_gru = compute_metrics(gru_preds, actuals)\n",
    "print(f\"\\nGRU Baseline Metrics:\")\n",
    "for metric, value in metrics_gru.items():\n",
    "    if metric in ['MSE', 'RMSE', 'MAE']:\n",
    "        print(f\"  {metric}: ${value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Simple Baselines (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Value Baseline\n",
    "last_preds = []\n",
    "for seq in test_seqs:\n",
    "    last_scaled = seq['x_obs'][-1]\n",
    "    pred = scalers[cusip].inverse_transform([[last_scaled]])[0, 0]\n",
    "    last_preds.append(pred)\n",
    "last_preds = np.array(last_preds)\n",
    "metrics_last = compute_metrics(last_preds, actuals)\n",
    "\n",
    "# Moving Average Baseline (3-day)\n",
    "ma_preds = []\n",
    "for seq in test_seqs:\n",
    "    ma_scaled = np.mean(seq['x_obs'][-3:])\n",
    "    pred = scalers[cusip].inverse_transform([[ma_scaled]])[0, 0]\n",
    "    ma_preds.append(pred)\n",
    "ma_preds = np.array(ma_preds)\n",
    "metrics_ma = compute_metrics(ma_preds, actuals)\n",
    "\n",
    "# Comparison table\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(f\"{'Model':<20} {'RMSE ($)':<15} {'MAE ($)':<15} {'MAPE (%)':<10}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Last Value':<20} {metrics_last['RMSE']:>12,.0f} {metrics_last['MAE']:>12,.0f} {metrics_last['MAPE']:>8.2f}\")\n",
    "print(f\"{'Moving Avg (3)':<20} {metrics_ma['RMSE']:>12,.0f} {metrics_ma['MAE']:>12,.0f} {metrics_ma['MAPE']:>8.2f}\")\n",
    "print(f\"{'GRU Baseline':<20} {metrics_gru['RMSE']:>12,.0f} {metrics_gru['MAE']:>12,.0f} {metrics_gru['MAPE']:>8.2f}\")\n",
    "print(f\"{'Latent ODE':<20} {metrics_ode['RMSE']:>12,.0f} {metrics_ode['MAE']:>12,.0f} {metrics_ode['MAPE']:>8.2f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualization (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Predicted vs Actual\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(actuals/1e6, predictions/1e6, alpha=0.6, s=50, label='Latent ODE')\n",
    "ax.scatter(actuals/1e6, gru_preds/1e6, alpha=0.6, s=50, label='GRU', marker='x')\n",
    "max_val = max(actuals.max(), predictions.max(), gru_preds.max()) / 1e6\n",
    "ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2)\n",
    "ax.set_xlabel('Actual (Millions $)')\n",
    "ax.set_ylabel('Predicted (Millions $)')\n",
    "ax.set_title('Predicted vs Actual')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Time series\n",
    "ax = axes[0, 1]\n",
    "ax.plot(actuals/1e6, 'b-', label='Actual', linewidth=2)\n",
    "ax.plot(predictions/1e6, 'r--', label='Latent ODE', linewidth=2)\n",
    "ax.plot(gru_preds/1e6, 'g:', label='GRU', linewidth=2)\n",
    "ax.set_xlabel('Test Sample')\n",
    "ax.set_ylabel('Volume (Millions $)')\n",
    "ax.set_title('Predictions Over Test Set')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution - Latent ODE\n",
    "ax = axes[1, 0]\n",
    "errors_ode = ((predictions - actuals) / actuals) * 100\n",
    "ax.hist(errors_ode, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Percentage Error (%)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Latent ODE Error Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution - GRU\n",
    "ax = axes[1, 1]\n",
    "errors_gru = ((gru_preds - actuals) / actuals) * 100\n",
    "ax.hist(errors_gru, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Percentage Error (%)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('GRU Baseline Error Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Model Comparison for {cusip}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Train on Second CUSIP (PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on second CUSIP\n",
    "if len(selected_cusips) > 1:\n",
    "    cusip2 = selected_cusips[1]\n",
    "    sequences2 = all_sequences[cusip2]\n",
    "    \n",
    "    n_total2 = len(sequences2)\n",
    "    n_train2 = int(0.7 * n_total2)\n",
    "    n_val2 = int(0.15 * n_total2)\n",
    "    \n",
    "    train_seqs2 = sequences2[:n_train2]\n",
    "    val_seqs2 = sequences2[n_train2:n_train2+n_val2]\n",
    "    test_seqs2 = sequences2[n_train2+n_val2:]\n",
    "    \n",
    "    print(f\"\\nTraining on {cusip2}\")\n",
    "    print(f\"Train: {len(train_seqs2)}, Val: {len(val_seqs2)}, Test: {len(test_seqs2)}\")\n",
    "    \n",
    "    model2 = LatentODEForecaster(input_dim=1, hidden_dim=32, latent_dim=8).to(device)\n",
    "    train_losses2, val_losses2 = train_model(\n",
    "        model2, train_seqs2, val_seqs2,\n",
    "        epochs=150, batch_size=8, lr=0.005, kl_weight=0.001\n",
    "    )\n",
    "    \n",
    "    predictions2, actuals2 = evaluate_model(model2, test_seqs2, scalers[cusip2])\n",
    "    metrics2 = compute_metrics(predictions2, actuals2)\n",
    "    \n",
    "    print(f\"\\nLatent ODE Metrics for {cusip2}:\")\n",
    "    for metric, value in metrics2.items():\n",
    "        if metric in ['MSE', 'RMSE', 'MAE']:\n",
    "            print(f\"  {metric}: ${value:,.0f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Interpretation Questions (15 points)\n",
    "\n",
    "Answer each question in 3-5 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1 (4 points)\n",
    "\n",
    "Why is irregular sampling problematic for traditional time series models like ARIMA? How do Neural ODEs address this challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2 (4 points)\n",
    "\n",
    "How would you modify this model to predict the average traded **price** (instead of volume) for the next trading day? What changes would be needed in the preprocessing and model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.3 (3 points)\n",
    "\n",
    "Compare the results between the two CUSIPs. What factors might explain any differences in model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.4 (4 points)\n",
    "\n",
    "How would you adapt the preprocessing and model to predict **dealer buy** and **dealer sell** volumes separately? What insights might this provide about market dynamics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Instructions\n",
    "\n",
    "1. Ensure all code cells run without errors\n",
    "2. Make sure all TODO sections are completed\n",
    "3. Answer all interpretation questions\n",
    "4. Save this notebook with your answers\n",
    "5. Submit the completed `.ipynb` file\n",
    "\n",
    "**Good luck!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_finance",
   "language": "python",
   "name": "dl_finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
